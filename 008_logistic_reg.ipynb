{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "\n",
    "### Images\n",
    "\n",
    "For this problem we'll use some simple images - a text classfication dataset that is a common ML toy dataset. Up until now we've only used text and numbers for data, how do we deal with images?\n",
    "\n",
    "We can think of an image as a matrix of pixels. If you ever looked at your TV up extremely close as a kid, you've seen this. Each image here is a 28 by 28 pixel grid, each point on that grid is one pixel that can be somewhere on the black to white scale, which is represented by 0 to 255. So our overall dataset is ~70000 images, each one being a 28 x 28 (784 pixel) x 1 matrix. The only thing making it an image instead of a big table of numbers is how we interpret it when reading the data - if we don't know it is an image, we'd look at it as a bunch of integers; if we know to interpret it as an image, we can use those integers to draw what we were looking for!\n",
    "\n",
    "If you have a 1080p TV or computer monitor the same logic applies: The screen is a 1920 x 1080 pixel grid, but here each pixel can be multiple colors (there are different color encodings, but the idea is the same) - so instead of each pixel on the grid having a depth of 1 (like our BW digits), each pixel has a depth of 3 - one for each of red, green, and blue, all on a 0 to 255 scale. This allows each pixel to have a position, and a color made up of a combo of those 3 values, giving us a pretty picture. If we were encoding a video, we'd have a series of these images in sequence - with 24, 30, 60 or however many frames per second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n",
    "\n",
    "Our images are currently stored as pixels:\n",
    "<ul>\n",
    "<li> Each image is 28 x 28 pixels, so that is 784 total pixels. \n",
    "<li> Each individual pixel is a value on a 255 pt scale - greyscale in this case. \n",
    "</ul>\n",
    "\n",
    "Our labels are just the numbers, if we look at a few, each is just a bunch-o-pixels. Overall, the feature set is the pixels and the target is the actual number (the label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       0.0       0.0       0.0       0.0  \n",
       "1       0.0       0.0       0.0       0.0  \n",
       "2       0.0       0.0       0.0       0.0  \n",
       "3       0.0       0.0       0.0       0.0  \n",
       "4       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images as Arrays\n",
    "\n",
    "The above visualization is one of those list of pixels if we picture it. There are 28 x 28 pixels in a 2D grid, and each of those pixels is some degree of \"colored\". It may be easier to see with a more elaborate image:\n",
    "\n",
    "![Image Array](images/image_array.jpeg \"Image Array\" )\n",
    "\n",
    "All of the \"outside\" pixels are 0 - black (ours are flipped - black text). Each part of the number has a higher number based on lightness. Our overall image is represented by a 28 x 28 x 1 array - width, height, and \"depth\" or \"color depth\", we only have one color (black) so the depth is 1.  This image is pretty low definition, so it is not super clear. The images on our monitors are the same, just with higher definition. We'll look at more elaborate images later, they are stored in the same way, except for color images we have 3 (usually) layers for depth. \n",
    "\n",
    "#### Examine an Image\n",
    "\n",
    "To look at one row of our array in its image form, we can first look at the values from the array, then ask our program to display it as an image. First, we will show an \"image\", one row of our data, as a 2D array, rather than 1D data in the dataframe. To do so:\n",
    "<ul>\n",
    "<li> Grab one row of data. (One image).\n",
    "    <ul>\n",
    "    <li> The weird index is because it is a 2D array. We are basically grabing from the \"start of 5\" to the \"start of 6\" (non-inclusive).\n",
    "    </ul>\n",
    "<li> Reshape that row of 784 pixels into a 28 x 28 array.\n",
    "<li> Print the numerical values of our 28 x 28 array, arranging them in a grid.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "5     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "5      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "5       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[1 rows x 784 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t13.0\t25.0\t100.0\t122.0\t7.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t33.0\t151.0\t208.0\t252.0\t252.0\t252.0\t146.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t40.0\t152.0\t244.0\t252.0\t253.0\t224.0\t211.0\t252.0\t232.0\t40.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t15.0\t152.0\t239.0\t252.0\t252.0\t252.0\t216.0\t31.0\t37.0\t252.0\t252.0\t60.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t96.0\t252.0\t252.0\t252.0\t252.0\t217.0\t29.0\t0.0\t37.0\t252.0\t252.0\t60.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t181.0\t252.0\t252.0\t220.0\t167.0\t30.0\t0.0\t0.0\t77.0\t252.0\t252.0\t60.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t26.0\t128.0\t58.0\t22.0\t0.0\t0.0\t0.0\t0.0\t100.0\t252.0\t252.0\t60.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t157.0\t252.0\t252.0\t60.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t110.0\t121.0\t122.0\t121.0\t202.0\t252.0\t194.0\t3.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t10.0\t53.0\t179.0\t253.0\t253.0\t255.0\t253.0\t253.0\t228.0\t35.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t54.0\t227.0\t252.0\t243.0\t228.0\t170.0\t242.0\t252.0\t252.0\t231.0\t117.0\t6.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t6.0\t78.0\t252.0\t252.0\t125.0\t59.0\t0.0\t18.0\t208.0\t252.0\t252.0\t252.0\t252.0\t87.0\t7.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t135.0\t252.0\t252.0\t180.0\t16.0\t0.0\t21.0\t203.0\t253.0\t247.0\t129.0\t173.0\t252.0\t252.0\t184.0\t66.0\t49.0\t49.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t136.0\t252.0\t241.0\t106.0\t17.0\t0.0\t53.0\t200.0\t252.0\t216.0\t65.0\t0.0\t14.0\t72.0\t163.0\t241.0\t252.0\t252.0\t223.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t105.0\t252.0\t242.0\t88.0\t18.0\t73.0\t170.0\t244.0\t252.0\t126.0\t29.0\t0.0\t0.0\t0.0\t0.0\t0.0\t89.0\t180.0\t180.0\t37.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t231.0\t252.0\t245.0\t205.0\t216.0\t252.0\t252.0\t252.0\t124.0\t3.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t207.0\t252.0\t252.0\t252.0\t252.0\t178.0\t116.0\t36.0\t4.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t13.0\t93.0\t143.0\t121.0\t23.0\t6.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n"
     ]
    }
   ],
   "source": [
    "tmp = np.array(X[5:6]).reshape(28,28)\n",
    "#sns.heatmap(tmp, cmap='gray')\n",
    "#print(tmp)\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in tmp]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print its image representation, there's a couple of steps to make it \"image-y\":\n",
    "<ul>\n",
    "<li>Take a row of data from the dataframe. \n",
    "<li>Make it into an array - 28 x 28. \n",
    "<li>Use mathplotlib to show the array of integers interpreted as an image.\n",
    "</ul>\n",
    "\n",
    "There are lots of ways to take image data and display an image, so examples may vary quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAH5ElEQVR4nO3dTajVdR7H8d+v7nihh5laZBToXKjEWboJeqChmARtMUarlrVpU7QIiogebBPZonAjWNFGjYTpCWyKNrUxIkrQjEAGZxiI0gjympfRiTOrgYE839Oco/d8zvX1Wvrh7/lhvPlHv463DwaDBuS5aNoHAM5OnBBKnBBKnBBKnBBKnBBKnBBKnCtA732+9/5q7/0fvffF3vuB3vumaZ+LyYhzZZhrrf2ztfbH1trvWmtPttb29t4XpnkoJtP9H0IrU+/9YGtt62Aw+Mu0z8J4vDlXoN771a21da21w9M+C+Pz5lxheu+/aa39tbX2t8Fg8MC0z8P4xLmC9N4vaq3taa39trX258FgcGbKR2ICc9M+AOdG77231l5trV3dWtsszNknzpVjR2vtD621Pw0Gg6VpH4bJ+dfaFaD3/vvW2t9ba/9qrf37f6YHBoPB7qkciomJE0K5SoFQ4oRQ4oRQ4oRQo65S/NciOP/62X7RmxNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCzU37ACyvxcXFcj958uTQbd++feWzx44dK/dHHnmk3Ofn58v9QuPNCaHECaHECaHECaHECaHECaHECaHcc86Yo0ePlvu2bdvK/ZNPPin3Q4cO/d9n+rW+/fbbct++fft5++xZ5M0JocQJocQJocQJocQJocQJofpgMKj2cmQ8X3/99dDtpZdeKp/dtWtXuS8tLZX7iH/ebe3atUO3yy+/vHz2q6++Kverrrqq3D/66KOh2/r168tnZ1w/2y96c0IocUIocUIocUIocUIocUIocUIoXxkbw48//ljujz32WLm/8cYbQ7cTJ06MdaZfa926deX+wQcfDN1Onz5dPjvqLvL48ePl/v3335f7hcabE0KJE0KJE0KJE0KJE0KJE0KJE0K55xzDW2+9Ve4vv/zyMp3kl66//vpy//DDD8t9zZo1Q7cjR46MdSbG480JocQJocQJocQJocQJocQJocQJodxzjmHv3r3n7fdeWFgo9xtvvLHcn3/++XKv7jFHqf6+Xc49b04IJU4IJU4IJU4IJU4IJU4IJU4I5Z5zDK+88kq579y5s9w3btw4dBv1fczVq1eX+/n03XffTe2zL0TenBBKnBBKnBBKnBBKnBBKnBDKVcoYrr322nJ/5plnlucgy2z//v3TPsIFxZsTQokTQokTQokTQokTQokTQokTQrnnnDHbt28v959++qncB4NBuffeh25ffvll+ewot9xyS7nfdNNNE/3+K403J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4TqI+696ksxzurUqVPlfvjw4aHbs88+Wz67b9++sc70X5Pcc44y6nuuH3/8cblfd911Y3/2jDvrH7o3J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Tyfc6zOHPmTLkfOHCg3O+5555y/+abb4Zul1xySfnsqLvEm2++udzff//9ch/1fdDKzz//XO5vvvlmuT/88MNDt1WrVo11plnmzQmhxAmhxAmhxAmhxAmhxAmhxAmhLsjvc54+fbrcR90F3n333RN9fvXzO2+//fby2VtvvbXcf/jhh3K/4447yv3QoUPlfj7t2bNn6LZly5by2fn5+XN8mmXl+5wwS8QJocQJocQJocQJocQJoVbsVUr1ta+nnnqqfHbbtm0TffamTZvKfdeuXUO3K664onz2+PHj5b558+Zy//zzz8u9upJ49NFHy2dHXcO888475V658847y33U2a688sqxP7u11jZs2DDR8yO4SoFZIk4IJU4IJU4IJU4IJU4IJU4INbP3nKP+GsYnnnhi6PbCCy+Uz1522WXl/txzz5X7vffeW+7Vndtnn31WPvvQQw+V+6jnb7jhhnLfsWPH0G3U19lOnDhR7vv37y/33bt3D93efffd8tmTJ0+W+yhr164t96NHj070+4/gnhNmiTghlDghlDghlDghlDghlDgh1Mzec1b3ca219uCDDw7dLr300vLZnTt3lvvGjRvL/dNPPy331157bej23nvvlc8uLS2V+9NPP13u9913X7mvWbOm3Kfl9ddfL/fqjvTXePHFF8t91P3whNxzwiwRJ4QSJ4QSJ4QSJ4QSJ4QSJ4Sa2XvOa665ptyPHTs2dBv14+LWr19f7qdOnSr3I0eOlPsktm7dWu6PP/54uV988cXn8jicG+45YZaIE0KJE0KJE0KJE0KJE0LN7FXKqB/JdvDgwWU6yS/ddddd5X7bbbcN3bZs2VI+u7CwUO5zc3PlTiRXKTBLxAmhxAmhxAmhxAmhxAmhxAmhZvaec3FxsdzffvvtodsXX3xRPrt69epyv//++8u9+hF/rbW2atWqcueC454TZok4IZQ4IZQ4IZQ4IZQ4IZQ4IdTM3nPCCuKeE2aJOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCHU3Ii9L8spgF/w5oRQ4oRQ4oRQ4oRQ4oRQ4oRQ/wFNpm7almNKQQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Look at an image\n",
    "def showDigit(digit, label, size=28):\n",
    "    some_digit = digit\n",
    "    #turn array into the correct shape\n",
    "    some_digit_image = np.array(some_digit).reshape(size, size)\n",
    "    #imshow displays an array like an image\n",
    "    plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "showDigit(X[5:6], y[5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Images\n",
    "\n",
    "Color images are a little more complicated, but the same idea applies. We have a 3D array, with 3 layers for each pixel. Each layer is a color, and each color is a value on a 0 to 255 scale. We'll deal with color images later, but for now, we can think of them as just 3 of our BW images stacked on top of each other - one layer is each color. \n",
    "\n",
    "![RGB Image](images/rgb_image_array.png \"RGB Image\" )\n",
    "\n",
    "A color image like this is a similar array as ours, but larger. If the image was 100 x 100 pixels, the array would be 100 x 100 x 3 (1 depth count per color) - this is also something called a tensor, which will be meaningful later. This is why we can do fun stuff with images like facial recognition - images are just big 'ol arrays. This is also why when we start dealing with high definition images or videos, things become MUCH slower; the amount of data in image data grows rapidly the better our images are. \n",
    "\n",
    "<b>Note:</b> there are different ways to encode images into data we can store. We won't worry about them, but the ideas are the same for our purposes - the format of the data in the feature set is just different.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Several Images\n",
    "\n",
    "We can create a little function that will print out a bunch of images that we can preview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display multiple digits\n",
    "def showDigits(digits, labels, indexes, size=28):\n",
    "    #Make a grid that is the right size\n",
    "    pics = len(indexes)\n",
    "    cols = 6\n",
    "    rows = math.ceil(pics/cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(14,6))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    #loop through the list of indexes, grab images and labels, plot in the \"next\" spot. \n",
    "    for i in range(0, pics):\n",
    "        n = indexes[i]\n",
    "        some_digit = digits[n:n+1]\n",
    "        some_digit_image = np.array(some_digit).reshape(size, size)\n",
    "        ax = axes[i//cols, i%cols]\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "        ax.set_title('Ind: {} - Lbl: {}'.format(indexes[i], labels[n]))\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random digits. \n",
    "showDigits(X, y, [10,11,12,15,16,78,863,112,46,76,34,454,232,55,43,2,5,102])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Images\n",
    "\n",
    "Like any other data, we can use logistic regression to classify images. \n",
    "\n",
    "Our logistic regression is another application of the gradient descent process, since there is no directly calculable solution. Our feature set has 784 features, and we are using each of those features, or pixels, to predict the label. \n",
    "\n",
    "### Cost Functions for Classification\n",
    "\n",
    "As with any gradient descent process, this one will need a cost function that the gradient descent can use to calculate its progress. In regression we generally use some measure of error like MSE/RMSE, but for classification we have a few options.\n",
    "\n",
    "One note is that the cost function isn't (really) a measure of accuracy (though it is related). The cost function is a measure of how far the model is from the \"correct\" answer, so the calculations inside of the cost function that measure the accuracy are calculating it based on how close the predictive model is to the correct answer, not just if it is right or wrong. \n",
    "\n",
    "#### Log Loss\n",
    "\n",
    "Log loss is a cost function that is used when we are doing binary classification. It is a measure of how far off our predictions are from the actual values. The formula for log loss is:\n",
    "\n",
    " $ \\min_{w} C \\sum_{i=1}^n \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right) + r(w). $\n",
    "\n",
    "Like the MSE/RMSE cost functions that we use in regression, log loss is a measure of how close our model's prediction is to the true value. Because we are predicting T/F classes, the \"true value\" is either 0 or 1; our model makes a prediction of a probability that the value is 1 (or the compliment of a prediction that the value is 0). The log loss is based on this difference, the \"more strongly\" the model predicts that something is True, if it is in actuality true, the lower the error for that prediction. On the flip side, if a value is really 0, the lower the probability the model predicts something is true, the lower the error.\n",
    "\n",
    "##### Log Loss by Example\n",
    "\n",
    "Let's say we have a model that predicts the probability that a value is 1. Our data has 8 values, one of which is in each row. We can walk through the log-loss calculation process by looking left to right in the table:\n",
    "\n",
    "![Log Loss](images/log_loss_ex.png \"Log Loss\" )\n",
    "\n",
    "<ul>\n",
    "<li> The \"Actual\" column is the true value of the data.\n",
    "<li> The \"Predicted Probabilities\" column is the predicted probability that the value is 1.\n",
    "<li> The \"Corrected Probabilities\" column is the predicted probability, expressed in terms of the correct value. \n",
    "    <ul>\n",
    "    <li> If the actual value is 1, the predicted probability is the correct probability.\n",
    "    <li> If the actual value is 0, the predicted probability is the compliment of the correct probability.\n",
    "    <li> This value is now a measure of \"how correct\" our prediction is - 1 is perfect, 0 is wrong, and the decimal values are a scale of correctness between the two.\n",
    "    </ul>\n",
    "<li> The \"Log\" column is the log of each of the corrected probabilities. \n",
    "</ul>\n",
    "\n",
    "As these values are negative, we calculate the total log loss by taking the average of the negative log values. If we were to work it all the way through, that is what the intial equation above is. \n",
    "\n",
    "This log loss is the cost function that the linear regression will use the gradient descent process to minimize, in just the same way that we used MSE/RMSE in linear regression. As with linear regression, the gradient calculation step of the process \"attributes\" the log-loss error back to each of the weights, and the gradient descent process uses that to update the weights. Whatever the weights are when the log loss is minimized is the best model for the data. The one odd thing about the log loss is that we have that conditional step where we treat the prediction differently based on the actual value of that instance. If we picture the log loss as a graph, it makes more sense:\n",
    "\n",
    "![Log Loss](images/logloss.png \"Log Loss\" )\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "One thing to note about logistic regression is that regularization is enabled by default - that default is L2, or Ridge, regualrization. We can change or disable regularization with a hyperparameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Is it a 3? \n",
    "\n",
    "We can build a simple model to classify if an image is a 3 or not using logistic regression. Making a model that is processing an image is the same as making a model that is processing text or numbers - we just need to make sure we understand the data:\n",
    "<ul>\n",
    "<li> The target is the label, is it a 3 or not.\n",
    "<li> The features are the pixels, the 64 values that make up the image.\n",
    "</ul>\n",
    "\n",
    "So we are doing a simple yes/no logistic regression classification - 64 features in our data, and a 1 or 0 target (is it a 3 or not).\n",
    "\n",
    "To make this simple, we'll make a new target value, one that matches the goal of our classification - 3 or not 3. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This dataset is one we can use as an exercise as we go through. It is a smaller version of the images that we are using. Most things translate pretty directly from the example. \n",
    "\n",
    "For now:\n",
    "<ul>\n",
    "<li>Load the data like we did with mnist. \n",
    "<li>Picture a digit, then a grid of digits. \n",
    "</ul>\n",
    "\n",
    "A solved example is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXERCISE\n",
    "#Load Data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images are 8 x 8, so the resolution is far worse (but the predictions are much faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDigit(digits.data[5:6], digits.target[5], size=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the dataset. We need to make a new target value, one that matches the goal of our classification - 3 or not 3. There are lots of ways to do the transformation, I just picked the first that came to mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_3 = (pd.Series(digits.target) == 3)\n",
    "X_train_dig, X_test_dig, y_train_dig, y_test_dig = train_test_split(digits.data, y_3, test_size=0.3, random_state=42)\n",
    "X_train_dig.shape, y_train_dig.shape, X_test_dig.shape, y_test_dig.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now in a \"normal\" format for logistic regression - 1 binary target value, and 784 features. We can proceed with the model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "estimator = LogisticRegression(n_jobs=-1)\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"log\", estimator)])\n",
    "\n",
    "pipe.fit(X_train_dig, y_train_dig.ravel())\n",
    "\n",
    "print(\"Testing Accuracy:\", pipe.score(X_train_dig, y_train_dig))\n",
    "print(\"Training Accuracy:\", pipe.score(X_test_dig, y_test_dig))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing a classification, we can revisit the confusion matrix to see a breakdown of how our errors occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "corr = confusion_matrix(y_test_dig,pipe.predict(X_test_dig))\n",
    "sns.heatmap(corr, annot=True, cbar=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Is it a 9 or 0?\n",
    "\n",
    "Try an example - is the value a 9 or a 0, or not? So if the value is 9 or 0, the target is 1, otherwise it is 0. You'll need some data prep for this one, not much though. \n",
    "\n",
    "<ul>\n",
    "<li> Value is 9 or 0 = 1. \n",
    "<li> Value is 1 through 8 = 0. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classifications - Softmax, One v All, One v One\n",
    "\n",
    "Logistic regression separated two classes, predictions are either labeled as a 1 or a 0. In reality, we often want to predict more than just yes/no questions. For example, if we are doing facial recognition we likely don't want to settle for saying \"yup, that is a person\", we want to be able to determine who that person is. \n",
    "\n",
    "When we looked at decision trees, they were capable of doing multiple classifications directly, no adjustments needed. Linear classifiers are different though, they only separate between two classes, so we need a different approach. \n",
    "\n",
    "![Multi Class](images/multi_class.png \"Multi Class\" )\n",
    "\n",
    "### Ways to do Multiple Classifications\n",
    "\n",
    "There are several different ways to do multiple classifications, here are a couple. \n",
    "\n",
    "#### One vs Rest\n",
    "\n",
    "One way to train a multiple classifier is to create a series of binary clasifiers, one for each outcome class vs \"the rest\". This is the default in sklearn's logistic regression. The end result is one classifier for each class. \n",
    "\n",
    "For our example: 1 vs not 1, 2 vs not 2, etc...\n",
    "\n",
    "#### One vs One\n",
    "\n",
    "Another method is to create a separate classifier for every combination of outcomes. This isn't implimented in sklearn's logistic regression but there is a class OneVsOneClassifier that allows you to plug in any classifier and the 1 vs 1 algorithm will be applied. When there are many classes, the number of classifiers and the volume of calculations can be very large, which is why we don't see this one often. \n",
    "\n",
    "For our example: 1 vs 2, 1 vs 3, 1 vs 4... 4 vs 5, 4 vs 6....\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "For logistic regression, we make these classifications using something called Softmax Regression, or Multinomial Logistic Regression. The idea behind this is pretty simple, we just calculate a score for each class and the highest score is the prediction. \n",
    "\n",
    "Softmax will get a bit of a deeper look when we get to neural networks, for now it is more or less a multi-way version of the sigmoid function that we are used to seeing in classfications. Rather than splitting an individual prediction into two possibilites like the sigmoid, the softmax breaks out an individual probability for each of the possible output classes. \n",
    "\n",
    "So if we are predicting between 3 classes - A, B, and C, a model that is predicting B with pretty high confidence might produce an output like:\n",
    "<ul>\n",
    "<li> A - .228\n",
    "<li> B - .619\n",
    "<li> C - .153\n",
    "</ul>\n",
    "\n",
    "If the true answer is B, we'd have a real distribution that looks like:\n",
    "<ul>\n",
    "<li> A - 0\n",
    "<li> B - 1\n",
    "<li> C - 0\n",
    "</ul>\n",
    "I.e. the probability of it being B is 100%, since that's the true value; the probability of A or C is 0, because it isn't either of those. \n",
    "\n",
    "#### Cross Entropy\n",
    "\n",
    "Cross entropy is a very common loss function used when doing multiple classifications. \n",
    "\n",
    "The cross entropy loss function compares the real distribution to the expected one, and generates a metric for loss (like any other loss function). It will compare the predictions produced by the softmax to the true value and then calculate the loss. If we take the example from above, the cross entropy can be calculated with the formula:\n",
    "\n",
    "![Cross Entropy](images/cross_ent.png \"Cross Entropy\" )\n",
    "\n",
    "Resulting in an actual calculation for the example above of:\n",
    "\n",
    " $ H = - (0.0*ln(0.228) + 1.0*ln(0.619) + 0.0*ln(0.153)) = 0.479 $\n",
    "\n",
    "The gradient descent uses this amount of loss as we'd expect it to, and the training process just keeps repeating until we converge on a minimum amount of loss or run out of iterations to try. There is an expanded explaination here: https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932#41990932 \n",
    "\n",
    "Overll, there is a bit of a two step process as shown above, the logits are calculated, then the softmax is applied to get the probabilities.\n",
    "\n",
    "![Softmax](images/softmax_multi.png \"Softmax\" )\n",
    "\n",
    "We will spend more time on the details of loss stuff in neural networks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver\n",
    "One of the hyperparameters in the logistic regression call is the solver. This defines the method that the algorithm uses to do the gradient descent. The short answer is that it isn't something that we need to worry about too much unless we are looking for optimizing speed with large datasets. The slightly less short answer is that lbfgs (the default) is probably OK for most cases and either liblinear or saga (large datasets) if we want to feature select using L1 regularization. In any case, don't obsess over this. The documentation provides a little table for selecting an appropriate solver:\n",
    "\n",
    "![Logistic Regression Solvers](images/log_reg_solvers.png \"Logistic Regression Solvers\" )\n",
    "\n",
    "It isn't uncommon to have options like this for some of the model building algorithms. Normally, they are different methods for doing the internal calculations needed to create the model, with the prime difference being efficiency with different types of data. \n",
    "\n",
    "<b>Note:</b> there are also \"multi-label\" classifications, those are classifications that can assign more than one outcome class to a single observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Multi-Class Classification\n",
    "\n",
    "We can now attempt one of the multi-class classification methods. Most of the code is similar, the main difference here is that we end up with 10 classes instead of the two we are used to. The different methods of doing the multi-class classification can be set via a hyperparameter, we'll try a couple in a grid search.\n",
    "\n",
    "Our target here is the numerical value, without any transformations, so we have 10 output classes. Other than that, the code is pretty much the same as the binary classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify the digits\n",
    "\n",
    "# This currently takes the first 10000 images, change commenting to take all\n",
    "# It may take a long time with all data, especially if there is lots of grid searching and CV\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.3)\n",
    "\n",
    "# Scale inputs in a pipe\n",
    "scaler = MinMaxScaler()\n",
    "estimator = LogisticRegression(n_jobs=-1, solver=\"saga\", max_iter=10000)\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"log\", estimator)])\n",
    "\n",
    "# Try different classifications for the multiple classes\n",
    "params = {'log__multi_class':[\"ovr\",\"multinomial\"]}\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid=params, cv=3, n_jobs=-1) \n",
    "clf.fit(X_train, y_train.ravel())\n",
    "best = clf.best_estimator_\n",
    "train_preds = best.predict(X_train)\n",
    "print(best)\n",
    "print(best.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results\n",
    "\n",
    "We can look at the results of our classification, the confusion matrix still works, it is just a little more complex with multiple classes. The intersection shows us the number of times that there was an error between those two classes. \n",
    "\n",
    "<b>Note:</b> we can change the commenting below to see half/full of the heatmap. I think that half is easier to understand, since the errors are easier to see and tend to be symmetrical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "preds = best.predict(X_test)\n",
    "\n",
    "corr = confusion_matrix(y_test,preds)\n",
    "#mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "#sns.heatmap(corr, annot=True, mask=mask)\n",
    "sns.heatmap(corr, annot=True, cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the heatmap to see how often we are wrong with different combinations of digits. For example, 7 and 9 having frequent errors isn't every surprising. \n",
    "\n",
    "The confustion matrix is a 2D array of counts of errors. We can extract the values for each number to look at the differences in performance for each digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr)\n",
    "\n",
    "# Grab each row, which represents one digit, and add up the errors. \n",
    "# Be sure to exclude the \"spine\" of counts. \n",
    "ers = []\n",
    "for i in range(len(corr)):\n",
    "    num = corr[i]\n",
    "    #print(num)\n",
    "    before = num[:i]\n",
    "    after = num[i+1:]\n",
    "    #print(before, after)\n",
    "    tmp_err = np.sum(before) + np.sum(after)\n",
    "    ers.append(tmp_err)\n",
    "print(ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Errors Per Number\n",
    "sns.barplot(y=ers, x=[0,1,2,3,4,5,6,7,8,9])\n",
    "plt.title(\"Errors Per Number\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution\n",
    "\n",
    "As we might expect the errors are skewed towards numbers that look similar, like a 6 and an 8. There's no intuitive way to know what we can do with our modeling to improve this - maybe some different algorithm gives better results, or a different set of HPs that we can find with a grid search. \n",
    "\n",
    "Most likely we'll need to do some processing of the data to understand them a little better as images rather than just tables of pixels. Image processing is something we'll look into a little bit more later on in the course. Real image processing for machine learning often involves doing something to process the data and extract features that are more useful; for example, with navigation or vision of the \"real world\", image processing steps are often used to extract features like edges, which can define things like the boundaries of roads or cars. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Multi-Class Classification\n",
    "\n",
    "Try with a slightly more simple example - an 8 x 8 version of the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "Xd, yd = digits[\"data\"], digits[\"target\"]\n",
    "print(Xd.shape)\n",
    "print(yd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at a digit\n",
    "showDigit(Xd[12:13], yd[12], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify the digits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a confusion matrix, I'll use the mask here to only show half of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too Simple? Try on the MNIST Dataset\n",
    "\n",
    "Do a multi-class classification on the MNIST dataset. You may need to take a sample of the data, as it can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
