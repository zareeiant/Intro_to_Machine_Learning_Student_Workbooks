{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Structured Data\n",
    "\n",
    "When dealing with large amounts of structured text data we can also do some stuff to speed things up, though there are some key differences that lessen our toolkit:\n",
    "<ul>\n",
    "<li> Data manipulation (filtering out features, customized data cleanup, etc...) is far easier to do in a tabular format like a dataframe. If there is going to be a lot of that, and the data is really large, we can do the 'manual' prep separately, write the data to a file, and then read it in again as ready-to-use data. </li>\n",
    "<li> As the data gets really large, most real life scenarios will either use big data approaches like Spark, or store structured data in a DB. That's the 'real' way to deal with large amounts of strucutred data, so there are not as many easy to use tools for this as we find with images. </li>\n",
    "<li> Further to the two ponts above, if the dataset is a CSV, we can likely load it into memory in its entirety as \"too many rows to fit in memory\" and \"this data is stored in a CSV file\" tend not to come around together all that often in a situation where there is actual infrastructure. </li>\n",
    "<li> Really large amounts of text can be broken into multiple smaller files, then we load a file at a time, similar to how we deal with images. This is common with NLP text, much more so than structured data. </li>\n",
    "<li> There is often an assumption that when needing to deal with large amounts of structured CSV data that we have the data already split into training and validation sets. This makes sense, as \n",
    "</ul>\n",
    "\n",
    "On the whole, dealing with large amounts of structured data tends to not be as large of an issue to be solved as dealing with large amounts of unstrucutured data in a non-big data environment. This is because huge data goes to big data strategies, or at least a DB, less huge data can just fit in memory and be dealt with how we have dealt with all other CSV based data. \n",
    "\n",
    "## TensorFlow Datasets\n",
    "\n",
    "Tensorflow Datasets are something that we used when loading image files from disk, as loading all of the data at once can be impossible for larger datasets. These datasets serve the same function as a regular dataframe for modle training purposes, but they are designed more to be able to efficiently load large amounts of data from disk than to allow easy viewing and manipulation of the data. \n",
    "\n",
    "Tensorflow datasets allow us to set several options on how the data is loaded, that we can use to make a dataset that is more efficient for our purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for Structured CSV\n",
    "\n",
    "The function below reads CSV data from disk and generates training and validation datasets that we can feed to our model. We also add batching, shuffle the training data, and use prefetch to make the data loading process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the CSV data and create a tf.data.Dataset object\n",
    "def create_dataset(csv_path, batch_size=32, buffer_size=1024, validation_split=0.2, shuffle=True, start=None):\n",
    "    # Load the CSV data\n",
    "    with open(csv_path) as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        header = next(csv_reader)\n",
    "        feature_names = header[start:-1]\n",
    "        label_name = header[-1]\n",
    "        features = []\n",
    "        labels = []\n",
    "        for row in csv_reader:\n",
    "            features.append([float(x) for x in row[start:-1]])\n",
    "            labels.append(float(row[-1]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    split_idx = int(len(features) * (1.0 - validation_split))\n",
    "    train_features, train_labels = features[:split_idx], labels[:split_idx]\n",
    "    val_features, val_labels = features[split_idx:], labels[split_idx:]\n",
    "\n",
    "    # Create a tf.data.Dataset object for the training data\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    train_ds = train_ds.cache()\n",
    "    if shuffle:\n",
    "        train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "    # Create a tf.data.Dataset object for the validation data\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.cache()\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "BASE_EPOCHS  = 20\n",
    "VAL_SPLIT = 0.2\n",
    "DIABETES_CSV_PATH = 'diabetes.csv'\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akeems/.keras/datasets/diabetes.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DIABETES_CSV_PATH):\n",
    "    url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/diabetes.csv'\n",
    "    d_path = tf.keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(d_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple and Small Example\n",
    "\n",
    "We can test the generator on a small file. \n",
    "\n",
    "<b>Note:</b> with small examples, we won't really see any advantage in terms of speed as we can probably just load the data into memory without concern, no matter what. This starts to matter more when dealing with large files, where the disk access time can actually add up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 11:01:43.272105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 10ms/step - loss: 1.4923 - accuracy: 0.5717 - val_loss: 0.8109 - val_accuracy: 0.5779\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8014 - accuracy: 0.6189 - val_loss: 0.7421 - val_accuracy: 0.5584\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.6645 - val_loss: 0.9670 - val_accuracy: 0.6299\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8389 - accuracy: 0.6319 - val_loss: 0.7448 - val_accuracy: 0.6364\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7085 - accuracy: 0.6580 - val_loss: 0.9807 - val_accuracy: 0.5130\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7467 - accuracy: 0.6336 - val_loss: 0.7956 - val_accuracy: 0.6039\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.6873 - val_loss: 0.7102 - val_accuracy: 0.6494\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6743 - val_loss: 0.7289 - val_accuracy: 0.6494\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.5833 - accuracy: 0.6906 - val_loss: 0.7175 - val_accuracy: 0.7078\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6857 - val_loss: 0.6655 - val_accuracy: 0.6169\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7598 - accuracy: 0.6482 - val_loss: 0.7164 - val_accuracy: 0.5909\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7054 - accuracy: 0.6694 - val_loss: 0.7504 - val_accuracy: 0.5584\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6938 - val_loss: 0.9148 - val_accuracy: 0.4805\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5927 - accuracy: 0.6759 - val_loss: 0.7070 - val_accuracy: 0.6364\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5529 - accuracy: 0.7199 - val_loss: 0.7520 - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5688 - accuracy: 0.7182 - val_loss: 0.6437 - val_accuracy: 0.6948\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.5589 - accuracy: 0.7264 - val_loss: 0.7128 - val_accuracy: 0.6558\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.6808 - val_loss: 0.6556 - val_accuracy: 0.6558\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7880 - accuracy: 0.6792 - val_loss: 0.8632 - val_accuracy: 0.4675\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.6109 - accuracy: 0.6759 - val_loss: 0.7581 - val_accuracy: 0.6104\n",
      "DS Training time: 2.965869188308716 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "train_ds, val_ds = create_dataset(d_path, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 1s 12ms/step - loss: 1.4143 - accuracy: 0.5847 - val_loss: 0.8531 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.8895 - accuracy: 0.6270 - val_loss: 1.1149 - val_accuracy: 0.6494\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.9542 - accuracy: 0.6319 - val_loss: 1.2748 - val_accuracy: 0.4675\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.8490 - accuracy: 0.6596 - val_loss: 0.6391 - val_accuracy: 0.6623\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6249 - accuracy: 0.7020 - val_loss: 0.6554 - val_accuracy: 0.6623\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5953 - accuracy: 0.6971 - val_loss: 0.6370 - val_accuracy: 0.6429\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5860 - accuracy: 0.7085 - val_loss: 0.7887 - val_accuracy: 0.6558\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6232 - accuracy: 0.7101 - val_loss: 0.6309 - val_accuracy: 0.6753\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6175 - accuracy: 0.7134 - val_loss: 0.6334 - val_accuracy: 0.6948\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.7003 - val_loss: 0.8738 - val_accuracy: 0.5325\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.6409 - accuracy: 0.6971 - val_loss: 0.6828 - val_accuracy: 0.5519\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.5952 - accuracy: 0.7182 - val_loss: 0.6345 - val_accuracy: 0.6818\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.7215 - val_loss: 0.6539 - val_accuracy: 0.6364\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.5828 - accuracy: 0.7085 - val_loss: 0.6414 - val_accuracy: 0.6623\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6064 - accuracy: 0.7052 - val_loss: 0.7094 - val_accuracy: 0.6558\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5515 - accuracy: 0.7362 - val_loss: 0.6573 - val_accuracy: 0.6494\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.5531 - accuracy: 0.7280 - val_loss: 0.6294 - val_accuracy: 0.6883\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7854 - accuracy: 0.6824 - val_loss: 0.7278 - val_accuracy: 0.6169\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6262 - accuracy: 0.7020 - val_loss: 0.6766 - val_accuracy: 0.6688\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7134 - accuracy: 0.7085 - val_loss: 0.6155 - val_accuracy: 0.6688\n",
      "DS Training time: 2.6833109855651855 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time dataframe for comparison\n",
    "df_small = pd.read_csv(d_path)\n",
    "df_small_y = df_small[\"Outcome\"]\n",
    "df_small_X = df_small.drop(columns=[\"Outcome\"])\n",
    "width = df_small_X.shape[1]\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(width,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(x=df_small_X, y=df_small_y, epochs=BASE_EPOCHS, validation_split=0.2, batch_size=BATCH_SIZE)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Example\n",
    "\n",
    "We can download a larger file, and try it out. We ill also use the .cache() method to cache the data in memory, so that we don't have to reload it every time we run the code. This CSV file is roughly 150mb in size, so it is large enough to be noticable when we need to load the entire thing, but small enough to fit in memory. For most CSV data that we might encounter, this is probably a good approach - most systems can handle the memory demands of the CSV file size we might see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akeems/.keras/datasets/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "\n",
    "zip_name = 'fraud.zip'\n",
    "if not os.path.exists(zip_name):\n",
    "    url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv'\n",
    "    zip_path = tf.keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0204 - accuracy: 0.9990 - val_loss: 0.0056 - val_accuracy: 0.9996\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0103 - accuracy: 0.9993 - val_loss: 0.0058 - val_accuracy: 0.9996\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9996\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9996\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9996\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 0.9996\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 10s 1ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 11s 1ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0061 - val_accuracy: 0.9996\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0084 - val_accuracy: 0.9996\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 10s 1ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0040 - val_accuracy: 0.9995\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0044 - val_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0040 - val_accuracy: 0.9996\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0036 - val_accuracy: 0.9995\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0051 - val_accuracy: 0.9995\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 0.0045 - val_accuracy: 0.9995\n",
      "Time to fit:  250.69341111183167\n"
     ]
    }
   ],
   "source": [
    "#big_file = \"/Users/akeems/.keras/datasets/creditcard.csv\"\n",
    "big_file = zip_path\n",
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "train_ds, val_ds = create_dataset(big_file, start=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 18.3415 - accuracy: 0.9955 - val_loss: 5.7364 - val_accuracy: 0.9987\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 4.5446 - accuracy: 0.9960 - val_loss: 4.9170 - val_accuracy: 0.9987\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.4800 - accuracy: 0.9974 - val_loss: 0.1178 - val_accuracy: 0.9987\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0448 - accuracy: 0.9979 - val_loss: 0.0118 - val_accuracy: 0.9987\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0162 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0202 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0277 - accuracy: 0.9980 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0237 - accuracy: 0.9979 - val_loss: 0.0103 - val_accuracy: 0.9987\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0235 - accuracy: 0.9979 - val_loss: 0.0111 - val_accuracy: 0.9987\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0205 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0194 - accuracy: 0.9978 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0229 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0202 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0185 - accuracy: 0.9979 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0164 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0233 - accuracy: 0.9981 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.0103 - val_accuracy: 0.9987\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0157 - accuracy: 0.9982 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0150 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0156 - accuracy: 0.9982 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Time to fit:  267.8153579235077\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv(big_file)\n",
    "df_large_y = df_large[\"Class\"]\n",
    "df_large_X = df_large.drop(columns={\"Class\"})\n",
    "width = df_large_X.shape[1]\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(width,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "start = time.time()\n",
    "model.fit(x=df_large_X, y=df_large_y, epochs=BASE_EPOCHS, validation_split=VAL_SPLIT, batch_size=BATCH_SIZE)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to Dataset\n",
    "\n",
    "If we have a dataframe we can convert it to a dataset using the from_tensor_slices() method. Manipulating the data in a dataframe is far easier, so we can prep in a df then convert to a dataset. The function below creates a dataset from a dataframe, long with a few of the other things we commonly want to do in our data prep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_dataset(df, target=\"target\", val_split=0.2, batch_size=32):\n",
    "    # Splitting the dataframe into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)\n",
    "\n",
    "    # Extracting the target variable from the dataframes\n",
    "    train_y = train_df.pop(target)\n",
    "    val_y = val_df.pop(target)\n",
    "\n",
    "    # Converting the target variable to categorical if necessary\n",
    "    num_classes = len(train_y.unique())\n",
    "    if num_classes > 2:\n",
    "        train_y = to_categorical(train_y, num_classes)\n",
    "        val_y = to_categorical(val_y, num_classes)\n",
    "\n",
    "    # Creating a tf.data.Dataset for training and validation sets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_df.values, train_y))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_df.values, val_y))\n",
    "\n",
    "    train_ds =train_ds.cache()\n",
    "    val_ds = val_ds.cache()\n",
    "\n",
    "    # Shuffling and batching the datasets\n",
    "    train_ds = train_ds.shuffle(len(train_df)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 19s 2ms/step - loss: 10.5821 - accuracy: 0.9963 - val_loss: 5.5160 - val_accuracy: 0.9983\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 1.0853 - accuracy: 0.9965 - val_loss: 0.0512 - val_accuracy: 0.9901\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.1486 - accuracy: 0.9978 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0183 - accuracy: 0.9983 - val_loss: 0.0135 - val_accuracy: 0.9983\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0148 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 20s 3ms/step - loss: 0.0138 - accuracy: 0.9983 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0295 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0238 - accuracy: 0.9980 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 21s 3ms/step - loss: 0.0267 - accuracy: 0.9980 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0322 - accuracy: 0.9979 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0298 - accuracy: 0.9982 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0180 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0173 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0147 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 19s 3ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Time to fit:  348.6873939037323\n"
     ]
    }
   ],
   "source": [
    "train_ds_df, val_ds_df = get_keras_dataset(df_large, target=\"Class\", val_split=VAL_SPLIT, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds_df, epochs=BASE_EPOCHS, validation_data=val_ds_df)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars DataFrame\n",
    "\n",
    "We can also use the faster and more efficient Polars DataFrame to load the data. This is a DataFrame that is written in Rust, and is much faster than Pandas. Polars dataframes aren't promised to be a one-to-one replacement for Pandas, but they are very similar, and can be used in most cases where Pandas is used with few, if any, changes.\n",
    "\n",
    "#### Polars Specifics\n",
    "\n",
    "Polars offers a fair bit of stuff for performance, as that is it's main selling point. Among them:\n",
    "<ul>\n",
    "<li> Low memory parameter - this will try to load the data in a way that uses less memory, but may be slower. </li>\n",
    "<li> Lazy execution - Polars has options to work lazily, which means that it won't actually do work like loading data until it is needed. </li>\n",
    "<li> Parallel execution - Polars can use multiple threads to do work, which can speed things up. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install polars\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       ".pl-dataframe > thead > tr > th {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<table border=\"1\" class=\"dataframe pl-dataframe\">\n",
       "<small>shape: (5, 31)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "Time\n",
       "</th>\n",
       "<th>\n",
       "V1\n",
       "</th>\n",
       "<th>\n",
       "V2\n",
       "</th>\n",
       "<th>\n",
       "V3\n",
       "</th>\n",
       "<th>\n",
       "V4\n",
       "</th>\n",
       "<th>\n",
       "V5\n",
       "</th>\n",
       "<th>\n",
       "V6\n",
       "</th>\n",
       "<th>\n",
       "V7\n",
       "</th>\n",
       "<th>\n",
       "V8\n",
       "</th>\n",
       "<th>\n",
       "V9\n",
       "</th>\n",
       "<th>\n",
       "V10\n",
       "</th>\n",
       "<th>\n",
       "V11\n",
       "</th>\n",
       "<th>\n",
       "V12\n",
       "</th>\n",
       "<th>\n",
       "V13\n",
       "</th>\n",
       "<th>\n",
       "V14\n",
       "</th>\n",
       "<th>\n",
       "V15\n",
       "</th>\n",
       "<th>\n",
       "V16\n",
       "</th>\n",
       "<th>\n",
       "V17\n",
       "</th>\n",
       "<th>\n",
       "V18\n",
       "</th>\n",
       "<th>\n",
       "V19\n",
       "</th>\n",
       "<th>\n",
       "V20\n",
       "</th>\n",
       "<th>\n",
       "V21\n",
       "</th>\n",
       "<th>\n",
       "V22\n",
       "</th>\n",
       "<th>\n",
       "V23\n",
       "</th>\n",
       "<th>\n",
       "V24\n",
       "</th>\n",
       "<th>\n",
       "V25\n",
       "</th>\n",
       "<th>\n",
       "V26\n",
       "</th>\n",
       "<th>\n",
       "V27\n",
       "</th>\n",
       "<th>\n",
       "V28\n",
       "</th>\n",
       "<th>\n",
       "Amount\n",
       "</th>\n",
       "<th>\n",
       "Class\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "-1.359807\n",
       "</td>\n",
       "<td>\n",
       "-0.072781\n",
       "</td>\n",
       "<td>\n",
       "2.536347\n",
       "</td>\n",
       "<td>\n",
       "1.378155\n",
       "</td>\n",
       "<td>\n",
       "-0.338321\n",
       "</td>\n",
       "<td>\n",
       "0.462388\n",
       "</td>\n",
       "<td>\n",
       "0.239599\n",
       "</td>\n",
       "<td>\n",
       "0.098698\n",
       "</td>\n",
       "<td>\n",
       "0.363787\n",
       "</td>\n",
       "<td>\n",
       "0.090794\n",
       "</td>\n",
       "<td>\n",
       "-0.5516\n",
       "</td>\n",
       "<td>\n",
       "-0.617801\n",
       "</td>\n",
       "<td>\n",
       "-0.99139\n",
       "</td>\n",
       "<td>\n",
       "-0.311169\n",
       "</td>\n",
       "<td>\n",
       "1.468177\n",
       "</td>\n",
       "<td>\n",
       "-0.470401\n",
       "</td>\n",
       "<td>\n",
       "0.207971\n",
       "</td>\n",
       "<td>\n",
       "0.025791\n",
       "</td>\n",
       "<td>\n",
       "0.403993\n",
       "</td>\n",
       "<td>\n",
       "0.251412\n",
       "</td>\n",
       "<td>\n",
       "-0.018307\n",
       "</td>\n",
       "<td>\n",
       "0.277838\n",
       "</td>\n",
       "<td>\n",
       "-0.110474\n",
       "</td>\n",
       "<td>\n",
       "0.066928\n",
       "</td>\n",
       "<td>\n",
       "0.128539\n",
       "</td>\n",
       "<td>\n",
       "-0.189115\n",
       "</td>\n",
       "<td>\n",
       "0.133558\n",
       "</td>\n",
       "<td>\n",
       "-0.021053\n",
       "</td>\n",
       "<td>\n",
       "149.62\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "1.191857\n",
       "</td>\n",
       "<td>\n",
       "0.266151\n",
       "</td>\n",
       "<td>\n",
       "0.16648\n",
       "</td>\n",
       "<td>\n",
       "0.448154\n",
       "</td>\n",
       "<td>\n",
       "0.060018\n",
       "</td>\n",
       "<td>\n",
       "-0.082361\n",
       "</td>\n",
       "<td>\n",
       "-0.078803\n",
       "</td>\n",
       "<td>\n",
       "0.085102\n",
       "</td>\n",
       "<td>\n",
       "-0.255425\n",
       "</td>\n",
       "<td>\n",
       "-0.166974\n",
       "</td>\n",
       "<td>\n",
       "1.612727\n",
       "</td>\n",
       "<td>\n",
       "1.065235\n",
       "</td>\n",
       "<td>\n",
       "0.489095\n",
       "</td>\n",
       "<td>\n",
       "-0.143772\n",
       "</td>\n",
       "<td>\n",
       "0.635558\n",
       "</td>\n",
       "<td>\n",
       "0.463917\n",
       "</td>\n",
       "<td>\n",
       "-0.114805\n",
       "</td>\n",
       "<td>\n",
       "-0.183361\n",
       "</td>\n",
       "<td>\n",
       "-0.145783\n",
       "</td>\n",
       "<td>\n",
       "-0.069083\n",
       "</td>\n",
       "<td>\n",
       "-0.225775\n",
       "</td>\n",
       "<td>\n",
       "-0.638672\n",
       "</td>\n",
       "<td>\n",
       "0.101288\n",
       "</td>\n",
       "<td>\n",
       "-0.339846\n",
       "</td>\n",
       "<td>\n",
       "0.16717\n",
       "</td>\n",
       "<td>\n",
       "0.125895\n",
       "</td>\n",
       "<td>\n",
       "-0.008983\n",
       "</td>\n",
       "<td>\n",
       "0.014724\n",
       "</td>\n",
       "<td>\n",
       "2.69\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-1.358354\n",
       "</td>\n",
       "<td>\n",
       "-1.340163\n",
       "</td>\n",
       "<td>\n",
       "1.773209\n",
       "</td>\n",
       "<td>\n",
       "0.37978\n",
       "</td>\n",
       "<td>\n",
       "-0.503198\n",
       "</td>\n",
       "<td>\n",
       "1.800499\n",
       "</td>\n",
       "<td>\n",
       "0.791461\n",
       "</td>\n",
       "<td>\n",
       "0.247676\n",
       "</td>\n",
       "<td>\n",
       "-1.514654\n",
       "</td>\n",
       "<td>\n",
       "0.207643\n",
       "</td>\n",
       "<td>\n",
       "0.624501\n",
       "</td>\n",
       "<td>\n",
       "0.066084\n",
       "</td>\n",
       "<td>\n",
       "0.717293\n",
       "</td>\n",
       "<td>\n",
       "-0.165946\n",
       "</td>\n",
       "<td>\n",
       "2.345865\n",
       "</td>\n",
       "<td>\n",
       "-2.890083\n",
       "</td>\n",
       "<td>\n",
       "1.109969\n",
       "</td>\n",
       "<td>\n",
       "-0.121359\n",
       "</td>\n",
       "<td>\n",
       "-2.261857\n",
       "</td>\n",
       "<td>\n",
       "0.52498\n",
       "</td>\n",
       "<td>\n",
       "0.247998\n",
       "</td>\n",
       "<td>\n",
       "0.771679\n",
       "</td>\n",
       "<td>\n",
       "0.909412\n",
       "</td>\n",
       "<td>\n",
       "-0.689281\n",
       "</td>\n",
       "<td>\n",
       "-0.327642\n",
       "</td>\n",
       "<td>\n",
       "-0.139097\n",
       "</td>\n",
       "<td>\n",
       "-0.055353\n",
       "</td>\n",
       "<td>\n",
       "-0.059752\n",
       "</td>\n",
       "<td>\n",
       "378.66\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-0.966272\n",
       "</td>\n",
       "<td>\n",
       "-0.185226\n",
       "</td>\n",
       "<td>\n",
       "1.792993\n",
       "</td>\n",
       "<td>\n",
       "-0.863291\n",
       "</td>\n",
       "<td>\n",
       "-0.010309\n",
       "</td>\n",
       "<td>\n",
       "1.247203\n",
       "</td>\n",
       "<td>\n",
       "0.237609\n",
       "</td>\n",
       "<td>\n",
       "0.377436\n",
       "</td>\n",
       "<td>\n",
       "-1.387024\n",
       "</td>\n",
       "<td>\n",
       "-0.054952\n",
       "</td>\n",
       "<td>\n",
       "-0.226487\n",
       "</td>\n",
       "<td>\n",
       "0.178228\n",
       "</td>\n",
       "<td>\n",
       "0.507757\n",
       "</td>\n",
       "<td>\n",
       "-0.287924\n",
       "</td>\n",
       "<td>\n",
       "-0.631418\n",
       "</td>\n",
       "<td>\n",
       "-1.059647\n",
       "</td>\n",
       "<td>\n",
       "-0.684093\n",
       "</td>\n",
       "<td>\n",
       "1.965775\n",
       "</td>\n",
       "<td>\n",
       "-1.232622\n",
       "</td>\n",
       "<td>\n",
       "-0.208038\n",
       "</td>\n",
       "<td>\n",
       "-0.1083\n",
       "</td>\n",
       "<td>\n",
       "0.005274\n",
       "</td>\n",
       "<td>\n",
       "-0.190321\n",
       "</td>\n",
       "<td>\n",
       "-1.175575\n",
       "</td>\n",
       "<td>\n",
       "0.647376\n",
       "</td>\n",
       "<td>\n",
       "-0.221929\n",
       "</td>\n",
       "<td>\n",
       "0.062723\n",
       "</td>\n",
       "<td>\n",
       "0.061458\n",
       "</td>\n",
       "<td>\n",
       "123.5\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "2\n",
       "</td>\n",
       "<td>\n",
       "-1.158233\n",
       "</td>\n",
       "<td>\n",
       "0.877737\n",
       "</td>\n",
       "<td>\n",
       "1.548718\n",
       "</td>\n",
       "<td>\n",
       "0.403034\n",
       "</td>\n",
       "<td>\n",
       "-0.407193\n",
       "</td>\n",
       "<td>\n",
       "0.095921\n",
       "</td>\n",
       "<td>\n",
       "0.592941\n",
       "</td>\n",
       "<td>\n",
       "-0.270533\n",
       "</td>\n",
       "<td>\n",
       "0.817739\n",
       "</td>\n",
       "<td>\n",
       "0.753074\n",
       "</td>\n",
       "<td>\n",
       "-0.822843\n",
       "</td>\n",
       "<td>\n",
       "0.538196\n",
       "</td>\n",
       "<td>\n",
       "1.345852\n",
       "</td>\n",
       "<td>\n",
       "-1.11967\n",
       "</td>\n",
       "<td>\n",
       "0.175121\n",
       "</td>\n",
       "<td>\n",
       "-0.451449\n",
       "</td>\n",
       "<td>\n",
       "-0.237033\n",
       "</td>\n",
       "<td>\n",
       "-0.038195\n",
       "</td>\n",
       "<td>\n",
       "0.803487\n",
       "</td>\n",
       "<td>\n",
       "0.408542\n",
       "</td>\n",
       "<td>\n",
       "-0.009431\n",
       "</td>\n",
       "<td>\n",
       "0.798278\n",
       "</td>\n",
       "<td>\n",
       "-0.137458\n",
       "</td>\n",
       "<td>\n",
       "0.141267\n",
       "</td>\n",
       "<td>\n",
       "-0.20601\n",
       "</td>\n",
       "<td>\n",
       "0.502292\n",
       "</td>\n",
       "<td>\n",
       "0.219422\n",
       "</td>\n",
       "<td>\n",
       "0.215153\n",
       "</td>\n",
       "<td>\n",
       "69.99\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (5, 31)\n",
       "┌──────┬───────────┬───────────┬──────────┬─────┬───────────┬───────────┬────────┬───────┐\n",
       "│ Time ┆ V1        ┆ V2        ┆ V3       ┆ ... ┆ V27       ┆ V28       ┆ Amount ┆ Class │\n",
       "│ ---  ┆ ---       ┆ ---       ┆ ---      ┆     ┆ ---       ┆ ---       ┆ ---    ┆ ---   │\n",
       "│ i64  ┆ f64       ┆ f64       ┆ f64      ┆     ┆ f64       ┆ f64       ┆ f64    ┆ i64   │\n",
       "╞══════╪═══════════╪═══════════╪══════════╪═════╪═══════════╪═══════════╪════════╪═══════╡\n",
       "│ 0    ┆ -1.359807 ┆ -0.072781 ┆ 2.536347 ┆ ... ┆ 0.133558  ┆ -0.021053 ┆ 149.62 ┆ 0     │\n",
       "│ 0    ┆ 1.191857  ┆ 0.266151  ┆ 0.16648  ┆ ... ┆ -0.008983 ┆ 0.014724  ┆ 2.69   ┆ 0     │\n",
       "│ 1    ┆ -1.358354 ┆ -1.340163 ┆ 1.773209 ┆ ... ┆ -0.055353 ┆ -0.059752 ┆ 378.66 ┆ 0     │\n",
       "│ 1    ┆ -0.966272 ┆ -0.185226 ┆ 1.792993 ┆ ... ┆ 0.062723  ┆ 0.061458  ┆ 123.5  ┆ 0     │\n",
       "│ 2    ┆ -1.158233 ┆ 0.877737  ┆ 1.548718 ┆ ... ┆ 0.219422  ┆ 0.215153  ┆ 69.99  ┆ 0     │\n",
       "└──────┴───────────┴───────────┴──────────┴─────┴───────────┴───────────┴────────┴───────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file at zip path into a polars dataframe\n",
    "df_polar = pl.read_csv(zip_path, ignore_errors=True, low_memory=True)\n",
    "df_polar.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polars\n",
    "\n",
    "Polars doesn't have the same native support in TensorFlow as Pandas does, so we need to convert it to a Pandas dataframe or an array to feed it into any models. One thing that may be useful with Polars would be to split a very large csv into multiple smallers ones, that could then be loaded one at a time. Something like the function below could be adapted to load a csv into a Polars dataframe, do whatever data manipulation is needed, then write it out to several smaller csv files. The make_csv_dataset is able to natively read in multiple csv files. \n",
    "\n",
    "<b>Note:</b> If we were actually doing something like this, it is likely easier to do a train-validation-test split as we write the output into different subfolders. Manipulating data is easier in a dataframe than a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !mkdir polar_out\n",
    "  !mkdir polar_out/train\n",
    "  !mkdir polar_out/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "split = 0.2\n",
    "#Shuffle data. Takes a peak of 2x memory to do so. \n",
    "df_polar = df_polar.sample(frac=1.0)\n",
    "for frame in df_polar.iter_slices(n_rows=BATCH_SIZE):\n",
    "    record_batch = frame\n",
    "    if i % 5 == 0:\n",
    "        fname = \"polar_out/val/data_{}.csv\".format(i)\n",
    "    else:\n",
    "        fname = \"polar_out/train/data_{}.csv\".format(i)\n",
    "    record_batch.write_csv(fname)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Folder\n",
    "\n",
    "We can create datasets from a folder of csv files. This is useful if we have a large csv file that we have split into multiple smaller ones. We can utilize any of the tuning things like cache and batch size to control the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern = \"polar_out/train/*.csv\",\n",
    "            batch_size=64, \n",
    "            num_epochs=BASE_EPOCHS,\n",
    "            num_parallel_reads=20,\n",
    "            shuffle_buffer_size=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "We can also make a generator for the above files. This one loads each file as one batch, so it fits with the idea above on embedding all the processing into the step that writes the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVGenerator(Sequence):\n",
    "    def __init__(self, folder_path, shuffle=True):\n",
    "        self.folder_path = folder_path\n",
    "        self.shuffle = shuffle\n",
    "        self.files = sorted(os.listdir(self.folder_path))\n",
    "        self.indexes = np.arange(len(self.files))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = os.path.join(self.folder_path, self.files[index])\n",
    "        data = pd.read_csv(file_path)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "       \n",
    "        return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CSVGenerator(\"polar_out/train\")\n",
    "val_generator = CSVGenerator(\"polar_out/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "228/228 [==============================] - 5s 16ms/step - loss: nan - accuracy: 0.9939 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 4/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 7/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 3s 13ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 4s 17ms/step - loss: nan - accuracy: 0.9983 - val_loss: nan - val_accuracy: 0.9983\n",
      "Time to fit:  65.70329189300537\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_generator, epochs=BASE_EPOCHS, validation_data=val_generator, steps_per_epoch=len(train_generator))\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
