{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Sklearn provides a tool for stiching together some of the data preprocessing steps that go into preparing our data, such as encoding categorical variables and rescaling numerical varaibles - the pipeline. \n",
    "\n",
    "The pipeline is concpetually pretty simple, we are just doing the same steps that we'd normally do to perform these data-prep functions. Probably the most challenging change is that the pipeline is more abstract - we don't see each step explicitly laid out, we set them all up and then just get results back. For this reason, it is important that we're comfortable with the ideas as there are fewer steps where we can troubleshoot. \n",
    "\n",
    "The basic process for using a pipeline is:\n",
    "<ul>\n",
    "<li>Create the pipeline object, which is imported from sklearn. \n",
    "<li>Set each preparation step, and the model step as arguments when making the pipeline. \n",
    "<li>Fit the pipeline object (instead of the model) with the data. (The pipeline roughly takes the place of the model)\n",
    "<li>Score and predict with the pipeline object. \n",
    "<li>Documentation is here: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html \n",
    "</ul>\n",
    "\n",
    "For example, we will take our dataset and rescale variables, before using a tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple Pipeline\n",
    "df1 = pd.read_csv(\"data/diabetes.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a categorical target and some numerical features. \n",
    "\n",
    "The pipeline will be to rescale the features, then train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Split data as normal\n",
    "y = np.array(df1[\"Outcome\"]).reshape(-1,1)\n",
    "X = np.array(df1.drop(columns={\"Outcome\"}))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#Build pipeline\n",
    "pipeline_steps = [('scaler', StandardScaler()),('DT', DecisionTreeClassifier()) ]\n",
    "pipe = Pipeline(pipeline_steps)\n",
    "# The pipeline can be used as any other estimator\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! Clean and simple!\n",
    "\n",
    "We could do the exact same thing with encoding if we had categorical varaibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex Embarked\n",
       "0         0       3    male        S\n",
       "1         1       1  female        C\n",
       "2         1       3  female        S\n",
       "3         1       1  female        S\n",
       "4         0       3    male        S"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"data/titanic_train.csv\")\n",
    "df2.drop(columns={\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Age\", \"SibSp\", \"Parch\"}, inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Survived  891 non-null    category\n",
      " 1   Pclass    891 non-null    category\n",
      " 2   Sex       891 non-null    category\n",
      " 3   Embarked  889 non-null    category\n",
      "dtypes: category(4)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "for i in df2.columns:\n",
    "    df2[i] = df2[i].astype('category')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8071748878923767"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Split data as normal\n",
    "y = np.array(df2[\"Survived\"]).reshape(-1,1)\n",
    "X = np.array(df2.drop(columns={\"Survived\"}))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#Build pipeline\n",
    "pipeline_steps = [('ohe', OneHotEncoder()),('DT', DecisionTreeClassifier()) ]\n",
    "pipe = Pipeline(pipeline_steps)\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Variable Types\n",
    "\n",
    "One thing we didn't handle \"properly\" above was the numerical values, we just dropped them. In order to deal with feature sets that have both categorical (requires encoding) and numerical (requires scaling) we need to use something slightly more complex - the Column Transformer. \n",
    "\n",
    "Our data processing pipeline will usually need to involve several different steps on several different features, some may need to be encoded, others may need to be scaled, and others may need to have missing values dealt with. The Column Transformer is a tool that allows us to split our data processing pipeline into several different paths, and send the appropriate subset of data into each of those paths. \n",
    "\n",
    "We'll look at how we can use a column transformer after a quick detour into one thing we commonly want to use inside of one, imputation..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "One common thing that we need to deal with in processing data for modelling is to handle data that is missing, and extending from the idea of the column transformer it is common that for different features we may need to deal with those missing values differently. Until now we have dealt with any missing data by just deleting the row, so there are no missing values; this action is a simplified version of a process called imputation. Imputation is the general process of filling in missing values. In this data we can see from the Describe that there are 891 rows, but under Age there are some missing values. We could delete these rows, or we could <b><i>impute</i><b> a value to insert as a placeholder - or generate a value to plug in, replacing the missing value. This allows us to keep those rows that have a missing value. \n",
    "\n",
    "Imputation is a very common data preparation step, it takes blank data and \"deals with it\". Dealing with it may mean removing those blanks, or using some more advanced analysis to determine a placeholder value to insert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"data/titanic_train.csv\")\n",
    "df3.drop(columns={\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"}, inplace=True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>891.0</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>714.0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>0.42</td>\n",
       "      <td>20.1250</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>80.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>891.0</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.9104</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>31.0</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count       mean        std   min      25%      50%   75%       max\n",
       "Survived  891.0   0.383838   0.486592  0.00   0.0000   0.0000   1.0    1.0000\n",
       "Pclass    891.0   2.308642   0.836071  1.00   2.0000   3.0000   3.0    3.0000\n",
       "Age       714.0  29.699118  14.526497  0.42  20.1250  28.0000  38.0   80.0000\n",
       "SibSp     891.0   0.523008   1.102743  0.00   0.0000   0.0000   1.0    8.0000\n",
       "Parch     891.0   0.381594   0.806057  0.00   0.0000   0.0000   0.0    6.0000\n",
       "Fare      891.0  32.204208  49.693429  0.00   7.9104  14.4542  31.0  512.3292"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Considerations\n",
    "\n",
    "The positive to doing this imputation is that we don't loose data. In many real world scenarios we often have some data that is missing. Think about a scenario where we are attempting to predict if people will get skin cancer - our features may be things like where they live, sun exposure, outdoor jobs, tanning, race/skin color, medical info, etc... In a real world study it is pretty difficult to gather a large dataset where every useful feature can be captured for every row in the dataset. Deleting every row where we have any missing value would likely cut our large, and hard and expensive to gather data, down to a shell of itself - if we did a large cancer study that spanned years, we probably wouldn't want to delete patients if they failed to complete one survey question. By imputing those missing values we can keep our valuable data by plugging in some placeholder for those occasional missing spots. \n",
    "\n",
    "For this example, we are plugging in an average (mean) age for the rows of Titanic-ers for which we have no age. This seems somewhat reasonable based on intuition - if there is some random passenger and we have no idea of their age, but we assume they are ~30, that probably doesn't mess things up too dramatically. There's a possibility that they are really old and we are way off, but the 75% percentile is 38, so the odds of that are pretty low. Overall we are probably 'winning' by keeping all that other data by using the imputation of age. \n",
    "\n",
    "The negative is that we are literally inventing a new value, and plugging it in - we are making up fake data. This can obviously introduce error if we do it poorly. For example, assume we are examining stock market data to predict stock price increases, to determine if we should invest. Imputing profitability (earnings per share) may not be super logical in this scenario. \n",
    "\n",
    "#### Why is it Missing?\n",
    "\n",
    "One key consideration in the real world in doing imputation is examining why the data is missing. Is the data just randomly missing, or is there some reason why some values may be missing?\n",
    "\n",
    "For example, survey data that involves people self-identifying with/as something that may incur discrimination (lgbt, race, disability, age) are commonly underidentified. Collecting data may be more challenging in remote environments vs cities. Forms may be poorly laid out, leading to one question being consistently overlooked. \n",
    "\n",
    "There are a lot of reasons that values may be missing, and examination with domain knowledge will help us determine the most appropriate imputation strategy. We will look at more sophisticated imputation stuff later on. \n",
    "\n",
    "#### Imputation Overall and Initial Strategy\n",
    "\n",
    "Imputation is extremely common in machine learning. In the smaller test datasets that we often use it isn't as important, as these tend to be more complete. In real world scenarios, especially when data is hard to collect (e.g. census, suveys, etc...), it is pretty common and often required to maintain the usability of our datasets. \n",
    "\n",
    "#### Imputation Approach\n",
    "\n",
    "For now, we can focus on simple imputation:\n",
    "\n",
    "<ul>\n",
    "<li>Numerical Imputation:\n",
    "<ul>\n",
    "<li>Replace with median: If we want to maintain median - outliers, skewed distributions such as income. \n",
    "<li>Replace with mean: If we want to maintain mean - normal(ish) distributions such as height. \n",
    "</ul>\n",
    "<li>Categorical Imputation:\n",
    "<ul>\n",
    "<li>Replace with mode (most common value)\n",
    "</ul>\n",
    "<li>Either:\n",
    "<ul>\n",
    "<li>If there is some default value, we can fill with a constant. For example, if you have data on number of citizenships that people have, the vast majority have 1. It makes sense to impute 1 if that data is missing (in this case that's the mode). \n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "There are several more complex imputation techniques that can be used, we'll look at some briefly when we look at clustering. The basic idea behind all of them is that they seek to impute a value that is \"better\" than one of the simple guesses by using the other data to come up with a more targeted value to impute. For example, if we were imputing a value for someone's income, and we knew their profession, it would likely make sense to impute a different value for the income of a brain surgeon than for a parking lot attendant. \n",
    "\n",
    "One more consideration is the number of missing values in a column - if we are just imputing a huge percentage of the values for one of our features, that feature probably isn't helping us all that much. A rule of thumb is that if you're missing about 20%-30% of the values in a column it is better to just drop that column rather than impute values for it. This rule of thumb is pretty loose - it really requires a specific look at the data, mainly the reason that values are missing. If we can infer some meaning from what's missing, we are more likely to be able to determine a logical imputation. In this Titanic example, Age is missing in a large number of rows, but keeping it is still worth it because the imputation is reasonable. \n",
    "\n",
    "Lastly, fillna in pandas also imputes similarly to what we are doing here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines with Column Transformers\n",
    "\n",
    "Our goal here is to create a pipleine to do the following:\n",
    "<ul>\n",
    "<li>Impute missing numeric values with the mean. \n",
    "<li>Scale numeric values with a standard scaler. \n",
    "<li>Encode categorical values.\n",
    "<li>Model with a decision tree. \n",
    "</ul>\n",
    "\n",
    "The column transformer object allows us to split the features that we are processing and deal with each group separately, this is very common in situations where we have both numeric and categorical variables to prepare - the numeric varaibles can be scaled, the categorical ones can be encoded, then they can be recombined into our prepared dataset.\n",
    "\n",
    "![Column Transformer](images/col_trans.png \"Column Transformer\")\n",
    "\n",
    "### Using a Column Transformer\n",
    "\n",
    "A column transformer functions like a slightly more elaborate pipeline:\n",
    "<ul>\n",
    "<li> The column transformer takes in multiple data subsets, and preprocessing pipelines. \n",
    "    <ul>\n",
    "    <li> Each argument in creating the CT contains 3 things - an informal label, the columns to process, and a pipeline of the processing steps. \n",
    "    </ul>\n",
    "<li> Each subset flows through the supplied processing pipeline. \n",
    "<li> At the end, the data is recombined, and the data is spit out ready for modelling. \n",
    "<li> In general, when using a column transformer, we will have:\n",
    "    <ul> \n",
    "    <li> A column transformer that is effectively \"the preprocessing pipeline\". \n",
    "    <li> Within that transformer, a pipeline that deals with each distinct subset of data (usually one for categorical, one for numerical).\n",
    "    <li> Another pipeline with two steps - the column transformer and the model, this is what we call \"fit\" on. \n",
    "    </ul>\n",
    "</ul>\n",
    "<b>Note:</b> we can split the columns into our groups in the column transformer either by manual inspection, or by some automatic filter like the data type. If you're automating it, make sure that you can trust that the data types are trustworthy - something like a rouge \"n/a\" in a numerical column could mess everything up. \n",
    "\n",
    "This all sounds a bit complex, but it isn't much worse than setting up a pipeline, here we've just layered in another step and we need to split the data into subsets. The steps like fit and score that we are used to with pipelines or models all work the same once the column transformer is established. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
    "numeric_transformer = Pipeline( steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "categorical_features = [\"Embarked\", \"Sex\", \"Pclass\"]\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer( transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the column transformer is created, putting it to use is a familiar process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7668161434977578"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data as normal\n",
    "y = df3[\"Survived\"]\n",
    "X = df3.drop(columns={\"Survived\"})\n",
    "\n",
    "#Build pipeline\n",
    "pipeline_steps = [('pre', preprocessor),('DT', DecisionTreeClassifier()) ]\n",
    "pipe = Pipeline(pipeline_steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Pipeline Objects\n",
    "\n",
    "As you may have picked up on, there's a lot of interchangeability between the sklearn objects that we use to prepare and model data. This is generally good, as we can arrange the exact pieces that we need and they'll work together in almost any combination. This can also be a little confusing, as there's not one fixed way to do things. In the long run, we'll get more used to this and be able to use it to our advantage - as long as something fits the template of these preprocessing tools (there are specific classes to extend, we'll look at it a little more later on) we can plug and play different preprocessing items, and even create our own. \n",
    "\n",
    "The ability to swap things in and out will be useful soon when we look at a grid search - the interchangability of steps helps allow us to automate the running of different trials to see what is most accurate. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Predict the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  target  \n",
       "0                     0.627   50       1  \n",
       "1                     0.351   31       0  \n",
       "2                     0.672   32       1  \n",
       "3                     0.167   21       0  \n",
       "4                     2.288   33       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv(\"data/diabetes.csv\")\n",
    "df_.rename(columns={\"Outcome\":\"target\"}, inplace=True)\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   target                    768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df_.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Column Transformer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6614583333333334\n"
     ]
    }
   ],
   "source": [
    "numeric_features_ = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "numeric_transformer_ = Pipeline( steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "categorical_features_ = []\n",
    "categorical_transformer_ = OneHotEncoder()\n",
    "\n",
    "preprocessor_ = ColumnTransformer( transformers=[\n",
    "        (\"num\", numeric_transformer_, numeric_features_),\n",
    "        (\"cat\", categorical_transformer_, categorical_features_),\n",
    "    ])\n",
    "\n",
    "#Split data as normal\n",
    "#y = np.array(df3[\"Survived\"]).reshape(-1,1)\n",
    "#X = np.array(df3.drop(columns={\"Survived\"}))\n",
    "y_ = df_[\"target\"]\n",
    "X_ = df_.drop(columns={\"target\"})\n",
    "\n",
    "#Build pipeline\n",
    "pipeline_steps_ = [('pre', preprocessor_),('DT', DecisionTreeClassifier()) ]\n",
    "pipe_ = Pipeline(pipeline_steps_)\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_)\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "\n",
    "pipe_.fit(X_train_, y_train_)\n",
    "print(pipe_.score(X_test_, y_test_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f49b311d837cdc5610aaa42e7d17bdec7b5ca2d09b36c852bd57f9468685896e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
