{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "STEPS = 3\n",
    "if IN_COLAB:\n",
    "    !pip install gpt-2-simple\n",
    "    STEPS = 100\n",
    "\n",
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import requests\n",
    "import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Borrowing GPT2\n",
    "\n",
    "Language models such as the popular GPT2/3/4/chat models are trained on lots of data and are absolutely huge in size. It isn't realistic for us to train a model that is anywhere near that size and sophistication, but we can borrow a model and repurpose it for our use. \n",
    "\n",
    "## Download Model\n",
    "\n",
    "The model itself is pretty large, we are downloading a model that is roughly 500MB, and we are using the smallest model. The large ones are large enough that they are impractical to deal with if we don't have some enterprise scale hardware.\n",
    "\n",
    "<b>Big Note:</b> these GPT models that we are downloading have one specific and annoying trait, they can't repurposed once created. Meaning that the \"sess\" object we define below is tied to the model and data we used it on first - it isn't like other examples where there can be a series of models all named \"model = Sequential...\". There will be an error quoting something like \"graph error\" if you try. To fix things, restart the runtime and run again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"124M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "\tprint(f\"Downloading {model_name} model...\")\n",
    "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Model\n",
    "\n",
    "We can take the model and tailor it to our use by providing it with some additional text that it can use for fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 11:55:32.045625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 11:55:52.970576: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-16\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-16\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 338025 tokens\n",
      "Training...\n",
      "[17 | 67.85] loss=4.56 avg=4.56\n",
      "[18 | 139.30] loss=3.90 avg=4.23\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-18\n"
     ]
    }
   ],
   "source": [
    "file_name = \"shakespeare.txt\"\n",
    "if not os.path.isfile(file_name):\n",
    "\turl = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\tdata = requests.get(url)\n",
    "\n",
    "\twith open(file_name, 'w') as f:\n",
    "\t\tf.write(data.text)\n",
    "\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              file_name,\n",
    "              model_name=model_name,\n",
    "              steps=STEPS)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "Now that the model is downloaded and fine tuned to our data, we can generate some new text. There are a few parts here that we should look at a little more closely:\n",
    "<ul>\n",
    "<li> Temperature: This is a value that controls how random the output is. The higher the value, the more random the output. The lower the value, the more likely the output is to be similar to the input. </li>\n",
    "<li> Length: This is the number of tokens that will be generated. </li>\n",
    "<li> Prefix: This is the text that will be used to seed the model, a.k.a. the \"starting point\" of the brand new text we'll be creating. </li>\n",
    "</ul>\n",
    "\n",
    "#### Text Generation Process\n",
    "\n",
    "Inside the model, the transformer generates new text by taking the prefix text and using it to then generate a series of tokens that make up our eventual output. \n",
    "\n",
    "![Text Generation](images/transformer_text_gen.png \"Text Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess, model_name=model_name, length=100, temperature=0.7, nsamples=5, batch_size=5, prefix=\"Where for art thou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 16:15:47.318110: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-1\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1\n",
      "This article is about the legendary character. You may be looking for Kudri. You may be looking for\n",
      "\n",
      "Kudri is an iconic character in Fallout 4 and Fallout: New Vegas. He is a human male whose name means \"sister\" and who took the name \"Kudri\" from his mother, Mary.\n",
      "\n",
      "Contents show]\n",
      "\n",
      "Biography Edit\n",
      "\n",
      "Kudri was born in the Kudri village of Anvil, a stone's throw from the continent of Tamriel, and grew up in the ruins of an abandoned town. After his mother's death, his father moved to the same town, after which the rest of the family members refused to give him any of their children. Despite the obvious kinship of his mother and father, Kudri's half-human half-human half-boy half-boy, who is named Kudri, was never brought up by his grandmother. He was raised by his son, who is named Kudri, and adopted by his grandmother, who taught him to read and write. He was neglected by his grandmother for about ten years after his father's death. Kudri was not even able to read at all until he was sixteen.\n",
      "\n",
      "The Kudri family was not able to explain the circumstances of his death, and was unable to answer any of the questions they had. However, they did eventually answer some of their questions, and discovered a link between their death and their mother's death. Kudri was later given a suit of armor, which allowed him to perform a number of tasks, and was able to escape his mother's body. The suit of armor was later scattered throughout the Wasteland, and he was later killed by the Courier in the Mojave Wasteland.\n",
      "\n",
      "Kudri has a number of tattoos, such as his left hand, and a much stronger hand that allows him to perform a number of tasks.\n",
      "\n",
      "In Fallout 3, Kudri appears in the Fallout: New Vegas Enhanced Edition as the \"Mother of Kudri\" in the cutscene \"Kudri's Dagger.\" He is voiced by Walter Vogt, with additional edits by Ville.\n",
      "\n",
      "Behind the scenes Edit\n",
      "\n",
      "The name of Kudri is a reference to the name of the character from the original Fallout series, \"The Courier's Wife\".\n",
      "\n",
      "Kudri is a reference to the character from the original Fallout series, \"The Courier's Wife\". Kudri was originally a user of a sewing machine called The Kudri. Though the name was eventually changed to \"Kudri\", it was later used by Fallout 3 voice actor and Fallout 4 voice actor John Woo in the game.\n",
      "\n",
      "In the game, Kudri is voiced by John Woo, though he voiced Kudri before he left to work on the Fallout 3 voice project.<|endoftext|>Usain Bolt's uncle, former Tottenham Hotspur midfielder and potential future England captain, has been named as a new coach of the England Under-21s.\n",
      "\n",
      "The former England captain had been linked with a move back to Tottenham's academy in July but now it appears Bolt has opted to stay at White Hart Lane.\n",
      "\n",
      "Bolt has seen plenty of time at England Under-21s and is now on his way to coaching the Premier League side.\n",
      "\n",
      "\"I am happy to be back in the team and I am delighted to be back in the team,\" Bolt told BBC Radio Manchester.\n",
      "\n",
      "\"I am delighted that I have been given the opportunity to play for England under-21s and I am delighted to have been given the opportunity to play for the Under-21s.\n",
      "\n",
      "\"It has been great to be able to have a chance to be part of the Under-21s again and I want to thank everyone for their support, for having the opportunity to coach for us.\n",
      "\n",
      "\"I am looking forward to working with the players and coaching them for a long time.\"\n",
      "\n",
      "Bolt has a history of success with England Under-21s, with several success stories including Sir Alex Ferguson at the club, Sir Alex Ferguson at Tottenham Hotspur and Tom Cleverley in the Premier League.\n",
      "\n",
      "However, he was not given the opportunity to play for England under-21s until 2012 when he was a free agent.<|endoftext|>It is time to talk about the end of the war against the Islamic State. It is time to talk about the end of the war against the Islamic State.\n",
      "\n",
      "The leaders of the Islamic State's branch, Al Qaeda, have accused the Syrian affiliate of acting on behalf of the group's leader, Gen. Salim Idris, who is in Syria.\n",
      "\n",
      "The conflict began in early April when the Islamic State of Iraq and the Levant (Isil) linked to the Syrian army seized a large chunk of territory in Iraq, Syria and Turkey. The world has been watching the fight in Syria for weeks, and media outlets have reported that ISIL is preparing to take over other key Islamic\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, model_name=model_name, length=100, temperature=0.7, nsamples=5, batch_size=5, prefix=\"Where the hood at\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Data\n",
    "\n",
    "We can load some different data and see what our model generates.\n",
    "\n",
    "<b>Note:</b> you might need to delete and restart the runtime here. If you get errors, try that, run the import stuff above, but not any modelling, then try this. Basically skip creating the first model that is Shakespeare tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 13:05:32.808532: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-18\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-18\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_20816/3306840578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m gpt2.finetune(sess_2,\n\u001b[0m\u001b[1;32m      6\u001b[0m               \u001b[0mtrain_text_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m               \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading dataset...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0mdata_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset has'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/gpt_2_simple/src/load_dataset.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(enc, path, combine)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mraw_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mtoken_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url2 = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb_clean.csv\"\n",
    "train_text_file = keras.utils.get_file('train_reddit.txt', url2)\n",
    "\n",
    "sess_2 = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess_2,\n",
    "              train_text_file,\n",
    "              model_name=model_name,\n",
    "              steps=STEPS)   # steps is max number of training steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess_2, model_name=model_name, length=100, temperature=0.7, nsamples=5, batch_size=5, prefix=\"I have diamond hands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess_2, model_name=model_name, length=100, temperature=0.7, nsamples=5, batch_size=5, prefix=\"Where for art thou\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
