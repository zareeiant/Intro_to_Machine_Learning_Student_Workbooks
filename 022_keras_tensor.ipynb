{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gGsMHgvKZVRO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_lu_r1ndZVRR"
      },
      "outputs": [],
      "source": [
        "# Helper to plot loss\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "52niBYdKZVRS"
      },
      "source": [
        "# Keras, TensorFlow, and Neural Network Regression\n",
        "\n",
        "As we have seen, neural networks aren't quite as complex as they appear at first, however we still generally don't want to have to build them from scratch very often. The libraries that we will primarily use for creating neural network models are Tensorflow and Keras. Keras and Tensorflow combine to be roughly what sklearn was for the other types of models we've used. \n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "Tensorflow, developed by Google, is one of the most popular libraries for neural networks. \n",
        "\n",
        "### Keras\n",
        "\n",
        "Keras is another package that provides an an API offering an easier to use interface to Tensorflow, allowing us to use it with code that is higher level, avoiding much of the linear math that can make Tensorflow frustrating. Since its introduction Keras has been wrapped in with Tensorflow and the two are normally now blended together as far as we are concerned. \n",
        "\n",
        "### Other Alternatives\n",
        "\n",
        "Keras and Tensorflow are not the only libraries of neural networks, the primary competitor to Tensorflow is PyTorch, which was developed by Facebook. PyTorch does pretty much the same thing as Tensorflow, we won't look at it. While PyTorch is less commonly used than Tensorflow right now, it has picked up steam recently. There's a pretty high likelihood that both Tensorflow and PyTorch will be common for the foreseeable future.\n",
        "\n",
        "### Optimization and Efficiency Notes\n",
        "\n",
        "Neural networks are very computationally expensive, and can take a long time to train. As we move into looking at some larger models in the near future, the processing time on a typical laptop will become prohibitive. We will utilize Google Colab for some of these examples, which will allow us to borrow some GPU processing that is often drastically faster than what we can get on our own machines.\n",
        "\n",
        "If you happen to have a machine with a nVidia GPU, an M1/M2 Mac, or potentially a couple of other GPUs, you can install an optimized version of Tensorflow on your machine that will allow this stuff to use your GPU and be way faster without using Colab. This isn't required, but it is a good idea if your computer is capable. I don't have a GPU, so I don't have a detailed example, but you can google \"install tensorflow GPU/M1 Mac\" or similar for your GPU and there will be some pretty simple step-by-step instructions. If available, this will make a big difference in the speed of your models. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remote Data and Google Colab\n",
        "\n",
        "Below we'll load some data, however here we'll load something from a data repository that I setup on Azure, rather than the desktop, as we normally would. This is easy to do, we just change the path to the URL and use our \"read_csv\" as usual. The reason for this is that we are going to use Google Colab for some of the processing here, and if we are loading data from some static location, like this server address, we don't need to worry about loading data files and maintaining file paths. This will likely be something that you need to at least pay some attention to if you are using Colab. The key thing to remember is that it doesn't matter where the data is stored, as long as you can get to it. So if we need to change the data to make things work, we may have to change the loading bit to make it work. \n",
        "\n",
        "#### Your Remote Data\n",
        "\n",
        "If you have a large-ish dataset that you're using, you can do the same thing. The datasets can be loaded directly from your github repository URL, as long as you get the \"Raw\" URL from Github. Click on the raw link on a file, then use the URL of the resulting page:\n",
        "\n",
        "![Raw Link](images/github_raw.png \"Raw Link\")\n",
        "\n",
        "Then you can use that URL to load the data. If you have data that comes from some site on the internet, you can likely load it similarly, assuming there's no security on the site blocking you from loading it. If there's something that doesn't work in any reasonable way, let me know, we might be able to put it on this Azure repository. I can't promise that in every situation though, it isn't free - though Azure, AWS, and Google Cloud all have free access of various levels that you could sign up for and use, if you wanted to. You can also save stuff to your Google Drive, and then load it from there - Google instructions if you need to do this. \n",
        "\n",
        "### Google Colab\n",
        "\n",
        "As noted, we'll use Colab for some of our neural network stuff, as it gives us GPU access, which is massively helpful in many situations. The URL for Colab is https://colab.research.google.com/ and you can sign in with your Google account. Once on Colab, it works pretty much like Jupyter notebooks. To load data from Github, we can go to File->Open Notebook->Github, and then paste in the URL of the repository we want to load and select the notebook. Once loaded, it is basically a normal notebook. The one thing to pay attention to is that if there are any file loading bits in the code, those will fail, as noted above. \n",
        "\n",
        "Inside of Colab, the one key thing we want is to enable GPU accelleration. To do this, go to Edit->Notebook Settings, and then select GPU as the hardware accelerator. This will make a big difference in the speed of your models when using tensorflow. One thing to note is that while we get a bunch of GPU time for free, it is limited. So it is probably a waste to run huge tests with massive numbers of epochs, over and over, as you will likely run out of GPU time at some point. It will reset, but how much is allocated to each person is basically up to the whims of Google at any given moment. The \"!\" command below will print the details on the GPU that is currently being used.\n",
        "\n",
        "I prefer to do all the editing in VS Code, save, upload to Github, then load and run the final code in a Colab window. This is more personal prefence than anything else, but it works well for me. You can also edit and save things in Colab, including back to Github, but you will likely need to do some authorization to allow access, and the process feels more annoying to me. For ease, I added this parameter to set the epochs globally for this notebook, so I can set it to a low number while working locally, then increase it when I want to test the actual performance on Colab. We could do a similar thing with sampling large datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "BASE_EPOCHS = 100\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JrKP3wO_ZVRT",
        "outputId": "1ac1822c-99a5-4e98-ea4d-0158aed55c65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21608</th>\n",
              "      <td>360000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1530</td>\n",
              "      <td>1131</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1530</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98103</td>\n",
              "      <td>47.6993</td>\n",
              "      <td>-122.346</td>\n",
              "      <td>1530</td>\n",
              "      <td>1509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21609</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2310</td>\n",
              "      <td>5813</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2310</td>\n",
              "      <td>0</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>98146</td>\n",
              "      <td>47.5107</td>\n",
              "      <td>-122.362</td>\n",
              "      <td>1830</td>\n",
              "      <td>7200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21610</th>\n",
              "      <td>402101.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1350</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5944</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21611</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1600</td>\n",
              "      <td>2388</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1600</td>\n",
              "      <td>0</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>98027</td>\n",
              "      <td>47.5345</td>\n",
              "      <td>-122.069</td>\n",
              "      <td>1410</td>\n",
              "      <td>1287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21612</th>\n",
              "      <td>325000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1076</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5941</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>1357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
              "21608  360000.0         3       2.50         1530      1131     3.0   \n",
              "21609  400000.0         4       2.50         2310      5813     2.0   \n",
              "21610  402101.0         2       0.75         1020      1350     2.0   \n",
              "21611  400000.0         3       2.50         1600      2388     2.0   \n",
              "21612  325000.0         2       0.75         1020      1076     2.0   \n",
              "\n",
              "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
              "21608           0     0          3      8        1530              0   \n",
              "21609           0     0          3      8        2310              0   \n",
              "21610           0     0          3      7        1020              0   \n",
              "21611           0     0          3      8        1600              0   \n",
              "21612           0     0          3      7        1020              0   \n",
              "\n",
              "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
              "21608      2009             0    98103  47.6993 -122.346           1530   \n",
              "21609      2014             0    98146  47.5107 -122.362           1830   \n",
              "21610      2009             0    98144  47.5944 -122.299           1020   \n",
              "21611      2004             0    98027  47.5345 -122.069           1410   \n",
              "21612      2008             0    98144  47.5941 -122.299           1020   \n",
              "\n",
              "       sqft_lot15  \n",
              "21608        1509  \n",
              "21609        7200  \n",
              "21610        2007  \n",
              "21611        1287  \n",
              "21612        1357  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#df = pd.read_csv(\"data/house_data.csv\")\n",
        "df = pd.read_csv(\"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/house_data.csv\")\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uLE617U8ZVRV",
        "outputId": "08d0318a-9c52-4e72-b585-ade094db2591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21613 entries, 0 to 21612\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   price          21613 non-null  float64\n",
            " 1   bedrooms       21613 non-null  int64  \n",
            " 2   bathrooms      21613 non-null  float64\n",
            " 3   sqft_living    21613 non-null  int64  \n",
            " 4   sqft_lot       21613 non-null  int64  \n",
            " 5   floors         21613 non-null  float64\n",
            " 6   waterfront     21613 non-null  int64  \n",
            " 7   view           21613 non-null  int64  \n",
            " 8   condition      21613 non-null  int64  \n",
            " 9   grade          21613 non-null  int64  \n",
            " 10  sqft_above     21613 non-null  int64  \n",
            " 11  sqft_basement  21613 non-null  int64  \n",
            " 12  yr_built       21613 non-null  int64  \n",
            " 13  yr_renovated   21613 non-null  int64  \n",
            " 14  zipcode        21613 non-null  int64  \n",
            " 15  lat            21613 non-null  float64\n",
            " 16  long           21613 non-null  float64\n",
            " 17  sqft_living15  21613 non-null  int64  \n",
            " 18  sqft_lot15     21613 non-null  int64  \n",
            "dtypes: float64(5), int64(14)\n",
            "memory usage: 3.1 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ThkDxsnGZVRV",
        "outputId": "fef44a1a-6690-4bd7-b618-dd6261194d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21613, 18) (21613, 1)\n"
          ]
        }
      ],
      "source": [
        "y = np.array(df[\"price\"]).reshape(-1,1)\n",
        "X = np.array(df.drop(columns={\"price\"}))\n",
        "print(X.shape, y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DY9JoSCXZVRW"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "Creating a NN model is slightly different from the normal process that we are used to in sklearn. We need to do a little more work to set it up. \n",
        "\n",
        "### Create Model and Add Layers\n",
        "\n",
        "First we need to make a NN model, it comes \"empty\". We will use a sequential model, which is the most simple type but is less configurable (which we don't care about much right now). The limitation of sequential models is that they can only take in one tensor and only output one tensor. The other options here are \"functional\", which allows for the structure of the model to be configured, and \"model subclassing\", which allows you to build almost everything from scratch. These other types are more complex and more flexible, but actually aren't really needed for most applications, and we won't use them. These more complex models are commonly used for scenarios where the data is complex, such as a self driving car - a model needs to output steering as well as velocity. Also for more complex problems such as language or image processing, this flexibility allows for models to be created that are better able to extract the information from the data.\n",
        "\n",
        "#### Layers\n",
        "\n",
        "Next we need to add some layers. We will start simple with only two \"thinking\" layers, and one to do some processing. We can think of the layers roughly like steps of the sklearn pipeline, with data entering at the first layer and predictions flowing out of the final layer. \n",
        "\n",
        "In addition to \"normal\" neural network layers, there are many other types that can do all kinds of other stuff. One example we will use here is the normalization one at the front. This layer does exactly what you'd expect - it normalizes our data so the rest of the network can use it. The normalize layer will also automatically handle the 2D nature of the data that we are used to, so we don't need to worry about that aspect here. Other layers can do everything from regularization to image processing, they are also commonly inhierited for developers to create custom layers targeting specific tasks. We'll use a few of the other ones as we move through some more complex models.\n",
        "\n",
        "![Keras Layers](images/keras_layers.jpg \"Keras Layers\")\n",
        "\n",
        "#### Dense Layers\n",
        "\n",
        "We'll use dense layers here, and they are the main building block in our models. When adding the layer we need to specify a couple of things. One is the input dimensions - we need to tell the network what the shape of the incomming data is. \n",
        "\n",
        "The other argument is the units, which represents the output dimension. When using these Keras dense layers we don't need to specify each layer's input/output like we did when we made it by hand. We specify both, using units and input_dim, for the first layer that takes in the input; for subsequent layers we can just specify the output and Keras will automatically figure the rest out. \n",
        "\n",
        "Note that there is also an input layer that can be added, we can avoid the need for it by using the input_dim or input_shape as shown below. The two examples there do the same thing, since the input is flat - 18 features. If we are dealing with inputs that do not start out as flat, such as in an image, use the input_shape since you can specify all dimensions; we will see an example of this next time with some images. \n",
        "\n",
        "#### Activation Function\n",
        "\n",
        "For each of our layers we need to define which activation function to use. For now we will use the ReLU function, which is probably the most popular. We'll look at other ones later on. Note that we've left the activation function off of the final layer - we are doing regression so we want that raw value. This is the same idea as with linear regression - we don't want the prediction to be transformed through something like the sigmoid, we just want the number.\n",
        "\n",
        "#### Summary\n",
        "\n",
        "After we've constructed the model, the summary command give us, well, a summary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIquYAGtZVRX"
      },
      "source": [
        "We are dealing with a bunch of numerical inputs here, so we can add a normalization layer at the front end. Like with sklearn, we want to fit the normalization to the training data only. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zWPhKP3wZVRX",
        "outputId": "4c6bc4d8-4a5f-4da7-edd8-7792b597b931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-15 12:04:54.601799: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization (Normalizatio  (None, 18)               37        \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 18)                342       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 19        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 398\n",
            "Trainable params: 361\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B6AH5AydZVRY"
      },
      "source": [
        "#### Compile Model\n",
        "\n",
        "Once a model is created we need to compile it. The complie step basically builds the layers we specified above and the loss and optimization parameters below together into a usable model object. When compiling the model we are providing it with the things it needs to calculate error:\n",
        "\n",
        "<ul>\n",
        "<li> Loss - we can provide a loss function that we'd like to use. \n",
        "<li> Optimizer - the optimizer is the algorithm that the model will use to perform the gradient descent to find the lowest error. Adam is a very common choice.\n",
        "<li> Learning rate - the learning rate is provided as a parameter of the optimizer. \n",
        "</ul>\n",
        "\n",
        "##### Optimizing Adam\n",
        "\n",
        "The optimizer is the algorithm used to perform the gradient descent and minimize error. For the most part this isn't something we need to be concerned about. The choice of optimizer is much more important if dealing with very large datasets because different optimizers have different levels of efficiency. For our purposes, we can use Adam and be pretty happy. Adam stands for Adaptive Moment Estimation which means basically that it will adjust itself depending on current gradients. It tends to be efficient both in time and memory, so it is very commonly used. \n",
        "\n",
        "<b>Note:</b> it is pretty common in tensorflow and keras to have things like the example below, where we can specify \"adam\" with either the name, or the full class with arguments. These different ways of doing things are generally interchangable, but it does make documentation more confusing, as the two different ways may often be interchanged. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fEya_QckZVRY"
      },
      "outputs": [],
      "source": [
        "#model.compile(loss='mean_absolute_error', optimizer=\"adam\")\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n9TUG422ZVRZ"
      },
      "source": [
        "### Fit the Model\n",
        "\n",
        "The fit command does the same thing that we are used to, it trains the model, however there are some differences. The main difference is that batch_size is almost always set in neural networks, while the sklearn models just take all the data at once. \n",
        "\n",
        "What's a batch? Batches are just subsets of the data, so if the batch size is 100 the algorithm will grab 100 rows at a time before making an update to the weights and bias. There are a few reasons this exists:\n",
        "\n",
        "<ul>\n",
        "<li> Memory constraints - it is common with neural networks to deal with datasets that are extremely large. Processing data that can't fit entirely in RAM is very slow (the computer must swap data from the hard drive to RAM as it is needed) compared to data that is in RAM. Cutting the batch size can avoid this issue. \n",
        "<li> Speed - the math involved in the back propagation can sometimes be very computationally intensive. \n",
        "<li> Accuracy - batch size can have an impact on accuracy, though that impact is not very predictable. For the most part finding an optimal batch size will need to be grid-searched. \n",
        "</ul>\n",
        "\n",
        "The fit command also has the epoch paramater, which instructs on how many times to work through ALL of the data. We want to ensure we have enough epochs to find the optimal solution. Training rounds, or epochs, are one of the key tuning factors when using neural networks. Similar to large trees, large neural networks are capable of learning the training data very well, and carry the same risk of overfitting. With neural networks, a common approach to tuning is to allow the model to train, and cut it off when we start overfitting, or when the testing accuracy starts to decrease.\n",
        "\n",
        "#### Plot the Loss\n",
        "\n",
        "One very common visualization we see with neural networks is a plot of both training and validation loss vs number of epochs. Generally we'll see the training loss drop - first sharply as the model initially fits itself, then more slowly as it becomes more fitted. The validation loss will usually somewhat mirror the training loss, except it will often reach a minimum at some point before again increasing. This minimum point is our best model, when the validation loss starts increasing again, that is a sign that the model has become overfitted - customized to the training data, but less and less generalizable to new data. \n",
        "\n",
        "Set the verbosity to 1 in the fit to get a full list of the loss for each epoch to pinpoint the exact \"ideal\" number of epochs. We'll look more at this in a minute. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDUXMj_pZVRZ",
        "outputId": "6facd69a-6643-4845-da6f-94199649ac5b"
      },
      "outputs": [],
      "source": [
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BASE_EPOCHS, validation_split=.2, verbose=1)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3vc6DpiZVRZ"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Make a simple neural network for predicting the price of homes in California. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqmBczcMZVRZ",
        "outputId": "da98e1e8-4f49-4a0b-ee35-34c696f10dca"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "Xcal = pd.DataFrame(cal.data)\n",
        "ycal = pd.DataFrame(cal.target)\n",
        "Xcal.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ycal.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TxqgdHUZVRa",
        "outputId": "f29561ee-1d8f-4b8a-f19f-b28371799e23"
      },
      "outputs": [],
      "source": [
        "# Model. Or whatevs. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oUfNO6HeZVRb"
      },
      "source": [
        "## Basics of Overfitting and Underfitting in Neural Networks\n",
        "\n",
        "Just like any other type of model, our primary task in trying to attain an accurate set of predictions is to balance the overfitting and underfitting. In a neural network, the ideas are the same as with standard models, however the tools and their usage can differ slightly. \n",
        "\n",
        "### Add Data\n",
        "\n",
        "Adding data to the training set is the number one way to improve accuracy. As noted above, neural networks are commonly able to acheive very high accuracy levels if provided with very large training sets. For smaller datasets, the probability of a neural network being the best model is much lower than with big data. There isn't a replacement for having large amounts of data (though there are a few tricks that we'll look at later), and modern large neural networks are (usually) the best tool that is able to take advantage of all that data. \n",
        "\n",
        "In the near future we'll look at some common pre-trained models that people/orgs such as Google have shared, most notably ones that do things like image recognition. These models are typically trained on really large datasets - often 10s of GB or more. This massive amount of training data allows these models to be more accurate than anything that we could create, but would be unrealistic for most people to train just due to the processing power and time needed. We can take them and adjust them a bit to our needs though...\n",
        "\n",
        "<b>By far, the most universally beneficial way to make a neural network more accurate is to add more (good) data.</b>\n",
        "\n",
        "### Model Capacity\n",
        "\n",
        "The model capacity is the \"size\" of the model - refering to the combination of the number of neurons on each layer and the number of layers. In general the larger a feature set is, the larger a capacity we will need to be able to avoid underfitting and make accurate predictions. However, similar to a decision tree, if the model becomes too large for the data, we are likely to overfit. \n",
        "\n",
        "In big data scenarios (e.g. Google or Tesla training image recognition models) the feature sets can be massive (e.g. a 5 megapixel image is at least 15 million features) so the networks used have a very high capacity. Because there is a lot of training data, the model is able to have a huge capacity, but not overfit. These models can take FOREVER to process (e.g. weeks with the work paralellized on dedicated and fast machines) but they are able to make very accurate predictions since they get all the \"benefits\" of overfitting - predictions highly tailored to the training data; along with all the \"benefits\" of underfitting - since there is so much training data, they are still generalized enough to predict new data. There is something of an open question on if we should expand capacity by making the layers larger with more neurons, or by adding more layers. Like most things, the answer is whatever is tested to be best for the specific problem. In general though, more layers seems to be better for most problems, and the neural networks that excel at things such as image recognition tend to be deep learning networks - those with many layers. The combination of large datasets, deep networks, and fast processing allows for most of the modern AI that we see or interact with. \n",
        "\n",
        "<b>Note:</b> the size numbers that we specify as we add layers is the size of the output (other than the \"input_dim\" we have on the first layer). For other layers, the input of one just adapts to the output of the previous layer.\n",
        "\n",
        "#### Capacities, For Now\n",
        "\n",
        "We will look more at the model capacities in the next section, for now we can follow a few rough guidelines:\n",
        "<ul>\n",
        "<li> Input layer output size is between the # of features, and some multiple (maybe up to ~5 max) of the # of features. \n",
        "<li> Output layer is defined by the target we are predicting.  \n",
        "<li> Hidden layers between the two either stay constant in size, or taper down. \n",
        "</ul>\n",
        "\n",
        "We will look at this more next time. Unfortunately there isn't a definitive rule that always works, the ultimate decision is based on results. For now, we can just make some reasonable models and observe the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZWfGHNqZVRb",
        "outputId": "1e595b6c-0e69-477e-dc67-c505ef26ba74"
      },
      "outputs": [],
      "source": [
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXjXrQEeZVRb",
        "outputId": "7d25815a-6f32-4c0d-c89b-a255ff13816f"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS*2, batch_size=1000, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tTocTQASZVRc"
      },
      "source": [
        "### Early Stopping\n",
        "\n",
        "Early stopping is very common with neural networks, due to the common pattern mentioned above of the optimal balance of over/under fitting occuring at some point within many, potentially dozens/hundreds/thousands, of epochs. Early stopping kills the process after it detects that validation loss is going back up. \n",
        "\n",
        "We can put early stopping in place by using a Keras function called a callback, which has odd syntax, but is quite simple to use. The patience pararmeter controls how many epcohs of worsening scores are tolerated before implementing the stop. The restore_best_weights tells the model to roll back all of its weights to the optimal point - so we automatically get the best model post-training. In most cases we probabyl want to use early stopping along with a high epoch number. We can let the model train, and just tell us when it is finished. \n",
        "\n",
        "Another parameter that we may want to implement is the \"min_delta\" parameter. This controls how much the validation loss needs to improve to be considered an improvement. If the validation loss is 0.1, and the min_delta is 0.01, then the model will stop if the validation loss goes up to 0.11, but not if it goes up to 0.12. This is useful if we have a very small validation loss, and we want to make sure that we are getting a significant improvement before stopping. In situations like the above one, where the loss is flat-ish, but bumps around a little, this may help deal with that noise and make the early stopping cutoffs work more in line with expectations.\n",
        "\n",
        "<b>Note:</b> the patience parameter requires a little bit of thought, and a good value can vary a little between scenarios. We want to make sure that we are not stopping too early, if the patience is really small, we might get a situation where a local minima is reached and we stop the model as it is trying to escape it. If the patience is too large, we might not stop the model in time, as we might have many flat loss values, then one or two that are an improvement, resetting the clock and making the training process keep going far too long. We need to base it somewhat on what the model actually does - very stable loss results might mean we need a higher patience, while a model that is very volatile might need a lower patience, as we might not have long stable periods anywhere. Values between 5 and 15 or so are common, as are settings of around 10% of the number of epochs, but it is a good idea to test a few different values to see what works best - if you actually work with larger examples we'll run several trials with different parameters or model configurations with samples of the full dataset. These trials will likely give us an idea of a reasonable cutoff point for patience. Many of the patience settings in examples that are in the next few workbooks are probably lower than \"ideal\", this is entirely for practicality purposes - we might be cutting off some slightly more ideal models, but we are also saving a lot of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0MYLfTBZVRc"
      },
      "outputs": [],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFlOi1yGZVRc",
        "outputId": "256f7158-b58b-48b4-d17e-877c40d84201"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True, min_delta=1000) \n",
        "\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS*3, batch_size=50, validation_split=.2, verbose=1, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation\n",
        "\n",
        "One change in the way we normally setup our data in neural networks is that we are performing checks during the training process with \"validation data\". This is another split of the data, like train-test, but this validation data is used during training to check the model's progress. This allows the model to constantly calculate the accuracy of the model on new data, similar to how things would work inside of a cross-validation in a grid search. The key thing that we can use this for is that it allows us to calculate the current \"test data\" performance, which is the \"validation loss\" that we can use to evaluate the model and cut short training. This validation dataset is normally \"good enough\" to be an indicator of the expected performance of the model, just like the test scores we are used to. We could still do a train-test split prior to training, and calcuate that true test accuracy, but we don't really need to. In most cases, what we referred to as test accuracy in sklearn models is just the validation accuracy in neural networks.\n",
        "\n",
        "### Early Stopping on Validation Loss\n",
        "\n",
        "More likely than stopping when the training loss starts to increase, we'll want to stop when the validation loss starts to increase. This is because the validation loss is a better indicator of how the model will perform on new data. This is also what allows us to set a high epoch number and walk away while training happens - as long as our model has \"room to grow\", or the capacity to fit the data, we can let it train until we've reached a minimum validation loss.\n",
        "\n",
        "![Early Stopping](images/early_stopping.png \"Early Stopping\")\n",
        "\n",
        "<b>Note:</b> early stopping on the validation loss will probably be the most common \"optimization\" that we'll do. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, min_delta=2000) \n",
        "\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS*3, batch_size=50, validation_split=.3, verbose=1, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXfwUFjSZVRc"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "Like other linear models, we can implement regularization to help tame overfitting. \n",
        "\n",
        "We can use both L2 (Ridge) regularization that will limit growth of coefficients, and L1 (Lasso) regularization that is able to eliminate features by shrinking their coefficients to 0. The functionality is the same as we are used to, a regularization term is added to the loss, and the optimization, such as gradient descent, is then performed as normal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgwwVgJ6ZVRd",
        "outputId": "7c4c3e58-3c0a-42ef-b502-764d87747a75"
      },
      "outputs": [],
      "source": [
        "# Regularization\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l2\"))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnRVShe2ZVRd",
        "outputId": "fb5bbfba-efcd-4d12-ba31-427900c8da53"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FXboQ7N2ZVRd"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Neural networks also commonly employ a technique call dropouts to prevent overfitting. This works just like the name says, every time the data is moved from one layer to another some portion of the features are randomly held out from being used. The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own. This sounds somewhat weird, but is actually effective. The number of features held out is called the dropout rate, typically between .2 and .5. \n",
        "\n",
        "![Dropout](images/dropout.webp \"Dropout\")\n",
        "\n",
        "An analogy can be drawn to the bootstrapping we looked at with trees - some random subset is selected each time, resulting in each batch getting \"a slightly different look at the data\", thus preventing overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUJv1QPEZVRd",
        "outputId": "b6f5125b-b396-4dfd-9cd4-95d84f9c452e"
      },
      "outputs": [],
      "source": [
        "# Dropout\n",
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(512, input_dim=18, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUJHDMXbZVRd",
        "outputId": "aff714cd-a820-49b3-8ede-e24737a275b7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AkcKFzNZVRd"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Once the model is trained, using it is mostly familiar to us from the sklearn stuff. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-jw_zjdZVRe",
        "outputId": "0a3fbf07-c78c-4bea-fa3e-652875330375"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(X_test)\n",
        "mean_absolute_error(y_test, preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9YyxGjZVRe"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use the California data from previously and try to add some regularization things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train_cal.shape, y_train_cal.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll mix in a couple of regularization things, this could be almost anything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGstyyzcZVRe"
      },
      "outputs": [],
      "source": [
        "# Dropout\n",
        "#Test Different Model Capacities\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYC8NGfZVRe"
      },
      "source": [
        "### Customized Loss\n",
        "\n",
        "Most scenarios are totally fine with a standard loss function, but what if we have something odd? What if we are playing on The Price is Right? We want to get as close as we can, without going over. We can write a loss function to mirror that! The Keras backend piece is basically a set of functions (like the \"switch\" we used, mean, sum, etc...) that are implemented in Tensorflow - they don't do anything differently from \"normal\" functions, but they are potentially more efficient.\n",
        "\n",
        "More practically, some real life scenarios have a disperse impact of different types of error. For example, if you are working for a call centre and predicting the number of agents to staff. Having slightly too many may be an error that costs a little bit of money, but not that big of a deal. Predicting too few might incur serious penalties if callers wait and you violate an SLA. Being off in one direction is bad, being off in the other direction can cause you to \"fall off of a cliff\" so to speak. In cases where the impact of the error is not uniform, custom loss functions may make sense. \n",
        "\n",
        "<b>Note:</b> these custom loss functions work, but if you are going to put one into real use, you would likely want to look into ensuring the calculations are efficient and that you actually get a tangible benefit from the custom loss function. This one is kind of a goofy example, we will normally just want to use a standard loss function. Due to the wild differences if the residual is positive or negative, this one will likely yeild some pretty dramatic swings and variability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyN6IYzbZVRe"
      },
      "source": [
        "## Optional Exercise\n",
        "\n",
        "Try to use the California data with a customized loss function. \n",
        "\n",
        "Note: this is a 20 way classification, so you'll probably want that many neurons on the output layer, an appropriate activation (softmax), and the y values will need to be run through np_utils.to_categorical. As well, think about the loss function, try categorical crossentropy.\n",
        "\n",
        "We'll look at activation and loss functions more next week. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNjj1-aHZVRe"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def priceIsRight(y_true, y_pred):\n",
        "    return K.switch(\n",
        "                    y_pred <= y_true, \n",
        "                    (y_true - y_pred) ** 2,\n",
        "                    (y_true - y_pred) ** 4\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Custom Function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAEhLMc6ZVRe"
      },
      "source": [
        "## Big Exercise - Newsgroup Classification\n",
        "\n",
        "Try to classify the newsgroup data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCHlBNRQZVRe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "remove = (\"headers\", \"footers\", \"quotes\")\n",
        "\n",
        "data_train = fetch_20newsgroups(\n",
        "    subset=\"train\", shuffle=True, remove=remove)\n",
        "\n",
        "data_test = fetch_20newsgroups(\n",
        "    subset=\"test\", shuffle=True, remove=remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1e1Iq4hZVRe",
        "outputId": "e2d62827-98eb-4a5b-87ea-e91475bd0b43"
      },
      "outputs": [],
      "source": [
        "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
        "X_train = news_tf.fit_transform(data_train.data)\n",
        "y_train = data_train.target\n",
        "X_test = news_tf.transform(data_test.data)\n",
        "y_test = data_test.target\n",
        "print(\"Train:\", X_train.shape, \"  Test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Tuning Summary vs Reality\n",
        "\n",
        "The techniques above generally expose a pattern that we can use to make accurate models:\n",
        "<ul>\n",
        "<li> Create a model that is accurate and overfitted. (Or setup process to generate an overfitted model)\n",
        "<li> Use hyperparameter tuning-ish methods to trial several different models. \n",
        "<li> Use tools such as regularization and early stopping to \"trim\" the overfitting back. \n",
        "</ul>\n",
        "\n",
        "For the most part, for what we are doing, this will probably work fine, without forcing us to put a tonne of thought into things upfront; we can just make a large model, then reduce it. In practice the main downside to this type of brute force approach is time, and by extension, cost. For us the datasets are mostly small enough that in an extreme case we could do something like setup a bunch of trials, let our computer train and test overnight, and wake up to a model that is pretty good. If our data was scaled up by a factor of 10,000 this becomes less practical. We would want to sample the data to make each trial run much more quickly, but we'd still be dealing with a non-trivial amount of processing time for each model that we want to try. Doing things that reduce the processing requirements of each epoch, such as feature selection, choosing a \"correctly\" sized model, or smart sets of hyperparameters to try, will reduce the amount of \"bad\" trials, and allow us to dedicate more time to \"good\" trials. This is one of the, relatively rare, scenarios that paying attention to processing time can have massive impacts on the end results. We'll look more at making a good sized model next. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "022_keras_tensor_sol.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml3950')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
