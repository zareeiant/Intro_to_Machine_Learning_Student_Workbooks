{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import linear_reg_demo_grad_desc\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Redux and Cost\n",
    "\n",
    "We covered linear regression quite a bit in stats, it is one of the most simple and intuitive methods to make a predictive model - one that most of us did intuitively when doing a best fit line back in math class. \n",
    "\n",
    "The linear regression process we looked at used the algorithm of Linear Least Squares, which tries to create a model (the line) that has the lowest average squared distance from all the points. That is, the MSE between the model and the real values is a measurement of \"badness\" of the model - the smaller this value is, the better the model. We can also think of this MSE calculation as something called a Cost Function - the higher the cost, the worse the model is. \n",
    "\n",
    "The fitting part of a linear regression model is an attempt to minimize this cost function. The algorithm looks for the parameters (not hyperparameters) that minimize the cost; in a linear regression those parameters are the coefficients and the y-intercept. \n",
    "\n",
    "The linear least sqaures can calculate this directly, so the process of \"looking\" for the minimum is only one step. If you think back to logistic regression we can see a more dramatic example, the algorithm needs to test, try, and repeat. A decision tree is another example, the gini/entropy is the cost, and the algorithm searches through all potential splits until it finds the one that is best. That type of iteritive process is really common. \n",
    "\n",
    "## Cost Functions\n",
    "\n",
    "The idea of cost is something that we will use throughout machine learning, it is critical for setting a goal that the algorithm can aim for when training. The cost function is just some function that measures the amount of error in a model, the lower the cost, the better a model we have. Usually this cost function is something that is a regular error metric that we have seen before - something like mean squared error for regression problems and accuracy or, more likely, cross-entropy for classifications. The cost to use is often something that we can specify as a hyperparameter when we create our models, such as the choice between gini and entropy in a tree. \n",
    "\n",
    "The cost function does not inherently need to be a regular error calculation, it could be almost any calculation at all. In certain weird situations the \"goodness\" of a model might not be measured accurately by a normal error calculation. Suppose you made a model to predict things on The Price is Right (https://priceisright.fandom.com/wiki/One_Bid); you want the model to make predictions that are close to the real price (probably well measured by MSE), however if the model predicts a price that is over, that's a tragedy and needs to be penalized severely. Maybe you'd want a model that calculated something more odd - MSE if the prediction is less than the real value, and residual^4 if the prediction is higher. \n",
    "\n",
    "#### Linear Regression Cost Function\n",
    "\n",
    "The cost function is just our old friend the Mean Squared Error:\n",
    "\n",
    "![Linear Regression Cost](images/lin_reg_cost.png \"Linear Regression Cost\" )\n",
    "\n",
    "<b>This is critical - the accuracy of our model is defined as some calculation of error (cost), and our model will keep training (updating its parameters) until that calculation of error meets its lowest point (or if it has reached a cut-off).</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random data for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random data\n",
    "X1 = 2 * np.random.rand(1000, 1)\n",
    "y1 = 4 + 3 * X1**3 + 3*np.random.randn(1000, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg = lin_reg.fit(X1, y1)\n",
    "\n",
    "preds1 = lin_reg.predict(X1)\n",
    "d1 = pd.DataFrame(X1, columns={\"X\"})\n",
    "d1[\"Y_pred\"] = preds1\n",
    "d1[\"Y_real\"] = y1\n",
    "sns.lineplot(data = d1, x=\"X\", y=\"Y_pred\", color=\"red\")\n",
    "sns.scatterplot(data=d1, x=\"X\", y=\"Y_real\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Results\n",
    "\n",
    "If we plot some data and generate a regression above, we have a solution that minimizes our cost function. There is no other model that we can create that has a lower mean squared residual between the model's predictions and the real values. Calculating the optimal model like we do here is great, however there are a couple of issues with that:\n",
    "\n",
    "<ul>\n",
    "<li>The computation cost when we have lots of data can become very large - growing exponentially. This can really matter when data grows massive. \n",
    "<li>Models that are not linear regression often can't be directly calculated (such as logistic regression). This is very common and we'll use gradient descent for things like neural networks later on. \n",
    "</ul>\n",
    "\n",
    "To deal with situations where we can't directly compute the optimal solution we need a different approach. Rather than determining the correct solution directly, we will make an attempt, evaluate the cost, make a slightly different attempt, and try to improve until we can't find a better cost score with subsequent attempts. This approach is called...\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a common approach to hunt for optimal solutions through an iteritive process. The process we can follow is:\n",
    "\n",
    "<ul>\n",
    "<li>Make an initial attempt to create a model, calculate the cost. This starting point is often random. \n",
    "<li>Compute the gradient - the derivitive or slope of the curve at that point. This indicates which direction to move. \n",
    "<li>Adjust the previous attempt, calculate the cost, compare to previous. The adjustment amount is determined by a value called the learning rate. \n",
    "<li>Repeat the previous step until moving does not improve the cost.\n",
    "</ul>\n",
    "\n",
    "We can visualize this process by looking at a curve of the cost function, and thinking about its derivitive or the slope. The gradient tells us two things:\n",
    "<ul>\n",
    "<li>Have we reached the best solution? If so, the gradient will be 0 indicating that we are at the minimum point on the curve. (i.e. there is no slope = we are at the bottom)\n",
    "<li>Which direction to go? We always want to go down the curve.\n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> This is the curve of the <i>cost</i>, not the actual model. The starting amount of error at our first attempt is the starting point, and the bottom of the curve is the lowest amount of error we can achieve. The algorithm is attempting to find the lowest point on the curve.\n",
    "\n",
    "![Gradient Descent](images/grad_desc.png \"Gradient Descent\" )\n",
    "\n",
    "When we've \"settled\" at the bottom, that is the lowest amount of cost, so there are no moves to make to find a better model. \n",
    "\n",
    "### Gradient Descent - From Scratch\n",
    "\n",
    "We can illustrate how gradient descent works. The file linear_reg_demo_grad_desc.py has an implementation of a linear regression that does gradient descent. I have modified it to return the set of predictions for each step of the training process. So what is happening in the background is:\n",
    "<ul>\n",
    "<li>Generate a linear regression. \n",
    "<li>Calculate the gradients.\n",
    "<li>Update the weights to move against the gradient (down the curve).\n",
    "<ul>\n",
    "<li>The size of the jump is defined by the learning rate. Big rate, big move!\n",
    "</ul>\n",
    "<li> Draw a line on the chart (controled by the num_show varaible). \n",
    "<li>Repeat until the trials are done. (In real implementations you'll also stop when improvement ends.)\n",
    "</ul>\n",
    "\n",
    "This example is what is called Batch Gradient Descent - at every step the entire process is recalculated. \n",
    "\n",
    "Play around with the learning rates and iterations and see how it progresses. The sliders control the settings for the gradient descent, if they aren't working, for whatever reason, just comment out the sliders and set the values directly in the code where shown in the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def f(a, b, c):\n",
    "    #display(a, b, c)\n",
    "    return a,b,c\n",
    "w = interactive(f, a=(0,.1,.01), b=(0,500,10), c=(1,50,1))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't work, change the commented section to use the preset values and comment out the slider parts. \n",
    "\n",
    "#Play around with some options and see the results!\n",
    "#learn_rate = .03 #Learning rate - how large to move each update.\n",
    "#iterations = 100 #How many iterations to run. \n",
    "#num_show = 10 #How ofen to chart the line, i.e. 1 = every prediction, 10 = every 10th, etc.. \n",
    "\n",
    "learn_rate = w.kwargs[\"a\"]\n",
    "iterations = w.kwargs[\"b\"]\n",
    "num_show = w.kwargs[\"c\"]\n",
    "\n",
    "\n",
    "train_rmses = []\n",
    "test_rmses =[]\n",
    "iters = []\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1)\n",
    "linreg = linear_reg_demo_grad_desc.LinearRegressionDemo(learning_rate=learn_rate, n_iters=iterations)\n",
    "preds, test_preds = linreg.fit(X_train, y_train.ravel(), X_test)\n",
    "sns.scatterplot(data=d1, x=\"X\", y=\"Y_real\")\n",
    "for i in range(len(preds)):\n",
    "    d_tmp = pd.DataFrame(X_train, columns={\"X\"})\n",
    "    d_tmp[\"Y_pred\"] = preds[i]\n",
    "    iters.append(i)\n",
    "    train_err = mean_squared_error(y_train, preds[i])\n",
    "    train_rmses.append(train_err)\n",
    "    test_err = mean_squared_error(y_test, test_preds[i])\n",
    "    test_rmses.append(test_err)\n",
    "    label = str(i) + \" - RMSE:\" + str(round(train_err, 3))\n",
    "    if (i%num_show) == 0:\n",
    "        sns.lineplot(data = d_tmp, x=\"X\", y=\"Y_pred\", label=label)\n",
    "d_iters = pd.DataFrame(iters, columns={\"Iteration\"})\n",
    "d_iters[\"Train\"] = train_rmses\n",
    "d_iters[\"Test\"] = test_rmses\n",
    "\n",
    "sns.lineplot(data = d1, x=\"X\", y=\"Y_pred\", color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descending\n",
    "\n",
    "The graph above shows the process of gradient descent. The algorithm proceeds by:\n",
    "<ul>\n",
    "<li> Generating the first model (line) and calculates the error of that model (RMSE).\n",
    "    <ul>\n",
    "    <li> This first try is (effectively) a guess. Real implementations use some logic to try to make the inital guess closer to accurate, but that isn't critical for the process. We just need some starting point.\n",
    "    </ul>\n",
    "<li> Calculate the <i><b>gradients</b></i> for the current error. \n",
    "    <ul>\n",
    "    <li> The gradients are the slopes of the error curve at the current point. \n",
    "    <li> The gradients tell us which direction to move to improve the error. \n",
    "    <li> This is the calculus bit, if we look back to the parabola of error image above, the algorithm calculates the slope of where it is on that error curve and moves towards the bottom. I.e. the slope tells the algorithm to go up or down. \n",
    "    </ul>\n",
    "<li> Increments the parameters (the slope and intercept in this case) either up or down a bit:\n",
    "    <ul>\n",
    "    <li> This is the key part of the process - the parameters are the model (think of how in linear regression slope and intercept are all we need to define a model), and here we adjust those parameters to make the model have a little less error - or be better fitted to the data.\n",
    "    <li> The size of the move is determined by the learning rate parameter.  \n",
    "    </ul>\n",
    "<li> Generate another model, calculate the error, and repeat the process. \n",
    "<li> When the error can't get lower by changing the parameters anymore (or the limit of trials is hit), finish. Whatever the parameters are at this point are the \"answer\".\n",
    "    <ul>\n",
    "    <li>Recall that the model is just the parameters, for a linear regression if we have the slope(s) and the intercept. For other algorithms the idea is the same, we'd just have different parameters, e.g. for a tree it would be the decision criteria.\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Each model generated at each step of training works its way bit by bit towards being more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Code Bits\n",
    "\n",
    "We can look at the .py file and examine some of the key bits (you can open it directly for details and full code)\n",
    "\n",
    "<b>perform gradient descent for n iterations</b>\n",
    "\n",
    "```python\n",
    "for _ in range(self.n_iters):\n",
    "    # get y_prediction\n",
    "    y_pred = self._get_prediction(X)\n",
    "    # compute gradients\n",
    "    dw, db = self._get_gradients(X, y, y_pred)\n",
    "    # update weights & bias with gradients\n",
    "    self._update_params(dw, db)\n",
    "```\n",
    "Here the fit function is pretty simple, we make a prediction, take that prediction to compute the gradients (slopes), then make and update to the weights...\n",
    "\n",
    "```python\n",
    "def _update_params(self, dw, db):\n",
    "    self.weights -= self.lr * dw\n",
    "    self.bias -= self.lr * db\n",
    "```\n",
    "Updating the weights is just incrementing them by the amount of the learning rate. \n",
    "\n",
    "```python\n",
    "def _get_gradients(self, X, y, y_pred):\n",
    "    # get distance between y_pred and y_true\n",
    "    error = y_pred - y\n",
    "    # compute the gradients of weight & bias\n",
    "    dw = (1 / self.n_samples) * np.dot(X.T, error)\n",
    "    db = (1 / self.n_samples) * np.sum(error)\n",
    "    return dw, db\n",
    "```\n",
    "Calculating the gradients is just a recalculation of the gradients at our new point on the cost curve. The math uses vector math (dot products) that we can ignore for now. \n",
    "\n",
    "So the overall process, repeated for N iterations is to create a prediction (make a linear regression model), calculate the gradients on the cost curve at that point, update our position based on the position by the amount of the learning rate, and repeat. Eventually we will reach a point where the gradients, the slope of the location on the cost curve, is 0, then we won't be updating anymore - think about why by looking at the _update_params function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors by Epoch\n",
    "\n",
    "We can visualize the errors by iteration, or epoch as it is often called. Here we can see how long it takes for us to narrow in on a solution. \n",
    "\n",
    "I've graphed both the test and train errors, here they tend to be extremely close and often flip-flop in terms of accuracy depending on random splits. This is not a constant pattern, it is due to the data. There are a few things that we can watch for in a chart like this, some common ones are:\n",
    "<ul>\n",
    "<li> If the testing error starts getting larger again after dropping, that is an indication we may be overfitting. \n",
    "<li> If the testing error is never \"catching\" the tratining error, that is an indication that we may be underfitting. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.lineplot(data=d_iters, x=\"Iteration\", y=\"Train\", label=\"Train\")\n",
    "sns.lineplot(data=d_iters, x=\"Iteration\", y=\"Test\", label=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm will work its way towards the solution. \n",
    "\n",
    "### Errors and Gradients\n",
    "\n",
    "For a simple linear regression with one feature like we have here, the challenge is pretty simple. Our cost will be convex (shaped like a bowl), and we will always be able to find some solution, even if it takes a long time. With more complex datasets this isn't always true, we don't have a simple 2D curve, we have something that is in X dimension - maybe 100 or more at times. We can end up with curves that resemble this:\n",
    "\n",
    "![Complex Gradient Descent](images/comp_grad_desc.png \"Complex Gradient Descent\" )\n",
    "\n",
    "Here we have things like a local minima - a point at which the cost is minimized, but only locally, not overall. We don't want the algorithm to get 'stuck' in one of these holes, because we'll find a low cost, but not the lowest. \n",
    "\n",
    "Dealing with issues like this is there the learning rate comes in. By ensuring the learning rate is large enough, we have our attempts 'jump around' a little. This results in progress towards the minimum cost that is a little slower, but it also gives the function the ability to 'jump out' of problems like local minima. There's a balance with the learning rate, not too high, not too low. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Types of Descents\n",
    "\n",
    "Batch gradient descent, like we did in the demo, suffers from the drawback that there is a lot of math involved in calculating the gradients, each time we make an adjustment to the parameters we have to do many calculations, this can be slow with large amounts of data. In practice, there are a couple of things that we can do to speed this up. \n",
    "\n",
    "In the sample each time we iterated we calculated all of those gradients. For simple 2 varible datasets that's no big deal, but when the data gets massive, that's pretty slow. There are other implementations of gradient descent that apply the same concept, but take some shortcuts to lessen the amount of calculations needed. We can visualize how each 'hunts' the solution - the batch descent progresses steadily, the stochastic jumps around semi-randomly, and the mini-batch is between the two:\n",
    "\n",
    "![Gradient Descent Patterns](images/grad_desc_patterns.png \"Gradient Descent Patterns\" )\n",
    "\n",
    "The process of gradient descent can be made faster by taking a shortcut in the calculations - rather than calculating the gradients for each record, we can calculate them for a subset of the records. These methods do the exact same thing and \"regular\" gradient descent, but when calculating the gradients they use only some of the records rather than all of them.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic (randomized) Gradient Descent, or SGD, speeds things up by just randomly selecting an instance and using that for the gradient calculation. The speed up comes from that massive reductions in the number of gradients calculated - instead of doing it for each record, SGD does it only once. Much faster. \n",
    "\n",
    "The downside to this is that there is much more randomness, and progress towards the solution isn't linear since you might randomly choose a point that is out of the way. This introduces some noise to the process, but the savings in computation time makes it generally worth it. \n",
    "\n",
    "#### Mini Batch Gradient Descent\n",
    "\n",
    "MBGD combines the ideas of batch and SGD - at each step a small random subset of the data is used for the gradient calculations. \n",
    "\n",
    "#### Gradients and Outcomes\n",
    "\n",
    "Note that the idea of all these algoritms is to generate a final model that is nearly the same, only the path there is different. Gradient descent isn't attempting to find a better model than linear regression (in a regular linear regression we can calculate the best model in a closed for solution), it is attempting to use a different method to reach that goal model. This idea of \"narrowing in\" on a solution can become even more useful when we don't have a solution that can be directly calculated - as long as we can set the cost function that defines accuracy, we can work our way towards an optimal solution. \n",
    "\n",
    "In general, the fewer records we use in calculating the gradient descent, the more randomized the progress towards the solution will be. This makes sense, as any one record in the data could be anything, so our progress can jump around more. \n",
    "\n",
    "<b>Note:</b> for this chart, m = number of rows, n = number of features. \n",
    "\n",
    "![Gradient Descent Outcomes](images/grad_desc_usage.png \"Gradient Descent Outcomes\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD in Practice\n",
    "\n",
    "We thankfully do not need to implement this descent process by hand in practice, we can use the built in SKlearn modules, and the idea is also built into several other algorithms. For linear regression we can use SGDregressor, a generic implementation of SGD.\n",
    "\n",
    "\n",
    "We can do some gradient descent with actual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/bodyfat.csv\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Datasets\n",
    "y2 = np.array(df[\"BodyFat\"]).reshape(-1,1)\n",
    "X2 = np.array(df.drop(columns={\"BodyFat\"}))\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Control Sample\n",
    "\n",
    "We can generate a standard linear regression model first, and see what the results are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closed Form Linear Regression\n",
    "pipe_LR_steps = [('scale', MinMaxScaler()), ('model', LinearRegression())]\n",
    "pipe_LR = Pipeline(pipe_LR_steps)\n",
    "\n",
    "start = time.process_time()\n",
    "pipe_LR.fit(X_train2, y_train2)\n",
    "print(time.process_time() - start)\n",
    "\n",
    "print('Training CrossVal Score:', cross_val_score(pipe_LR, X_train2, y_train2, cv=5))\n",
    "print('Testing score:', pipe_LR.score(X_test2, y_test2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression\n",
    "\n",
    "Now we can create a model using gradient descent. There are a few options that we can specify in the SGD that are important:\n",
    "<ul>\n",
    "<li>eta0 - the initial learning rate. \n",
    "<li>learning_rate - how the learning rate is managed. In the examples we made by hand, the learning rate was constant. The SGD implementation provides for a way to adapt the learning rate - as you get closer to a solution, the learning rate slows. The defult is \"invscaling\", defined as: eta = eta0 / pow(t, power_t). \n",
    "    <ul>\n",
    "    <li> t = number of updates.\n",
    "    <li> power_t = the exponent for inverse scaling learning rate.\n",
    "    <li>This has the effect of using large learning rates to quickly narrow down to a close solution, hopefully taking advantage of both that speed increase and the ability to \"jump\" out of local minima. As the algorithm progresses, the rate slows to close in directly on a solution. \n",
    "    </ul>\n",
    "<li>early_stopping - should the algorithm stop when it fails to improve. This will set aside a validation dataset, and if predictions for this set stop improving, end the training. If we look back to the demo, at some point the error flattens out and doesn't change much, early stopping stops at this point. We'll look at early stopping in more detail later on. \n",
    "<li>penalty - SGD applies regularization by default, which we'll discuss next time.\n",
    "</ul>\n",
    "\n",
    "<b>Note: Scaling values is very important in SGD algorithms. We'll probably get poor results if we forget.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Regressor. \n",
    "pipe_SGD_steps = [('scale', MinMaxScaler()), ('model', SGDRegressor(max_iter=10000, eta0=.1))]\n",
    "pipe_SGD = Pipeline(pipe_SGD_steps)\n",
    "\n",
    "start = time.process_time()\n",
    "pipe_SGD.fit(X_train2, y_train2.ravel())\n",
    "print(time.process_time() - start)\n",
    "\n",
    "#Print best model and test score\n",
    "print('Training CrossVal Score:', cross_val_score(pipe_SGD, X_train2, y_train2.ravel(), cv=5))\n",
    "print('Testing score:', pipe_SGD.score(X_test2, y_test2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the learning rate and see what impact it has on the speed of SGD as compared to regular linear regression. \n",
    "\n",
    "<b>Note:</b> for regression, the cross validation score is the R^2 value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in General\n",
    "\n",
    "When doing a linear regression, gradient descent isn't really <i>needed</i> because we can calculate a closed form solution, such as the linear least squares calculation. In other models, this isn't true - we start with an initial guess and have to use gradient descent to find the best solution. This is true with logistic regression, these implementations of linear regression, neural networks, and many other models.\n",
    "\n",
    "We'll spend more time on the inners workings of gradient descent when we get to neural networks - those types of models commonly do many rounds of training to get to an accurate model, often with massive datasets, and the gradient descent is a key part of that process.\n",
    "\n",
    "Next week we'll look at the next step in the process - regularization. Regularization is another way to improve the fit of our models, largely again by limiting overfitting. Regularization does its work by changing the way the cost funtion... functions, so the gradient descent process itself is manipulated to help us get a better model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Predict the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = pd.read_csv(\"data/house_data.csv\")\n",
    "df_ex.drop(columns={\"lat\",\"long\",\"zipcode\"}, inplace=True)\n",
    "df_ex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preview - we don't have missing values, but we do have some things to address. There looks to be some, at least partially, redundant variables - largely related to size. There are also some numeric measures that may actually be better as categorical variables.There are also a few things for spatial location that we can probably drop (I did it above). Also, I'll do some basic outlier filters. \n",
    "\n",
    "I'll try first with a simple model without much preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More Data Prep \n",
    "\n",
    "Try with some data transformations:\n",
    "<ul>\n",
    "<li> Make renovated into a boolean.\n",
    "<li> Make has a basement into a boolean.\n",
    "<li> Change yr_built to age. \n",
    "<li> Treat condition and grade as categorical variables.\n",
    "<li> Remove some redundant variables relating to size. \n",
    "<li> Treat floors as categorical.\n",
    "</ul>\n",
    "\n",
    "Will these work? We don't know - based on some domain knowledge (what we know about the housing market) these are reasonable steps that will make the data needed for our predictions slightly more simple, and might improve things... but we don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results... overall changes look good. The scores are slightly better, our model is smaller and simpler, we've removed some of the multicollinearity, and we've made the data easier to interpret."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Predict car prices using an SGD Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"data/CarPrice_Assignment.csv\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: There is probably a large amount of multicollinearity here. We'll try this example again with regularization soon, which is one way to deal with it in practice. For now, I won't address it, for simplicity. In general, we won't do every single thing that might be useful for every single example, it would just take too much time and space. That doesn't mean those things aren't relevant, we want to keep them for \"real\" applications.</b>\n",
    "\n",
    "First, Import some useful libraries for the preparation pipeline. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a column transformer to handle the categorical variables separately from the numeric ones. For the numeric variables I'm going to use a simple imputer (though we can see that there are no missing values, just for fun!) as well as a scaler since the features have different ranges. For the categorical variables I'm going to use a one hot encoder to create dummy variables.\n",
    "\n",
    "<b>Note:</b> Depending on randomness there may be a need to deal with rare categorical values. Keep this in mind, you should be able to find 2+ solutions by Googling the error. \n",
    "\n",
    "Assemble the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
