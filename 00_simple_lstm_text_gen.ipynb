{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:48:59.336784: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install Keras-Preprocessing\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>I cut my losses on 1/29 AMC options on Robinho...</td>\n",
       "      <td>1</td>\n",
       "      <td>l70kie</td>\n",
       "      <td>https://i.redd.it/5u1cwalki3e61.png</td>\n",
       "      <td>4</td>\n",
       "      <td>1.611878e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 02:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35401</th>\n",
       "      <td>Xl fleet will soon take more than amc from me</td>\n",
       "      <td>35</td>\n",
       "      <td>lq83uq</td>\n",
       "      <td>https://i.redd.it/vto8ehjcd5j61.jpg</td>\n",
       "      <td>19</td>\n",
       "      <td>1.614080e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-23 13:33:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Remember the kind of people behind Melvin Capital</td>\n",
       "      <td>3</td>\n",
       "      <td>l6wwwt</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611870e+09</td>\n",
       "      <td>[Business Insider: Former SAC Capital Employee...</td>\n",
       "      <td>2021-01-28 23:44:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19798</th>\n",
       "      <td>AMC GME Yes, I am still holdingðŸ’° Will buy more...</td>\n",
       "      <td>1408</td>\n",
       "      <td>l9hl1m</td>\n",
       "      <td>https://i.redd.it/jz8vfbunhpe61.jpg</td>\n",
       "      <td>192</td>\n",
       "      <td>1.612144e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-01 03:52:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43019</th>\n",
       "      <td>Holding</td>\n",
       "      <td>336</td>\n",
       "      <td>mc4sv4</td>\n",
       "      <td>https://i.redd.it/musbjsyc2zo61.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>1.616619e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-24 22:45:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31048</th>\n",
       "      <td>What I'll be using to spend my GME when I sell...</td>\n",
       "      <td>88</td>\n",
       "      <td>ldlk7l</td>\n",
       "      <td>https://i.redd.it/mm20csax4rf61.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>1.612600e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-06 10:28:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52842</th>\n",
       "      <td>77K PTRA YOLO (on margin)</td>\n",
       "      <td>103</td>\n",
       "      <td>ozbwja</td>\n",
       "      <td>https://i.redd.it/7a1siaxp0sf71.png</td>\n",
       "      <td>27</td>\n",
       "      <td>1.628273e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-08-06 20:56:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>The FACT that DeepFuckingValue might have star...</td>\n",
       "      <td>1</td>\n",
       "      <td>l732fa</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611884e+09</td>\n",
       "      <td>Anywhere you look currently there are no polit...</td>\n",
       "      <td>2021-01-29 03:26:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44092</th>\n",
       "      <td>Ride it or sell on next gap up?</td>\n",
       "      <td>9</td>\n",
       "      <td>mpur6o</td>\n",
       "      <td>https://i.redd.it/075l0wu28vs61.jpg</td>\n",
       "      <td>22</td>\n",
       "      <td>1.618316e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-04-13 15:07:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10974</th>\n",
       "      <td>So uhhhh... time to Occupy Wall Street again? ðŸ¤”</td>\n",
       "      <td>1</td>\n",
       "      <td>l71i24</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 02:32:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "8528   I cut my losses on 1/29 AMC options on Robinho...      1  l70kie   \n",
       "35401      Xl fleet will soon take more than amc from me     35  lq83uq   \n",
       "864    Remember the kind of people behind Melvin Capital      3  l6wwwt   \n",
       "19798  AMC GME Yes, I am still holdingðŸ’° Will buy more...   1408  l9hl1m   \n",
       "43019                                            Holding    336  mc4sv4   \n",
       "31048  What I'll be using to spend my GME when I sell...     88  ldlk7l   \n",
       "52842                          77K PTRA YOLO (on margin)    103  ozbwja   \n",
       "14576  The FACT that DeepFuckingValue might have star...      1  l732fa   \n",
       "44092                    Ride it or sell on next gap up?      9  mpur6o   \n",
       "10974    So uhhhh... time to Occupy Wall Street again? ðŸ¤”      1  l71i24   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "8528                 https://i.redd.it/5u1cwalki3e61.png          4   \n",
       "35401                https://i.redd.it/vto8ehjcd5j61.jpg         19   \n",
       "864    https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "19798                https://i.redd.it/jz8vfbunhpe61.jpg        192   \n",
       "43019                https://i.redd.it/musbjsyc2zo61.jpg         35   \n",
       "31048                https://i.redd.it/mm20csax4rf61.jpg         14   \n",
       "52842                https://i.redd.it/7a1siaxp0sf71.png         27   \n",
       "14576  https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "44092                https://i.redd.it/075l0wu28vs61.jpg         22   \n",
       "10974  https://www.reddit.com/r/wallstreetbets/commen...          2   \n",
       "\n",
       "            created                                               body  \\\n",
       "8528   1.611878e+09                                                NaN   \n",
       "35401  1.614080e+09                                                NaN   \n",
       "864    1.611870e+09  [Business Insider: Former SAC Capital Employee...   \n",
       "19798  1.612144e+09                                                NaN   \n",
       "43019  1.616619e+09                                                NaN   \n",
       "31048  1.612600e+09                                                NaN   \n",
       "52842  1.628273e+09                                                NaN   \n",
       "14576  1.611884e+09  Anywhere you look currently there are no polit...   \n",
       "44092  1.618316e+09                                                NaN   \n",
       "10974  1.611880e+09                                                NaN   \n",
       "\n",
       "                 timestamp  \n",
       "8528   2021-01-29 02:00:06  \n",
       "35401  2021-02-23 13:33:09  \n",
       "864    2021-01-28 23:44:03  \n",
       "19798  2021-02-01 03:52:17  \n",
       "43019  2021-03-24 22:45:40  \n",
       "31048  2021-02-06 10:28:36  \n",
       "52842  2021-08-06 20:56:46  \n",
       "14576  2021-01-29 03:26:08  \n",
       "44092  2021-04-13 15:07:09  \n",
       "10974  2021-01-29 02:32:50  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "train_text = pd.read_csv(train_text_file)\n",
    "train_text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = 1000\n",
    "OUTPUT_LENGTH = 25\n",
    "OUT_DIM = 8\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence to text words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = train_text['body']\n",
    "## Remove punctuation\n",
    "raw_text = raw_text.dropna()\n",
    "raw_text = raw_text.apply(lambda x: x.replace('[{}]'.format(string.punctuation), ''))\n",
    "vocab = set()\n",
    "sentences = []\n",
    "for sentence in raw_text:\n",
    "  current_sentence = text_to_word_sequence(sentence)\n",
    "  sentences.append(current_sentence)\n",
    "  vocab.update(current_sentence)\n",
    "#vocab\n",
    "#sentences\n",
    "max_length = max([len(sentence) for sentence in sentences])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x7fae3b0e4eb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert data to sequence of tokens \n",
    "def make_sequence(sentences):\n",
    "    input_sequences = []\n",
    "    for line in sentences:\n",
    "        for i in range(1, len(line)):\n",
    "            n_gram_sequence = line[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequenceGenerator(Sequence):\n",
    "    def __init__(self, sequences, batch_size):\n",
    "        self.index = 0\n",
    "        self.sequences = sequences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    #def on_epoch_end(self):\n",
    "        # Update\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        for i in range(index, index + self.batch_size):\n",
    "            sequence = self.sequences[i]\n",
    "            x = sequence[:-1]\n",
    "            y = sequence[-1]\n",
    "            x = tokenizer.texts_to_sequences(x)\n",
    "            x = pad_sequences(x, maxlen=max_length-1, padding='pre')\n",
    "            y = ku.to_categorical(y, num_classes=TOKENS)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = make_sequence(sentences)\n",
    "tok_seq = make_sequence(tokenized_sentences)\n",
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_94536/3616360772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_seq[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Pad\n",
    "text_gen = sequenceGenerator(sequences, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "### Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(vocab),output_dim=OUT_DIM,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(vocab), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(text_gen, epochs=10, verbose=1, steps_per_epoch=len(text_gen)//BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(train_text['body'].dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVGenerator(Sequence):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = file_path\n",
    "        self.index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = os.path.join(self.folder_path, self.files[index])\n",
    "        data = pd.read_csv(file_path)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        print(\"X: \", X, \"y: \", y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = CSVGenerator(train_text_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 09:27:05.557453: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=TOKENS,\n",
    "    #output_mode='int',\n",
    "    output_sequence_length=OUTPUT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n\nAttempt to convert a value (<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset'>) to a Tensor.\n\nCall arguments received by layer 'text_vectorization' (type TextVectorization):\n  â€¢ inputs=<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_53564/2146225311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvect_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvect_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n\nAttempt to convert a value (<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset'>) to a Tensor.\n\nCall arguments received by layer 'text_vectorization' (type TextVectorization):\n  â€¢ inputs=<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
     ]
    }
   ],
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices(train_text[\"body\"].dropna())\n",
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n",
    "\n",
    "vect_text = vectorize_layer(text_dataset)\n",
    "vect_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "We can clean and prep our text here. The data cleanup we need is to:\n",
    "<ul>\n",
    "<li> Remove punctuation.\n",
    "<li> Tokenize the text, as we did previously in NLP processing. \n",
    "<li> <b>Generate sequences of tokens.</b> This is the key to the LSTM model, we are structuring the data to be a sequence of tokens. Our model will attempt to predict the next token, which in this case is the next word in the sentence.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  strip_punct = train_text[\"body\"].dropna().str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstrip_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0minp_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence_of_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0minp_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_50997/955513030.py\u001b[0m in \u001b[0;36mget_sequence_of_tokens\u001b[0;34m(corpus, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mn_gram_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \"\"\"\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_sequence_of_tokens(corpus, tokenizer):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "strip_punct = train_text[\"body\"].dropna().str.replace('[{}]'.format(string.punctuation), '')\n",
    "inp_seq, total_words = get_sequence_of_tokens(strip_punct, Tokenizer())\n",
    "inp_seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Prep - Padding and Targets\n",
    "\n",
    "We also need to take the sequences and pad them, or make them all the same length. We will also create the targets - the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we model. The data that we made mirrors the construction of a sentence.\n",
    "<ul>\n",
    "<li> X features - the sentence up to this point. \n",
    "<li> Y target - the word(s) that should come next. \n",
    "</ul>\n",
    "\n",
    "So, the model is effectively working to generate text just like a time series model works to predict the next value in a sequence of stock prices or hourly temperature. We train the model on, hopefully a large number of senteneces, where is sees many examples of \"here are some words\" (X values) and \"here is the next word\" (Y value). If we give it lots and lots of that training data, it should become better and better at determining what should come next, given the existing sentence. \n",
    "\n",
    "To do this well, we'd need a lot more data than we have, and much more time to train. We'd want to give the model enough data so that it can see lots and lots of examples of the same word in different contexts, and of similar contexts with different words. The patterns of language are really complex, so we need data that provides enough variation to demonstrate the patterns. \n",
    "\n",
    "The model is wrapped in a little function, so we can make a model to output a different number of words with more convenience.\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "We also use an embedding layer here, which accepts our enocoded inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, output_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layers - LSTM Layer\n",
    "    model.add(LSTM(100, return_sequences = True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, OUTPUT_LENGTH)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can create a little function to generate text. We can give it a seed text, and it will generate text based on that. We can also give it a number of words to generate, and it will generate that many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
