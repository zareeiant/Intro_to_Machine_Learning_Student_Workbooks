{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install Keras-Preprocessing\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "  plt.plot(history.history['acc'], label='accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM\n",
    "\n",
    "As we have seen, LSTM models are excellent at dealing with sequential data. As luck would have it, text is also sequental data! We can train a model to predict the next word in a sentence, then use that smarts to generate new text. Basically ChatGPT, but far better. \n",
    "\n",
    "### Text for Training\n",
    "\n",
    "We need some text from which to train our model to speak, I captured a small extract of text from Reddit posts, which vaguely resembles actual language. We'll first need to clean up our data a bit before we can assemble it for modelling. The inital cleaning bits are just like what we used in NLP, we just need to get rid of all the junk. \n",
    "\n",
    "We can use pretty much anything that you can imagine as source, and assuming we can gather enough data and train our model, the generated speech will be styled after the source. I liken it to going on vacation in Indonesia and talking to Indonesians who spoke English like Australian surfer bros - their training data was a little weird, so the output was a little weird too. If you're looking to build your best ChatGPT competitor you will want a lot of data, specifically a lot of data that is representative of the full gamut of how you want your model to write. If you want slang in the new text, you can't really train on Shakespeare and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162100</th>\n",
       "      <td>xxxxy xxxxxxxx eaither went business sold comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53852</th>\n",
       "      <td>contacted many time failed provide necessary d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89049</th>\n",
       "      <td>started receiving account statement suntrust t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29485</th>\n",
       "      <td>victim identity notified collection creditor s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23310</th>\n",
       "      <td>tried file dispute online equifax continue get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79532</th>\n",
       "      <td>used direct express debit card corner store at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55667</th>\n",
       "      <td>filed dispute regard incorrect item credit rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76358</th>\n",
       "      <td>sent equifax proof true debt owed xxxxxxxx ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112236</th>\n",
       "      <td>looking buy specific breed cat online research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145282</th>\n",
       "      <td>toyota financial service reported husband cred...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                narrative\n",
       "162100  xxxxy xxxxxxxx eaither went business sold comp...\n",
       "53852   contacted many time failed provide necessary d...\n",
       "89049   started receiving account statement suntrust t...\n",
       "29485   victim identity notified collection creditor s...\n",
       "23310   tried file dispute online equifax continue get...\n",
       "79532   used direct express debit card corner store at...\n",
       "55667   filed dispute regard incorrect item credit rep...\n",
       "76358   sent equifax proof true debt owed xxxxxxxx ord...\n",
       "112236  looking buy specific breed cat online research...\n",
       "145282  toyota financial service reported husband cred..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reddit WSB data\n",
    "#train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "#USE_COL = \"body\"\n",
    "\n",
    "# Complaint data\n",
    "train_text_file = keras.utils.get_file('train_text3.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/complaints_processed_clean.csv')\n",
    "USE_COL = \"narrative\"\n",
    "\n",
    "train_text = pd.read_csv(train_text_file)\n",
    "train_text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = 500\n",
    "OUTPUT_LENGTH = 45\n",
    "OUT_DIM = 8\n",
    "BATCH_SIZE = 128\n",
    "SAMP = 7\n",
    "SHUFFLE = 500\n",
    "UNITS = 50\n",
    "\n",
    "#Some larger values for Colab\n",
    "TEST_EPOCHS = 5\n",
    "TEST_BATCH = 512\n",
    "\n",
    "if IN_COLAB:\n",
    "    TEST_EPOCHS = 200\n",
    "    TEST_BATCH = 1024\n",
    "    !mkdir weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Build Sequences\n",
    "\n",
    "We can clean and process the text data with any strategy. Here we'll use a simple strategy of removing punctuation and converting all text to lowercase.\n",
    "\n",
    "Next we'll use a Keras utility to convert our text into a sequence of words, which we'll then tokenize into integers. The sequence that is created is pretty simple, just a list of all the words in the sentence in the correct order. Basically an OCD version of a normal sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account',\n",
       " 'reported',\n",
       " 'abbreviated',\n",
       " 'name',\n",
       " 'full',\n",
       " 'name',\n",
       " 'servicer',\n",
       " 'account',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'date',\n",
       " 'last',\n",
       " 'payment',\n",
       " 'reporting',\n",
       " 'payment',\n",
       " 'due',\n",
       " 'account',\n",
       " 'time',\n",
       " 'however',\n",
       " 'date',\n",
       " 'inconsistent',\n",
       " 'however',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'show',\n",
       " 'show',\n",
       " 'none',\n",
       " 'date',\n",
       " 'match',\n",
       " 'activity',\n",
       " 'payment',\n",
       " 'made',\n",
       " 'three',\n",
       " 'bureau',\n",
       " 'reporting',\n",
       " 'high',\n",
       " 'credit',\n",
       " 'balance',\n",
       " 'show',\n",
       " 'transunion',\n",
       " 'show',\n",
       " 'amount',\n",
       " 'violation',\n",
       " 'failed',\n",
       " 'addressed',\n",
       " 'repeatedly',\n",
       " 'amount',\n",
       " 'match',\n",
       " 'information',\n",
       " 'reported',\n",
       " 'percent',\n",
       " 'accurately',\n",
       " 'according',\n",
       " 'law',\n",
       " 'transunion',\n",
       " 'repeatedly',\n",
       " 'failed',\n",
       " 'report',\n",
       " 'account',\n",
       " 'dispute',\n",
       " 'removed',\n",
       " 'account',\n",
       " 'dispute',\n",
       " 'comment',\n",
       " 'company',\n",
       " 'reporting',\n",
       " 'account',\n",
       " 'top',\n",
       " 'invalid',\n",
       " 'account',\n",
       " 'number',\n",
       " 'also',\n",
       " 'listed',\n",
       " 'authorized',\n",
       " 'user',\n",
       " 'considered',\n",
       " 'account',\n",
       " 'removed',\n",
       " 'another',\n",
       " 'account',\n",
       " 'reporting',\n",
       " 'listed',\n",
       " 'authorized',\n",
       " 'user',\n",
       " 'considered',\n",
       " 'account',\n",
       " 'reporting',\n",
       " 'high',\n",
       " 'credit',\n",
       " 'limit',\n",
       " 'credit',\n",
       " 'limit',\n",
       " 'left',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'high',\n",
       " 'credit',\n",
       " 'amount',\n",
       " 'credit',\n",
       " 'limit',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'reporting',\n",
       " 'date',\n",
       " 'supposed',\n",
       " 'match',\n",
       " 'since',\n",
       " 'supposed',\n",
       " 'receive',\n",
       " 'exact',\n",
       " 'information',\n",
       " 'company',\n",
       " 'date',\n",
       " 'last',\n",
       " 'payment',\n",
       " 'transunion',\n",
       " 'show',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'also',\n",
       " 'show',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'date',\n",
       " 'last',\n",
       " 'payment',\n",
       " 'show',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'date',\n",
       " 'last',\n",
       " 'payment',\n",
       " 'none',\n",
       " 'date',\n",
       " 'company',\n",
       " 'match',\n",
       " 'reported',\n",
       " 'accurately',\n",
       " 'still',\n",
       " 'also',\n",
       " 'making',\n",
       " 'payment',\n",
       " 'account',\n",
       " 'since',\n",
       " 'paid',\n",
       " 'far',\n",
       " 'two',\n",
       " 'year',\n",
       " 'payment',\n",
       " 'history',\n",
       " 'even',\n",
       " 'reflect',\n",
       " 'none',\n",
       " 'bureau',\n",
       " 'reporting',\n",
       " 'information',\n",
       " 'verified',\n",
       " 'account',\n",
       " 'still',\n",
       " 'reporting',\n",
       " 'wrong',\n",
       " 'making',\n",
       " 'payment',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'company',\n",
       " 'show',\n",
       " 'payment',\n",
       " 'history',\n",
       " 'co',\n",
       " 'however',\n",
       " 'show',\n",
       " 'making',\n",
       " 'payment',\n",
       " 'since',\n",
       " 'reporting',\n",
       " 'wrong',\n",
       " 'show',\n",
       " 'date',\n",
       " 'last',\n",
       " 'active',\n",
       " 'transunion',\n",
       " 'making',\n",
       " 'payment',\n",
       " 'account',\n",
       " 'since',\n",
       " 'however',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'date',\n",
       " 'last',\n",
       " 'payment',\n",
       " 'show',\n",
       " 'last',\n",
       " 'reported',\n",
       " 'date',\n",
       " 'show',\n",
       " 'transunion',\n",
       " 'show',\n",
       " 'information',\n",
       " 'inaccurate',\n",
       " 'reported',\n",
       " 'inconsistently',\n",
       " 'three',\n",
       " 'bureau',\n",
       " 'verify',\n",
       " 'information',\n",
       " 'correct',\n",
       " 'attached',\n",
       " 'evidence',\n",
       " 'refuse',\n",
       " 'change',\n",
       " 'company',\n",
       " 'obligated',\n",
       " 'fcra',\n",
       " 'report',\n",
       " 'information',\n",
       " 'totally',\n",
       " 'accurate',\n",
       " 'severely',\n",
       " 'affecting',\n",
       " 'credit',\n",
       " 'score',\n",
       " 'equifax',\n",
       " 'also',\n",
       " 'reporting',\n",
       " 'credit',\n",
       " 'limit',\n",
       " 'transunion',\n",
       " 'reporting',\n",
       " 'reporting',\n",
       " 'high',\n",
       " 'credit',\n",
       " 'transunion',\n",
       " 'showing',\n",
       " 'three',\n",
       " 'bureau',\n",
       " 'reporting',\n",
       " 'past',\n",
       " 'due',\n",
       " 'balance',\n",
       " 'violation',\n",
       " 'still',\n",
       " 'corrected',\n",
       " 'showing',\n",
       " 'past',\n",
       " 'due',\n",
       " 'amount',\n",
       " 'transunion',\n",
       " 'show',\n",
       " 'account',\n",
       " 'already',\n",
       " 'carrying',\n",
       " 'balance',\n",
       " 'reflect',\n",
       " 'past',\n",
       " 'due',\n",
       " 'amount',\n",
       " 'violation',\n",
       " 'company',\n",
       " 'refusing',\n",
       " 'proper',\n",
       " 'investigation',\n",
       " 'error',\n",
       " 'even',\n",
       " 'asked',\n",
       " 'method',\n",
       " 'verification',\n",
       " 'provide',\n",
       " 'information',\n",
       " 'requested',\n",
       " 'allowed',\n",
       " 'law',\n",
       " 'request']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = train_text[USE_COL]\n",
    "## Remove punctuation\n",
    "raw_text = raw_text.dropna()\n",
    "raw_text = raw_text.apply(lambda x: x.replace('[{}]'.format(string.punctuation), '').lower())\n",
    "vocab = set()\n",
    "sentences = []\n",
    "for sentence in raw_text:\n",
    "  current_sentence = text_to_word_sequence(sentence)\n",
    "  sentences.append(current_sentence)\n",
    "  vocab.update(current_sentence)\n",
    "#vocab\n",
    "#sentences\n",
    "max_length = max([len(sentence) for sentence in sentences])\n",
    "sentences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "We can take the text that we split above and encode it as a sequence of integers. We'll then use the tokenizer to convert our sequences into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 56,\n",
       " 55,\n",
       " 113,\n",
       " 55,\n",
       " 2,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 28,\n",
       " 2,\n",
       " 7,\n",
       " 136,\n",
       " 30,\n",
       " 136,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 145,\n",
       " 145,\n",
       " 30,\n",
       " 343,\n",
       " 5,\n",
       " 36,\n",
       " 202,\n",
       " 29,\n",
       " 6,\n",
       " 1,\n",
       " 26,\n",
       " 145,\n",
       " 186,\n",
       " 145,\n",
       " 38,\n",
       " 121,\n",
       " 243,\n",
       " 38,\n",
       " 4,\n",
       " 56,\n",
       " 382,\n",
       " 82,\n",
       " 186,\n",
       " 243,\n",
       " 3,\n",
       " 2,\n",
       " 18,\n",
       " 65,\n",
       " 2,\n",
       " 18,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 25,\n",
       " 23,\n",
       " 134,\n",
       " 337,\n",
       " 2,\n",
       " 65,\n",
       " 128,\n",
       " 2,\n",
       " 6,\n",
       " 134,\n",
       " 337,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 412,\n",
       " 1,\n",
       " 412,\n",
       " 440,\n",
       " 186,\n",
       " 6,\n",
       " 1,\n",
       " 38,\n",
       " 1,\n",
       " 412,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 6,\n",
       " 30,\n",
       " 64,\n",
       " 120,\n",
       " 4,\n",
       " 8,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 186,\n",
       " 145,\n",
       " 30,\n",
       " 126,\n",
       " 23,\n",
       " 145,\n",
       " 30,\n",
       " 126,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 145,\n",
       " 30,\n",
       " 126,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 30,\n",
       " 8,\n",
       " 56,\n",
       " 49,\n",
       " 23,\n",
       " 318,\n",
       " 5,\n",
       " 2,\n",
       " 64,\n",
       " 44,\n",
       " 131,\n",
       " 58,\n",
       " 5,\n",
       " 235,\n",
       " 67,\n",
       " 29,\n",
       " 6,\n",
       " 4,\n",
       " 171,\n",
       " 2,\n",
       " 49,\n",
       " 6,\n",
       " 260,\n",
       " 318,\n",
       " 5,\n",
       " 282,\n",
       " 64,\n",
       " 8,\n",
       " 145,\n",
       " 5,\n",
       " 235,\n",
       " 136,\n",
       " 145,\n",
       " 318,\n",
       " 5,\n",
       " 64,\n",
       " 6,\n",
       " 260,\n",
       " 145,\n",
       " 30,\n",
       " 126,\n",
       " 186,\n",
       " 318,\n",
       " 5,\n",
       " 2,\n",
       " 64,\n",
       " 136,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 145,\n",
       " 126,\n",
       " 56,\n",
       " 30,\n",
       " 145,\n",
       " 186,\n",
       " 145,\n",
       " 4,\n",
       " 73,\n",
       " 56,\n",
       " 202,\n",
       " 29,\n",
       " 200,\n",
       " 4,\n",
       " 149,\n",
       " 147,\n",
       " 301,\n",
       " 383,\n",
       " 338,\n",
       " 8,\n",
       " 119,\n",
       " 3,\n",
       " 4,\n",
       " 185,\n",
       " 1,\n",
       " 95,\n",
       " 141,\n",
       " 23,\n",
       " 6,\n",
       " 1,\n",
       " 412,\n",
       " 186,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 186,\n",
       " 192,\n",
       " 202,\n",
       " 29,\n",
       " 6,\n",
       " 167,\n",
       " 28,\n",
       " 26,\n",
       " 121,\n",
       " 49,\n",
       " 192,\n",
       " 167,\n",
       " 28,\n",
       " 38,\n",
       " 186,\n",
       " 145,\n",
       " 2,\n",
       " 214,\n",
       " 26,\n",
       " 167,\n",
       " 28,\n",
       " 38,\n",
       " 121,\n",
       " 8,\n",
       " 470,\n",
       " 96,\n",
       " 163,\n",
       " 67,\n",
       " 72,\n",
       " 479,\n",
       " 219,\n",
       " 86,\n",
       " 4,\n",
       " 105,\n",
       " 82,\n",
       " 47]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=TOKENS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "tokenized_sentences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Sequences\n",
    "\n",
    "We are going to be a little slack in the construction of the datasets for training because we are limited in the amount of resources we can handle. All of the datasets that are fed to the model need to be the same length, so we'll set a cap and trucate it here for resource concerns. Our dataset will be constructed as:\n",
    "<ul>\n",
    "<li> A sequence of (up to) 24 words as the X data. \n",
    "<li> The next word as the Y data.\n",
    "</ul>\n",
    "\n",
    "So each sequence is effectively one set of features, and its target is the next word. If we were doing this in reality, we'd want to prep more records from our sample:\n",
    "<ul>\n",
    "<li> Suppose a sample sentence is \"The quick brown fox jumps over the lazy dog\". Our ideal data would have something like:\n",
    "    <ul> \n",
    "    <li>X = \"the quick brown\", Y = \"fox\"\n",
    "    <li> X = \"quick brown fox\", Y = \"jumps\"\n",
    "    <li> X = \"brown fox jumps\", Y = \"over\"\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This is superior both because we are generating much more data to train the model and because we are training the model to predict words in all different positions in the sentence. We'd be predicting almost every word in the training dataset. The words that frequently end a sentence are not necessarily the same as the words that start a sentence or sit in the middle, so making predictions up and down the text will likely lead to a more useful model.\n",
    "\n",
    "<b>We'd end up with a better model if we generated more sequences from our data and/or added more data. The resource demands make that tough, so we have cut some corners that are easy to remedy in a real-world scenario.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest real sequence: 1548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 56,\n",
       " 55,\n",
       " 113,\n",
       " 55,\n",
       " 2,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 28,\n",
       " 2,\n",
       " 7,\n",
       " 136,\n",
       " 30,\n",
       " 136,\n",
       " 186,\n",
       " 6,\n",
       " 30,\n",
       " 126,\n",
       " 145,\n",
       " 145,\n",
       " 30,\n",
       " 343,\n",
       " 5,\n",
       " 36,\n",
       " 202,\n",
       " 29,\n",
       " 6,\n",
       " 1,\n",
       " 26,\n",
       " 145,\n",
       " 186,\n",
       " 145,\n",
       " 38,\n",
       " 121,\n",
       " 243,\n",
       " 38,\n",
       " 4,\n",
       " 56,\n",
       " 382,\n",
       " 82]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find longest sequence\n",
    "max_real_length = max([len(sentence) for sentence in tokenized_sentences])\n",
    "print(\"Longest real sequence:\", max_real_length)\n",
    "if max_real_length > OUTPUT_LENGTH:\n",
    "  extra_sequences = [s_[OUTPUT_LENGTH:] for s_ in tokenized_sentences]\n",
    "  trunc_token_sequences = [t_[:OUTPUT_LENGTH] for t_ in tokenized_sentences]\n",
    "trunc_token_sequences = trunc_token_sequences + extra_sequences\n",
    "trunc_token_sequences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad Sequences\n",
    "\n",
    "We need to pad our sequences so that they are all the same length, as our neural networks require that. The pad_sequences utility does just that, it will fill 0s at either the beginning or end of the sequence, depending on the \"padding\" option, and make everything the same length. We want to pad before the real data, because we are always planning on predicting the next token, so we want to work from the last value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  56,  55, 113,  55,   2, 186,   6,  30, 126,   5,   6,   5,\n",
       "        28,   2,   7, 136,  30, 136, 186,   6,  30, 126, 145, 145,  30,\n",
       "       343,   5,  36, 202,  29,   6,   1,  26, 145, 186, 145,  38, 121,\n",
       "       243,  38,   4,  56, 382,  82], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_trunc_token_sequences = pad_sequences(trunc_token_sequences, maxlen=OUTPUT_LENGTH, padding='pre')\n",
    "padded_trunc_token_sequences[SAMP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Tokenized Data to Disk\n",
    "\n",
    "We can write the tokenized data to disk so that we can use it later. This will save us from having to redo that step that is slow. Since we chopped our data size down a bit, this isn't super needed. I tried to generate all of the sequences noted above and I both exceeded the Colab memory limits and it took a while to run. I wouldn't want to repeat that if I can avoid it, and hard drive space is cheap, so writing the interim results to disk is a good work around. In real scenarios when we had massive amounts of data we would need to load it incrementally from disk anyway, so this is a free win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_path = 'data/sample_tokenized_sentences.csv'\n",
    "token_path = 'data/padded_sample_tokenized_sentences.csv'\n",
    "if IN_COLAB:\n",
    "  !mkdir data\n",
    "  \n",
    "if os.path.exists(token_path):\n",
    "    df_prepped = pd.read_csv(token_path, header=None)\n",
    "    prepped_sentences = np.array(df_prepped.values.tolist())\n",
    "else:\n",
    "    df_prepped = pd.DataFrame(padded_trunc_token_sequences)\n",
    "    df_prepped.to_csv(token_path, header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Array\n",
    "\n",
    "This data can be reasonably used either as a regular array or or a tensorflow dataset. We'll use both here, just to show that it can be done either way. If we we using the full set of sequences the data would be quite a bit larger with all the fully padded out sequences, we'd probably need to write the data to disk and load it with a dataset or generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324822, 45)\n",
      "Y shape: (324822, 500)\n",
      "X shape: (324822, 44)\n"
     ]
    }
   ],
   "source": [
    "print(padded_trunc_token_sequences.shape)\n",
    "y_t = padded_trunc_token_sequences[:, -1].reshape(-1, 1)\n",
    "y_t = ku.to_categorical(y_t, num_classes=TOKENS)\n",
    "print(\"Y shape:\", y_t.shape)\n",
    "X_t = padded_trunc_token_sequences[:, :-1]\n",
    "print(\"X shape:\", X_t.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we model. The data that we made mirrors the construction of a sentence.\n",
    "<ul>\n",
    "<li> X features - the sentence up to this point. \n",
    "<li> Y target - the word(s) that should come next. \n",
    "</ul>\n",
    "\n",
    "So, the model is effectively working to generate text just like a time series model works to predict the next value in a sequence of stock prices or hourly temperature. We train the model on, hopefully a large number of senteneces, where is sees many examples of \"here are some words\" (X values) and \"here is the next word\" (Y value). If we give it lots and lots of that training data, it should become better and better at determining what should come next, given the existing sentence. \n",
    "\n",
    "To do this well, we'd need a lot more data than we have, and much more time to train. We'd want to give the model enough data so that it can see lots and lots of examples of the same word in different contexts, and of similar contexts with different words. The patterns of language are really complex, so we need data that provides enough variation to demonstrate the patterns. <b>The actual performance of our model at creating sensible text will be bad, we should not get our hopes up, but the process is sound.</b>\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "We also use an embedding layer here, which accepts our integer inputs and converts them to a vector of a specified size. This is a way of representing the words in a way that is more useful for the model. We saw embeddings with word2vec during the NLP portions. Just as it did then, embedding will represent each of our tokens as a vector of a specified size, or as a value in N dimensional space. \n",
    "\n",
    "![Embedding](images/embedding.png \"Embedding\")\n",
    "\n",
    "When we check the summary of the model we can see that the embedding layer has a lot of parameters, this is because it is learning the vector representation of each of the words in our vocabulary. When we used word2vec we used a pretrained model - the N dimensional space was already defined, and we placed our words into it. Here, we are letting the model learn the space and the vectors that represent the words, so the vector representation of each word is learned directly from the data. If we are dealing with a scenarion where we have a lot of data and a specialized vocabulary (such as in industry) this can be very useful. The model will learn which words are similar and which are not, based on the context of the text provided in training.\n",
    "\n",
    "If we consider embedding from the perspective of the actual prediction, it also makes a lot of sense that this is a prefered approach. We are not generally trying to predict the exact word, as we would with a classification that chooses the correct answer from a list of options. When dealing with language we are more likely trying to predict the next <i>type</i> of word - both if it is a noun/verb/etc and the meaning. Embedding the words allows us to use that multidimensional representation aim for words that are similar. When dealing with generative models with large vocabularies our model will follow \"Do you want to go out sometime and grab a ________\" with something like \"beer\" or \"coffee\" which are similar in this context, and close in their embedding representation. Using embeddings, the loss that is calculated each round is measuring the distance between the predicted word and the actual word in N dimensions, so grabbing words that are very close, or similar, is expected and desired. \n",
    "\n",
    "#### Let's Go Bi\n",
    "\n",
    "For this model we'll use a bidirectional LSTM layer. This is a layer that will process the input sequence in both directions, so it will see the sequence from the beginning and from the end. This is useful because it allows the model to consider what should come next, but also what should come before, which can be useful for determining the next word. The idea of a bidirectional layer is that if our data is in a sequence we can feed it through normally, but we can also reverse it. Doing this gives the model a differnet view on the data, and especially in cases like language creation, where the meaning of the output can depend on the entirety of the sequence. \n",
    "\n",
    "![Bidirectional LSTM](images/bidirectional.webp \"Bidirectional LSTM\")\n",
    "\n",
    "Bidirectional layers are most prevalent in NLP, as adding the ability to look at the sequence of words in both directions can really help models build a better understanding of the context of the words. There is a speed penalty due to each layer doing roughly double the number of calculations, but for language tasks such as this, it is pretty likely to be worth it, particularly if we were to expand the dataset. Bidirectional layers can be added simply, as below; on the Keras documentation there are a few more details, mainly that the forward and backward layers can be configured separately then combined. When using a bidirectional layer, one option that we should commonly pay attention to is the \"merge_mode\" option, which determines how the forward and backward layers are combined to produce the final output. The default is \"concat\" which combines the outputs by concatenating them, but there are other options such as \"sum\" or \"ave\" which can be useful in some cases - hyperparameter tuning is the way to find the \"best\" option. We will use the default here, and can sidestep the other details of bidirectional layers for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 44, 8)             4000      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 44, 100)          23600     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 44, 100)          60400     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 100)              60400     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               50500     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,900\n",
      "Trainable params: 198,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=TOKENS, output_dim=OUT_DIM, input_length=OUTPUT_LENGTH-1))\n",
    "#model.add(LSTM(UNITS, return_sequences=True))\n",
    "#model.add(LSTM(UNITS, return_sequences=True))\n",
    "#model.add(LSTM(UNITS))\n",
    "model.add(Bidirectional(LSTM(UNITS, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(UNITS, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(UNITS)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(TOKENS, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In making this, lots of epochs definately gave better results, as did a swap to bidirectional layers. I saw this in both the raw accuracy scores and the relative sense of the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "635/635 [==============================] - 542s 835ms/step - loss: 3.9244 - acc: 0.3266\n",
      "Epoch 2/5\n",
      "635/635 [==============================] - 480s 756ms/step - loss: 3.6631 - acc: 0.3486\n",
      "Epoch 3/5\n",
      "635/635 [==============================] - 570s 897ms/step - loss: 3.5028 - acc: 0.3746\n",
      "Epoch 4/5\n",
      "635/635 [==============================] - 707s 1s/step - loss: 3.3980 - acc: 0.3971\n",
      "Epoch 5/5\n",
      "300/635 [=============>................] - ETA: 8:21 - loss: 3.3405 - acc: 0.4099"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_8420/2449695798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights/lstm_gen_weights.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_BATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml3950/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try with dataframes\n",
    "early_stop = EarlyStopping(monitor='loss', patience=50)\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint('weights/lstm_gen_weights.h5', save_best_only=True, monitor='loss', mode='min', save_weights_only=True)\n",
    "\n",
    "history = model.fit(X_t, y_t, batch_size=TEST_BATCH, epochs=TEST_EPOCHS, verbose=1, callbacks=[early_stop, save_weights])\n",
    "plot_acc(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't changed anything, this will pick up where the training above left off, we really only need one or the other, but we're getting wild and crazy here. If you're running this locally, one or the other will probably do the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_t, y_t)).shuffle(SHUFFLE).batch(TEST_BATCH).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 28s 554ms/step - loss: 5.3921 - acc: 0.0538\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 28s 567ms/step - loss: 5.4334 - acc: 0.0538\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 25s 516ms/step - loss: 5.4142 - acc: 0.0538\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 26s 534ms/step - loss: 5.4069 - acc: 0.0538\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 26s 530ms/step - loss: 5.3859 - acc: 0.0563\n"
     ]
    }
   ],
   "source": [
    "# Or with datasets...\n",
    "history = model.fit(train_ds, epochs=TEST_EPOCHS, verbose=1, callbacks=[early_stop, save_weights])\n",
    "plot_acc(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Text\n",
    "\n",
    "We can generate text once the model is trained. We'll start with a seed sentence, and then we'll use the model to predict the next word. We'll then append that word to the sentence and use the model to predict the next word, and so on. There is a little helper function to do this for us with limited repetition.\n",
    "\n",
    "Another new thing is that we create an inverse dictionary to map the encoded words back to the original words.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "One weirdly named factor that is important in text generation is the temperature. The temperature is a factor that we can use to control the randomness of the output. To generate text, we are essentially using a probability distribution to determine what the next word should be - the softmax output of the model will tell us the most likely next word. The issue is that certain words are way more likely than others - \"the\", \"it\", \"a\", \"and\", etc. are all very common words so we can expect the model to predict them as \"most likely\" a lot, probably too often. \n",
    "\n",
    "![Temperature](images/temperature.gif \"Temperature\")\n",
    "\n",
    "The most direct way to combat this is to add some degree of randomness to which word we select - we'll still pick the most likely word more often than any other, but we'll also pick other words that have some degree of likelihood at random. The higher the temperature the more randomness is introduced. A correct value requires tuning with human feedback, and it'll vary depending on the base quality of the model - large models that are trained on huge volumes of text and are deep enough to pick up on the \"what type of word should be here\" patterns will be able to generate better text with lower temperatures. Our model here is small and kind of sucks, so the temperature needs to be higher to get anything remotely usable. The implementation here is stolen shamelessly from the internet, the details don't really matter all that much, we just need to vary our predictions away from always simply picking the most likely word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dict = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "def sampleWord(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds.flatten(), 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, next_words, model, length_max, tok, inverse, temperature=1.0):\n",
    "    out = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tok.texts_to_sequences([out])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=length_max-1, padding='pre')\n",
    "        #print(token_list)\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        #print(tok.sequences_to_texts(predicted))\n",
    "        word = inverse[sampleWord(predicted, temperature=temperature)]\n",
    "        out += \" \"+word\n",
    "        #print(out)\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake Text Time!\n",
    "\n",
    "<b>Note:</b> we haven't built in any error handling, so it is possible to get random errors with unknown seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with not the days the you and strong of can fuck im do will there to me on know value it way almost not its a guys comments x200b b 3 \n",
      "\n",
      "We are going to the  them anything they i as new options money the 1 change and width not and already is some auto retards weeks have life for my then chart and apes and \n",
      "\n",
      "Where are all of the apes it to its in both up a on but now over why low potential auto money really pretty if done at total know his with and shit a market \n",
      "\n",
      "I am scared have of stock t it and take as a holding was 0 great a people t well lot help a moon be s gme no stocks keep been time to \n",
      "\n",
      "School is out for bad to that in format end r what stock big time by covid i 3 that the a here after in time of can't day shorts be end and s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"We are going to the \", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "#print (generate_text(\"Where are all of the\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "#print (generate_text(\"I am scared\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"School is out for\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few long ones... as with a time series, we shouldn't be surprised if the text becomes worse the farther we go from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with go their like the the months until than 2021 100 term on i in the the or the today may put platform in with however s and was why are i 15 https a still hold no at hold it so term advice if a them please 2 my today investor hedge great huge all though t t the webp s 4 moon may right back still could profit and back strong the the lose put trading 8 there png good that's 13 every 5 i'm go high in is here public t after selling on 13 am the use \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_5990/2819587574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Last night I went on a date with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My toe is purple and I am\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Toronto Raptors won and Rob Ford's body rose from the dead\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minverse_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/px/vhm_920n7zx2wvqq_ht0q5tm0000gp/T/ipykernel_5990/342767763.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(seed_text, next_words, model, length_max, tok, inverse, temperature)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(tok.sequences_to_texts(predicted))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampleWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"When is work happening\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")\n",
    "print (generate_text(\"The Toronto Raptors won and Rob Ford's body rose from the dead\", next_words=100, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, this isn't really that much worse than actual wallstreetbets posts. Also, in one run while making this my model generated a rocket ship emoji, because we are going to the moon. Emojis are just weird character sequences, so that makes sense, but I had never really considered that as a possibility before. \n",
    "\n",
    "And if we make the temperature low, we likely get worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last night I went on a date with gme the moon stock stock gme the moon stock stock stock the moon stock stock gme gme gme of the of the of the of the of the of the \n",
      "\n",
      "We are going to the  moon moon gme stock stock stock stock the stock stock the moon gme gme the moon stock stock stock of the of the of the of the of the of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Last night I went on a date with\", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict, temperature=.1), \"\\n\")\n",
    "print (generate_text(\"We are going to the \", next_words=30, model=model, length_max=OUTPUT_LENGTH, tok=tokenizer, inverse=inverse_dict, temperature=.1), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homemade Text Generation Results\n",
    "\n",
    "The LSTM model works well for text generation, if we had more data, more time, and spent a bit more effort cleaning up the edge cases in our code here we could likely get something pretty legible. \n",
    "\n",
    "The biggest difference between this and the impressive larger models is that they are higher capacity, trained with more data, and sometimes tuned with human feedback. These things combine to make the model more likely to pick up on the patterns of language and generate text that is more likely to be grammatically correct and follow the expected flow of language. Our model is really struggling to just find a reasonable word to use next, we haven't allowed it to learn enough to really construct logical sentences. \n",
    "\n",
    "For our next trick, we can borrow the power of gpt-2..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
