{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "We can try to build a neural network from scratch to see the details of how it works. We won't be using these homemade networks for real purposes, but it is good to get a bit of a look at the mechanics to help make things make sense. \n",
    "\n",
    "We will start by trying to create to solve a problem that we are familiar with - logistic regression. \n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "Neural networks are a type of machine learning algorithm that is inspired by the way that the human brain works. The human brain is made up of neurons that are connected to each other, and a neural network aims to replicate that. How much this actually models a human brain is up in the air (I don't think it actually works anything like a real human), but it's ability to make predictive models is excellent. We have inputs, output(s), and some number of layers in between. This sounds complex, and it can be, but we can break it down into a few simple steps.\n",
    "\n",
    "![Neural Network](images/nn_structure.png \"Neural Network\")\n",
    "\n",
    "The strucutre of a neural network is based on layers:\n",
    "<ul>\n",
    "<li> Input layer - the data that we are feeding into the network. Each of the neurons above gets one variable from the data set.\n",
    "<li> Output layer - the prediction that the network makes. This generates an actual prediction, just like any of our other models. \n",
    "<li> Weights - just like the slopes in regressions, we have weights that are multiplied by the inputs. Also just like regression, these weights are what the model learns to make its predictions.\n",
    "<li> Middle (hidden) layers - these are the layers that are in between the input and output layers. These layers are where the magic happens and are what allows a neural network to learn about our data so well. \n",
    "</ul>\n",
    "\n",
    "The basic process of training a neural network is also similar to regression:\n",
    "<ul>\n",
    "<li> The data comes into the input layer, each is multipled by a weight, and then the results are comined into a prediction. This is called forward propagation.\n",
    "<li> The model, with the current weights makes a prediction. \n",
    "<li> The error of the prediction is calculated.\n",
    "<li> The errror is broken down and attributed back to each of the weights, in a process called backpropagation. This is just like gradient descent finding adjustments for the weights in a regression, but slighty more complex since we normally have several layers. \n",
    "<li> The weights are adjusted based on the error.\n",
    "<li> A new model with the new weights is made, and the process repeats until the model is good enough or we hit a limit. \n",
    "</ul>\n",
    "\n",
    "![SimpleNN](images/simple_nn.png \"SimpleNN\" )\n",
    "\n",
    "This is basically what we looked at in a logistic regression! That is true for a one layer NN, with an activation (more on this later) of a sigmod. We calcualte the weights, make a prediction, use gradient descent to adjust, and filter the output through the sigmoid calcualtion to get a yes/no prediction. The difference here is that we have multiple layers, each with its own set of weights. The more layers we add, the more ability the model has to learn relationships in the data. In a sense (this is a thought process, not a literal description) each layer is kind of similar to its own little logistic regression, and we layer them together like a boosting model to make improvements layer by layer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers - Base Layer and Support Functions\n",
    "\n",
    "First we need a few helper functions - separating these out will make our code much easier to read. Below we have:\n",
    "<ul>\n",
    "<li> Base Layer Class: we are going to have two types of layers - the \"normal\" fully connected one, and an activation one to apply the activation function. In the future the activation is wrapped into the normal layer, but this will help us see the parts more clearly. \n",
    "<li> Activation Functions and Derivitives: we have two activation functions, and the derivitive of each. We'll talk about different activation functions in more detail later on. \n",
    "<li> Loss Function and Derivitive: we have the loss function and its derivitive. We'll keep it simple and use MSE, in a real classification this would probably be log-loss. We'll also talk more about different loss functions later on. \n",
    "<li> Convert to Bool: this just translates probability predictions [0,1] into a binary classification. We just need it to calculate predicted accuracy of our test data predictions. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  fz = sigmoid(z)\n",
    "  return fz * (1 - fz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a decimal between 0 and 1 to either 0 or 1, based on if it is\n",
    "# above or below the cutoff. \n",
    "def conv_to_bool(float_list, cutoff=.5):\n",
    "    new_list = []\n",
    "    for i in float_list:\n",
    "        if i < cutoff:\n",
    "            new_list.append(0)\n",
    "        else:\n",
    "            new_list.append(1)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usable Layers\n",
    "\n",
    "We can create the layers that we'll actually use now. Normally the FClayer and the activation layer we are creating would be combined, but this makes it easier to explore.\n",
    "\n",
    "#### Fully Connected Layer (Dense)\n",
    "\n",
    "The main component of the neural network is the fully connected, or dense, layer. The fully connected (dense) part just means that each neuron is connected to each neuron in the next layer. The three functions each have simple jobs:\n",
    "\n",
    "<ul>\n",
    "<li> Initialization: set up the matrix of weights and a vector of bias values. Here they are initialized to a random bumber, in practice these are initialized using some smart method, configurable with a parameter. Each neuron connects to each other neuron, so there is a wight for each of those connections - a matrix of input size by output size. Each output has one bias value. \n",
    "<li> Forward Propagation: in forward propagation this layer generates predictions by multiplying weights * values and adding the bias. The calculation itself is a dot product - which multiplies each input by its corresponding weight in the matrix automatically. This could be done by lots of loops, but this is more compact and efficient. Recall that each connection has a weight, so we end up with this large matrix. \n",
    "<li> Backward Propagation: this layer calculates the impact of the error stemming from each input X. This is done by calculating the gradient of the loss with respect to each input - thus allowing us to attribute portions of the output error to the different inputs. The calculation is done using the derivitive of the activation function and some math called the chain rule. Once we have this, we update all the weights and the bias so that we shrink our loss. \n",
    "</ul>\n",
    "\n",
    "![Weights](images/weights.png \"Weights\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        # update parameters\n",
    "        # dBias = output_error\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "\n",
    "        return input_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "One of the most important parts of a neural network is the activation function. The activation function is what allows the network to learn non-linear relationships. Without an activation function, the network would be no more powerful than a linear regression. The activation function is applied to the output of each neuron, and it is what determines whether the neuron is \"on\" or \"off\". Prior to the output of each neuron being sent on to the next layer, the activation function is applied. If you're paying close attention, the output going into an activation function is just a linear regression - the sum of the features times their weight. The activation function changes this, and allows the neural network to be \"more\" than a complicated linear regression. In real applications, we usually have a few layers in our neural network, and the combination of multiple layers and activation functions allows us to learn very complex relationships. The term \"deep learning\" refers to networks that have lots of layers. \n",
    "\n",
    "This step is just like the sigmod step in a logistic regression - we have the raw value that comes into the function, the output is not that raw value, it is whatever that raw value translates to after the activation function is applied. If this wasn't applied, a multi-layer neural network could be decomponsed into a simple linear equation (with a bunch of linear algebra work).\n",
    "\n",
    "![NN Steps](images/nn_steps.jpeg \"NN Steps\" )\n",
    "\n",
    "There are many different activation functions, including the one that we are used to - the sigmoid. These activation functions are things that we can experiment with and tune like our hyperparameters. Some tend to be better suited for different types of problems, such as ReLU being common for image work and tanh being common for text processing, but there is no definitive answer. There are also considerations for computational efficiency, and some activation functions are more computationally expensive than others. For the most part we will start with the most common one for the type of problem we are doing, and then experiment with others if we feel the need. We'll look at these more later. \n",
    "\n",
    "![Activation Functions](images/activation-functions.png \"Activation Functions\")\n",
    "\n",
    "#### Activation Layer\n",
    "\n",
    "The activation layer is more simple, it just applies the activation function. Normally this is built into the other layer, but this is a little more simple to build by hand. \n",
    "\n",
    "<ul>\n",
    "<li> Initialization: set the activation function and derivitive to use going forward. \n",
    "<li> Forward Propagation: in forward propagation the activation layer just takes the input that it gets and applies the activation function to it.  \n",
    "<li> Backward Propagation: in backward propagation the activation layer translates the error of the predictions back up by multiplying the error by the derivitive of the activation function. This has the effect of translating the error we got with respect to the output of the activation function into error with respect to the input of the activation function. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Network\n",
    "\n",
    "We can now build our layers into an actual model. Our code here is a little long, but most of it is pretty simple - the majority of the extra stuff is to make the model flexible. \n",
    "\n",
    "<ul>\n",
    "<li> Add - call this to add layers to the model. \n",
    "<li> Use - provide a loss function, and the derivitive of a loss function for the model to use. \n",
    "<li> Predict - generate prediction. This is just one execution of a forward propagation for the new inputs. \n",
    "<li> Fit - train the model. \n",
    "    <ul>\n",
    "    <li> Each iteration is called an epoch. We loop through the process until we've hit the desired number of epochs. \n",
    "    <li> Run each sample through a forward propagation. \n",
    "    <li> Get the prediction, and calculate the error. \n",
    "    <li> Pass the error to the back propagation. Repeat\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Another way to phrase the fitting and optimizing of the model is to think of the errors on each epoch. First we generate a prediction (FP), and find the error of that prediction by simply comparing it to the true value - this is the error with respect to y, the output. \n",
    "\n",
    "### Back Propagation and Gradient Descent\n",
    "\n",
    "Next we do the back propagation, this takes that error with respect to y that we figured out, and \"breaks\" that error down into error with respect to each term from the previous layer. Recall that one neuron creates a prediction (during FP) that is equal to w1*x1 + w2*x2 +... b; this does a sort of reverse execution of that - starting with the overall error and mapping it to each term. The error is effectively split into parts and each of the m*x terms and the b term is labeled as being accountable for some of it. Since the x values are the inputs, they can't change, so we modify the weights and bias to lessen the error - this is the gradient descent part. This gets repeated back through each layer until the first layer, then another FP begins with the new updated weights. The learning rate controls the size of the adjustments at each round.\n",
    "\n",
    "![Back Propagation](images/backprop.webp \"Back Propagation\" )\n",
    "\n",
    "This back propagation part is the key to the high accuracy ceiling that neural networks have when the data is large - we can tune each of many neurons specifically to their contribution of error. If we have enough data, we can get very accurate predictions. This is also the gradient descent process that we are used to from logistic regression. The difference is that we walk that process back through the entire network, through all the layers, and update the weights and bias all the way through. This can become computationally expensive, as we are doing partial derivitives on a bunch of (potentially large) matricies over and over, but the concept is simple - we are adjusting the weights to improve the error, or performing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        errors = []\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                #print(self.loss)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            if i % 100 == 0:\n",
    "                err /= samples\n",
    "                errors.append(err)\n",
    "                print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Network\n",
    "\n",
    "We have a neural network now - we need to use it! Since we were smart and made the framework able to handle input data of an arbitrary size, we can use our model for pretty much any application!\n",
    "\n",
    "<b>Note:</b> for all of these examples, the syntax that we are using is similar to the \"real world\", but not exactly the same. Our handmade neural network doesn't quite have the same functionality of the libraries we can use, but it is close enough to get the idea. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example - XOR\n",
    "\n",
    "We can test and see what our network can do with a very simple trial. XOR is a logical operation - exclusive or. It is 1 only if exactly one input is 1, or else it is false. We have that data as the training X and y, we can see if our network can learn this very simple relationship before we test it on some real data. \n",
    "\n",
    "![XOR](images/xor.webp \"XOR\" )\n",
    "\n",
    "One thing to note here is that the XOR problem can't be solved with a single layer. We need at least two layers to be able to learn the relationship. Let's see if our model can figure it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.397781\n",
      "epoch 101/1000   error=0.282690\n",
      "epoch 201/1000   error=0.281922\n",
      "epoch 301/1000   error=0.269284\n",
      "epoch 401/1000   error=0.205885\n",
      "epoch 501/1000   error=0.004860\n",
      "epoch 601/1000   error=0.001466\n",
      "epoch 701/1000   error=0.000824\n",
      "epoch 801/1000   error=0.000564\n",
      "epoch 901/1000   error=0.000425\n",
      "[array([[0.00146386]]), array([[0.97417827]]), array([[0.97381539]]), array([[-0.00016839]])]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 2))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(2, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a Logistic Regression\n",
    "\n",
    "We can create the network that we defined above. You may also recognize this 1 layer model as a logistic regression:\n",
    "<ul>\n",
    "<li> The weights are the coefficients of the linear regression.\n",
    "<li> The bias is the intercept of the linear regression.\n",
    "<li> The activation function is the sigmoid function.\n",
    "<li> The features are the features. \n",
    "</ul>\n",
    "\n",
    "At our most simple, that is all a neural network is. The ones that are more capable and flexible are just more complex versions of this, normally with more layers. Each layer adds one of these logistic regression like steps, only they are being performed on the output of the previous layer. Multiple layers allow us to learn more complex relationships, as each layer can learn a different relationship from the transformed data. \n",
    "\n",
    "<b>Note:</b> We have to supply the \"sigmoid_prime\" and similar functions here. That's something that the library would normally do for us, but since we're building it from scratch, we have to do it ourselves. The prime version is what the nerual network uses to calculate the error in back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Data\n",
    "\n",
    "One weird thing that we will do to the data here is that we will reshape the Xs into a 3 dimensional array. This is due to the structure of the model - it is expecting data to be in that shape. When we start using Keras (the package we use for neural network models) this can generally be handled automatically, but since we've made this from scratch, we need to do it here. We do need to be a little more comfortable with the shape of the data, and potentially shifting it around, so this is a good exercise.\n",
    "\n",
    "What we end up with for the data is:\n",
    "<ul>\n",
    "<li> 8 numerical features. We will have 8 neurons / Xs in the input of our network. \n",
    "<li> 1 binary categorical output. We will have one neuron for the output of the network. \n",
    "<li> Activation function applied to our output, to ensure we get categorization. \n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> we always want to scale data for our neural networks. \n",
    "\n",
    "![SimpleNN](images/simple_nn.png \"SimpleNN\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (576, 1, 8)\n",
      "train_set_y shape: (576, 1)\n",
      "test_set_x shape: (192, 1, 8)\n",
      "test_set_y shape: (192, 1)\n",
      "[[0.13333333 0.46231156 0.42622951 0.         0.         0.4485842\n",
      "  0.02798756 0.01666667]]\n",
      "[0]\n",
      "[[0.6        0.59798995 0.6557377  0.35353535 0.         0.43219076\n",
      "  0.0821857  0.13333333]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"Outcome\"]).reshape(-1,1)\n",
    "X = df.drop(columns={\"Outcome\"})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train).reshape(-1,1,8)\n",
    "X_test = scaler.transform(X_test).reshape(-1,1,8)\n",
    "\n",
    "#Look at some details\n",
    "print (\"train_set_x shape: \" + str(X_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))\n",
    "print(X_test[0])\n",
    "print(y_test[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a one layer network, with a simoid activation function. This is a logistic regression, just like my beautiful diagram above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.249393\n",
      "epoch 101/1000   error=0.235536\n",
      "epoch 201/1000   error=0.232637\n",
      "epoch 301/1000   error=0.230459\n",
      "epoch 401/1000   error=0.228414\n",
      "epoch 501/1000   error=0.226460\n",
      "epoch 601/1000   error=0.224590\n",
      "epoch 701/1000   error=0.222802\n",
      "epoch 801/1000   error=0.221091\n",
      "epoch 901/1000   error=0.219453\n",
      "0.703125\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(8, 1))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(X_train, y_train, epochs=1000, learning_rate=0.0001)\n",
    "\n",
    "# Evaluate on test set\n",
    "out = net.predict(X_test)\n",
    "pred_labels = conv_to_bool(out)\n",
    "print(accuracy_score(y_test, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "One thing that we can see is that our model doesn't really improve all that much if we let training keep running. This should make some intuitive sense - we are only using one layer, and thus only doing a \"regular\" gradient descent. Once the model has found good weights, there's not really anything else to adjust, it is kind of just bouncing around near the minimum on the gradient descent curve. The amount that can be learned is pretty limited, and we don't need any more iterations than a regular logistic regression would. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a Larger Network\n",
    "\n",
    "What if we try adding some layers? Outside of the input and output the rest of the stuff inside our network is configurable. We can add whatever we want in terms of layer, and each layer can be any size, as long as the shape matches the previous and next layer. We can also try that other activation function if we want. What's the \"right\" size? That question doesn't have a direct answer, we'll look at some things we can use to estimate it ~ 2 workbooks from now. We can play around a bit with it now and see what we get. As you can probably gues, larger models tend to be more flexible, more likely to overfit, while smaller models are less flexible, but less capable of learning the complex relationships in data. \n",
    "\n",
    "If our data isn't linearly separable, we should see our model more able to fit the data as we start adding layers. Try a few, the only thing that we need to be careful of is that the shapes of inputs and outputs match up. Other than that, we can add as many layers as we want, with different activation functions. \n",
    "\n",
    "![MultiLayerNN](images/multi_layer_nn.jpeg \"MultiLayerNN\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.241192\n",
      "epoch 101/1000   error=0.160898\n",
      "epoch 201/1000   error=0.156465\n",
      "epoch 301/1000   error=0.154747\n",
      "epoch 401/1000   error=0.153363\n",
      "epoch 501/1000   error=0.151914\n",
      "epoch 601/1000   error=0.150291\n",
      "epoch 701/1000   error=0.148638\n",
      "epoch 801/1000   error=0.147218\n",
      "epoch 901/1000   error=0.146054\n",
      "0.78125\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(8, 50))            \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 50))               \n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 50))               \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 25))               \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(25, 1))  \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(X_test)\n",
    "pred_labels = conv_to_bool(out)\n",
    "\n",
    "print(accuracy_score(y_test, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "These results will vary depending on exactly what you added for layers, size, and activations, but we will generally see a couple of things as we add some more layers. First, we will see that the model is able to learn the data better. This is because we are adding more layers, and thus more logistic regression steps. Each layer is learning a different relationship from the data, and we are able to learn more complex relationships. Second, we should see more (it might not be a lot, it depends on the setup) learning as we progress through the rounds of training. This is because the capacity of our model to learn is far larger, we have more weights that can be combined in different combinations to deliver the best fit. The model will take longer to learn which combinations are best as it is doing that gradeint descent process. This is basically why large neural networks can take days or weeks to train, there is just a lot of math as the data sizes and the network sizes get large. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Example - MNIST Images\n",
    "\n",
    "Now that we are at least a little comfortable with the usage of neural networks, we can start to apply it to more useful applications, such as our old friends, the MNIST digits. Here we will make a larger model and we can also try a differnet activation function. One other difference to note is the \"to_categorical\" applied to the target y values. This works with the final layer of 10 output neurons - we get a softmax type (note: not actually softmax here) of output where there is a probability for being in each class. Technically, this would actually be a one-vs-rest for each potential output. The output layer is often felxible like this in a neural network, depending on what we are doing. Here each output neuron is the probability of being in that class, we have 10, one for each digit. Our raw probabilities, which we'll print a few of below, are the probabilites of being in each class; note that those probabilities don't sum to 1, since each prediction is independent. We'd then need to add a function that is similar to \"argmax\", to choose the highest probability and assign a label. These details are cleaner and easier to handle with Keras, so don't get hung up on that part. With the library functions it is a bit more intuitive to deal with multiple output classes. The structure of our model will be similar to this, the middle layers will depend on whatever we add:\n",
    "\n",
    "![MINST](images/mnist_nn.jpeg \"MINST\" )\n",
    "\n",
    "Since these images, and the relationships in the data, are much more complex than the simple examples above, we should expect to see higher accuracy with more training become more visible here. More layers will also likely help, so try a few variations. In general, we can probably get really good accuracy here, it could be even better if we used more data to train, but it'll take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n",
      "epoch 1/700   error=0.282446\n",
      "epoch 101/700   error=0.062150\n",
      "epoch 201/700   error=0.044879\n",
      "epoch 301/700   error=0.035292\n",
      "epoch 401/700   error=0.029622\n",
      "epoch 501/700   error=0.025319\n",
      "epoch 601/700   error=0.021368\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[6.83534351e-03, 7.71500738e-03, 1.16532539e-01, 2.04954793e-02,\n",
      "        3.10298112e-03, 6.83885505e-03, 6.52023056e-04, 9.18520905e-01,\n",
      "        5.22747803e-02, 1.39342149e-01]]), array([[2.55987191e-01, 3.07169560e-02, 7.30540378e-01, 1.96762587e-01,\n",
      "        1.04247221e-01, 1.17377273e-02, 8.55676286e-02, 7.26970572e-04,\n",
      "        7.23164115e-02, 9.40550232e-03]]), array([[1.36811592e-03, 9.43221070e-01, 2.94288994e-02, 1.66516692e-02,\n",
      "        8.80341966e-04, 2.98053732e-02, 7.21774835e-03, 6.37819547e-02,\n",
      "        1.34114944e-01, 1.12712648e-02]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data \n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 100))       \n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))             # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "#net.add(FCLayer(50, 1)) \n",
    "\n",
    "# train on 1000 samples\n",
    "# we didn't use batches, which we'll look at more next time, so we can't use too much data or it will be slow. \n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=700, learning_rate=0.001)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
