{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, AveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Images\n",
    "\n",
    "Images are perhaps the place where neural networks have had the most dramatic impact. The best neural networks can very accurately perform image recognition, to the point that they can identify disease in medical imaging better than doctors, or track people's faces (or even their gait) in real time video. \n",
    "\n",
    "Image processing and applications such as image recognition is one of the most visible and exciting applications of neural networks. In contrast to our previous models that can perform image classification, the structure of neural networks allows them to be tailored to be very good at dealing with image data. This also tends to be true for many \"analog\" data types, such as audio, video, and text, which is why many of the most impressive current applications of AI, like live translation and facial recognition, use neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data\n",
    "\n",
    "Until now we've used simple images that only have one color, we can expand this a bit now to handle more \"normal\" images. We will use one of the sample ones from Keras called cifar10. \n",
    "\n",
    "### Color Images\n",
    "\n",
    "Color images have a greater depth - one layer for each color. Usually this is one for red, blue, and green, or RGB. There are other color encodings, but the idea is pretty similar. Of note for us, these images are now 3 dimensional - in terms of their representation as an array. \n",
    "\n",
    "![RGB](images/rgb.png \"RGB\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization in Images\n",
    "\n",
    "One common difference in dealing with image data is the normalization process. Images, at least with the RGB encoding we are looking at, have their values encoded on a standardized scale, normally 0 to 255. Because of this, rescaling the data to 0 and 1 can be as simple as dividing by 255. Since every feature (pixel) is on the same scale, this works exactly the same as using a MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Note: the class names are taken from the documentation\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[7].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Shapes and Color\n",
    "\n",
    "One of our images, number 7 in the dataset, has the dimensions 32x32x3. This means that it is 32 pixels wide, 32 pixels tall, and has 3 layers of color - red, green, and blue. Each image is effectively the 3 color layers stacked on top of each other to generate a full color image.\n",
    "\n",
    "There are other ways to encode image data, or different color spaces. RGB is probably the most common, but other ones exist and may perform better in certain scenarios. The concept is the same, but the color data is broken up into different components other than red, green, and blue. For example, a lot of image processing is done in YCbCr, which is a color space that tends to be more efficient for image processing. We won't go into the details of different color encodings, if we need to deal with images that are defined in that color space, we can use a library to convert them to RGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing\n",
    "\n",
    "We can use imshow to display one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[7])\n",
    "plt.xlabel(class_names[y_train[7][0]]) #The CIFAR labels happen to be arrays, so we need the extra index\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We can make a simple helper to display an image. We can also use our loss plotting function from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(data, labels, names, index):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(data[index])\n",
    "    plt.xlabel(names[labels[index][0]]) #The CIFAR labels happen to be arrays, so we need the extra index\n",
    "    plt.show()\n",
    "\n",
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "  plt.plot(history.history['accuracy'], label='accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the helper function to display an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(X_train, y_train, class_names, 192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shape and Model\n",
    "\n",
    "Our data starts as images that are 32 x 32 x 3 - 32 pixels by 32 pixels by color depth of 3 (RGB).\n",
    "\n",
    "#### Flatten\n",
    "\n",
    "One new addition we can utilize is the Flatten layer, which does exactly what is says.  The flatten layer does the same thing we did when reshaping digit images, it makes them into a flat array. We specify the shape of one example of our dataset as the input shape argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - Activation and Loss\n",
    "\n",
    "Since we are doing a classification with Keras now we need to make a few small changes to handle that. \n",
    "\n",
    "#### Activation\n",
    "\n",
    "The first change is the activation on the output layer. When doing regression we want raw predictions, so there's no activation. Here we want to classify so we need to add activation. Since we are classifying into multiple classes we can use softmax to do so. We also have to set the units to the number of classes that we are predicting, in this case 10. \n",
    "\n",
    "Recall from when we first looked at multiclass classifications, the result of softmax is that we get a breakdown of probabilities that each record belongs to each of the classes, totalling to 1. Each class is represented by an output neuron, and the largest one wins and gets the label. \n",
    "\n",
    "#### Loss\n",
    "\n",
    "We also want to use different loss metrics when doing classifications. Here we will use categorical cross entropy. \n",
    "\n",
    "We will need to use to_categorical here to make our current labels (e.g. [4]) into a one-hot categorical array (e.g. [0,0,0,0,1,0,0,0,0,0]). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Check\n",
    "\n",
    "I've added an example here of checking if we are in Colab. For me, I'm going to increase the default epochs and shrink the defauly batch size if so. The idea here is that I can run it on a limited basis locally when developing, but then run it on Colab when I want to do a full run. If you're old and senile like me, it helps to keep you from forgetting! We could do something similar for lots of stuff, we could take a subsample of the data for developing then use it all for training, or print verbose results when developing and nothing when training, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Base Parameters\n",
    "# Increase processing demands if on Colab\n",
    "BASE_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "BASE_PATIENCE = 5\n",
    "MIN_DELTA = .02\n",
    "MONITOR = \"val_accuracy\"\n",
    "MODE = \"max\"\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  BASE_EPOCHS = 75\n",
    "  BATCH_SIZE = 32\n",
    "  BASE_PATIENCE = 10\n",
    "\n",
    "acc = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "pre = keras.metrics.Precision(name=\"precision\")\n",
    "rec = keras.metrics.Recall(name=\"recall\")\n",
    "metric_list = [acc, pre, rec]\n",
    "\n",
    "callback = EarlyStopping(monitor=MONITOR, patience=BASE_PATIENCE, restore_best_weights=True, min_delta=MIN_DELTA, mode=MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy: About 50% in Most Experiments\n",
    "\n",
    "Our simple model likely didn't do all that well. We can likely do better in making predictions! If we were simpletons we'd probably look for ways to cut back that obvious overfitting. We are super sophisticated though, so we'll get all crazy and try a totally different approach..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs - Convolutional Neural Networks\n",
    "\n",
    "To deal with images a little bit better we can use a different kind of neural network design - a CNN, or convolutional neural network. In short, a CNN is able to look at an image \"as it is\" caputuring spatial relationships that processing an image as a flattened array do not. When using a CNN we can first process the image in its original dimensions in the initial layers of the network, then flatten it down to go through a more familiar set of layers for the final prediction. \n",
    "\n",
    "A CNN looks at an image bit by bit, looking at a small square, then sliding over a few pixels, looking at another square, and so on. This has the effect of being able to extract features from areas of an image - as an example, think of an image of a bike, a CNN would be able to identify the distinct shape of a seat or handle bars as the image passes through the layers. This improves the ability of the model to identify patterns of data that define shapes in images, no matter where in the image that shape is.\n",
    "\n",
    "### CNN Structure\n",
    "\n",
    "A CNN has some new types of layers:\n",
    "<ul>\n",
    "<li> Convolutional layer - the convolutional layer looks at a small frame of the image at a time.\n",
    "<li> Pooling layer - the pooling layer reduced the dimensionality of the data. \n",
    "<li> Regular neural network - after the convolutional parts to their work, we can flatten the data and pass it to a regular fully connected network at the final layers. \n",
    "</ul>\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "##### Simple Image\n",
    "\n",
    "![CNN](images/cnn.gif \"CNN\")\n",
    "\n",
    "##### More Complex Image\n",
    "\n",
    "The convolutional layer is easiest to think of as a microscope that scrolls over an image looking at one small square of it at a time. This image is the most illustrative annimation I found of showing the convolution process; note that this one shows a stride of 2, which is why the filter seems to jump and is why the size of the output is so much smaller. We will normally rely on the pooling layer to reduce size, and use a stride of 1 with same padding to keep our outputs the same size as the inputs.  \n",
    "\n",
    "![Kernel](images/cnn_kernel.gif \"Kernel\" )\n",
    "\n",
    "\n",
    "This convolution operation translates the input \"feature map\" into an \"output map\". After the transformation the result is that each layer captures some features in the image - edges, orientation, etc... and map those down to lower layers. \n",
    "\n",
    "### Filters (a.k.a kernel) in the Convolutional Layer\n",
    "\n",
    "One of the arguemnts we provide when making a convolutional layer is the number of filters. We have a whole bunch of filters, each learns to find some different characteristic from the image data as the training progreses. Another argument is the size of the filter. In general, filter sizes are small squares such as 3 by 3 or 5 by 5, but as images become massively higher in resolution and computers become faster, larger filters are becoming more widespread and the typical size will likely continue to increase to some degree. \n",
    "\n",
    "The actual values in the kernel are learned during training in a CNN, like most other things, they are determined throughout the backpropagation steps in the gradient descent process. In the context that we are typically using, image classification, the weights in the filters are learned to minimize the loss. In other words, the filters are learning to find the features that are most important to the model in order to make the best predictions. Or if we phrase it in a less nerdy way, the filters are trained to identify the parts of the image that allow it to be classified. This part is somewhat like magic, the model learns how to differentiate the different images, and it learns how to make filters that extract the key things needed to do so. \n",
    "\n",
    "### Padding\n",
    "\n",
    "Padding is a setting that determines if the dimensionality of the data is reduced in the convolutional layer or not. We have two choices:\n",
    "\n",
    "<ul>\n",
    "<li> Valid padding - dimensions are reduced. \n",
    "<li> Same padding - dimensions are maintained. \n",
    "</ul>\n",
    "\n",
    "This is probably most easily illustrated by looking at the image above. That image is showing same padding - those 0s around the border are inserted to ensure that the kernel can start at the edge and still capture the entire picture. If this were valid padding the kernel would start at the real edge, and those edge values would never make it to the middle of the image. The resulting values will then be of a smaller dimension than the original. Using same padding allows the model to better capture the information around the border, avoiding what is known as the border effect. \n",
    "\n",
    "In general we should expect fractionally better performance with padding enabled, at the cost of some processing time and memory. This effect isn't usually massive, as with most pictures the valuable stuff is in the middle - so the impact depends on the dataset. If you think of the minst images or something like a passport photo, the edges are usually pretty unimportant. If you think of a picture caputered by a self driving car, the edges may be the most important, to see things like curbs and kids jumping in front of the car.\n",
    "\n",
    "#### Strides\n",
    "\n",
    "The stride value is how many pixels the kernel window shifts each time it looks at a window. Strides of 1 move 1 pixel at a time, larger strides \"skip\" some pixels. Our stride will usually just be 1. \n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "The pooling layer reduces the dimensionality of the data down. In image terms that you may have heard elsewhere we are downsampling - taking something that is at some higher resolution and transforming it to a lower resolution. \n",
    "\n",
    "This pooling step reduces the size of the data, making for more efficient calculations. It also helps generalize the ability of the model to recognize certain features. We can capture this generalization by thinking of an example - higher vs lower definition images. If we have a high definition image of something it is extremely clear, and if we have similar objects it is easy to tell them apart. For example if there are multiple cars in an image, we can probably tell them apart pretty easily - the details show the differences. If the cars are in the the background of an image (if they are in the background we are only getting a low definition version of those cars) it is harder to tell them apart - the details are different, but the general \"characteristic\", the fact it is a car, is consistent. Our pooling has the same impact - the pooling changes the higher definition images to lower ones, and we are better able to identify those general characteristics - making it easier to spot things that are \"the same\" in other images. \n",
    "\n",
    "The default pooling window is 2x2, so 4 features are collapsed into one. \n",
    "\n",
    "#### Max and Average Pooling\n",
    "\n",
    "There are two common pooling strategies - max and average. Max pooling takes the maximum value in the pooling window as the output, average takes the average of the values in the pooling window.\n",
    "<ul>\n",
    "<li> Max is more common, it tends to do a better job at finding contrast - differences between light and dark, which is helpful in doing things like separating foreground and background or doing edge detection (think of navigation).\n",
    "<li>Average tends to capture a more broad set of information on the entire image, with less focus on areas of distinct difference. \n",
    "</ul>\n",
    "\n",
    "![Max_Average_Pooling](images/max_avg_pool.png \"Max_Average_Pooling\" )\n",
    "\n",
    "### Normal Neural Network Layers\n",
    "\n",
    "Once the above work is done, potentially with several layers of layers, the final layers in the network are a normal neural network. The CNN parts act to extract features from the image, the final layers take those features and produce a prediction, just as we are used to. So, loosely, the CNN classification model does some image processing in the convolutional layers to extract features and patterns from the image, we then feed the information extracted from the images into a 'normal' neural network model that makes a prediction from some features. \n",
    "\n",
    "### Overal Structure\n",
    "\n",
    "After the entire model is constructed we end up with something like this. The image is translated into a series of representations - one per layer, through the convolutional process. The pooling then lowers the dimensions of those representation, and the process (potentially repeats). At the end of all the convolutional steps we feed our final represenations into the dense stages - these features are in the \"shape\" of an image - with the details being totally different - each layer is a filter (rather than a color) and the dimensions of the \"image\" are determined by the amount of pooling and padding. This is all flattened and the dense part goes on as we are used to in making predictions. \n",
    "\n",
    "![CNN Structure](images/cnn_structure.jpg \"CNN Structure\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple CNN\n",
    "\n",
    "We can build a simple CNN to make some predictions on our images. In our network we'll have:\n",
    "<ul>\n",
    "<li> Convolutional portion:\n",
    "    <ul>\n",
    "    <li> Convolutional layer - 32 filters, relu activation, same padding. \n",
    "    <li> Pooling layer - (2, 2) size. Results in data 1/4 of original size.\n",
    "    </ul>\n",
    "<li> Dense portion:\n",
    "    <ul>\n",
    "    <li> Flattening of the convolutional results. \n",
    "    <li> Dense layer. \n",
    "    <li> Output layer, 10 classes, softmax prediction. \n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Set of Metrics\n",
    "\n",
    "We will define some metrics to watch, then train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Request - Combining Metrics and Labels\n",
    "\n",
    "There might be a more efficient way to do this, metric._name seems like it should return the name of the metric without bothering with the inspection part. It didn't work for me, so I did it this way. I may have had a mistake that I didn't troubelshoot - reader challenge to make it better!\n",
    "\n",
    "<b>Note:</b> the request part was from last year, this is just a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Results\n",
    "import inspect\n",
    "metric_list_fin =[]\n",
    "for met in metric_list:\n",
    "    atts = inspect.getmembers(met)\n",
    "    tmp_name = [a for a in atts if(a[0] == \"_name\")]\n",
    "    metric_list_fin.append(tmp_name[0][1])\n",
    "\n",
    "metric_list_fin.insert(0,\"loss\")\n",
    "metric_df = pd.DataFrame(\n",
    "    {'Metrics': metric_list_fin,\n",
    "     'Train': train_eval,\n",
    "     'Test': test_eval\n",
    "    })\n",
    "metric_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interim Results\n",
    "\n",
    "We haven't really done all that much to tune our new model, but in the tests that I ran we were usually seeing somewhere around a 70% validation accuracy at this point - over 20% better than what we got with a non-convolutional model. This is the power of the convolutional layers - they are able to extract features from the images that are useful for making predictions in a way that a \"normal\" model can't. They also do this without requiring any real insight from us - we don't have to tell the model what features to look for, it figures it out on its own, so we don't have to be incredibly knowledgeable about images we are dealing with to make a good model.\n",
    "\n",
    "<b>Note:</b> our results are likely to be a little worse than expected below due to early stopping, if we made the delta a little more permissive, we'd likely see some of the models get a little better slowly. That's what happened in inital trials, but it can take a long time to run, so we compromise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Trials\n",
    "\n",
    "We'll try a few different CNN models, and see how they perform. The steps here are broadly pointing towards a more accurate model, but not super specifically. We are trying a few things here to see what we get. By the end, we'll likely have an overall more accurate model, we hope...\n",
    "\n",
    "#### Multiple Convolutional Layers\n",
    "\n",
    "We can try several convolutional layers, and see if that improves our results. We'll add a couple more convolutional layers, and corresponding pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs - Convolutional Layers, Padding, Kernel Size\n",
    "\n",
    "We can change the main options for our CNN without much difficulty. These changes will have impacts on our model similar to changes that we can make to any sets of hyperparameters in any models. Some changes will make minor diffrences, other large ones, and some will have no impact at all. The impact depends on the dataset that we are dealing with, and the strucutre of the model we are using. We have some intuition that we can use to guide trials, such as setting the padding depending on if the edges of our image are valuable or not, but for the most part this is trial and error land. \n",
    "\n",
    "#### Padding\n",
    "\n",
    "The default padding is valid, we set ours to same up above. With padding set to same our models should do a better job of capturing information around the edges of the image. We can see what the results are when we revert it to valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Size\n",
    "\n",
    "We can also modify the kernel size. The kernel size is the size of the \"window\" - or how many pixels the filter looks at each time. 3x3 and 5x5 are probably the most common, for larger images sometimes something larger is used. The kernel size is pretty much always odd, so it is symetrical around the center pixel, this isn't a requirement, but it is a common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Size\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5,5), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (5,5), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (5,5), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers \n",
    "\n",
    "Just like with a regular neural network we can change the number of layers. In order to allow for many layers to exist despite the pooling, we will need to increase the number of filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpooled Convolutional Layers\n",
    "\n",
    "We can also stack convolutional layers without pooling in between. This will allow us to capture more information, but will also increase the size of our data. We can try to double up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "It is also relatively common to try batch normalization - or normalization layers applied between layers of a network. Batch normalization tends to have two main impacts - increase in model stability and acceleration of convergance. The reason for this is because each time data is transformed the inputs to the next layer can have their distribution shifted - something called internal covariate shift. Batch normalization adjusts this by renormalizing in the middle of each step. Note also that here we separate the dense layer and the activation - as we did with the from-scratch version. Batch normalization also can allow for faster learning rates in many cases - the improved convergance lets the algorithm go faster. \n",
    "\n",
    "Batch normalization is relatively new, put forth in around 2015, and the exact nature of how it improves things mathmatically is still debated (which surprised me). In particular, there is still debate on if it should be inserted between the regular layer and the activation as we have done here, or applied post activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 3), padding=\"same\"))\n",
    "model.add(BatchNormalization(epsilon=.1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=\"l2\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(X_train, y_train, epochs=BASE_EPOCHS, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[callback])\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Configuration Results\n",
    "\n",
    "As we can see, the convolutional neural networks give us models that seem to be much more capable of making predictions on our images. In most of my trials, the most accurate models got to roughly 80% accuracy on the validation data, which is not bad, considering a regular model is sub 50%. Down below, after a surprise, we'll combine a bunch of these things together and see what we can get. \n",
    "\n",
    "<b>Note:</b> in a few trials, the batch normalization example above tended to be most impacted by setting the early stopping. If we let it run, the accuracy has seemed to improve, but slowly and with a lot of variation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories of Images\n",
    "\n",
    "We are going to grab some data, but this time it is not getting loaded into some data structure in our application, the file is being saved to disk and uncompressed. The end result of the code below is the same as if you were to download a file and unzip it (or in this case, un-tarball it). \n",
    "\n",
    "Processing images in a way similar to this is common - we may have a large number of images, and we don't want to load them all into memory at once. We can also use this approach to load images from a database, or from a remote location. The idea of loading the data into datasets is basically the same, but the details can vary a fair bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import PIL \n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Download by Printing One Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "When dealing with things like images we commonly have actual images, not arrays or dataframes. Keras has a preprocessing function to take a folder of images and automatically create a dataset from it. A dataset is a built in datatype in tensorflow, it is kind of a specialized type of data structure that is meant to store larger volumes of generally non-tabular data, and is purpose made to be put through tensorflow networks. Here we will basically have the image files on disk be automatically loaded and split into two datasets - training and validation. When fitting the model we can use this dataset just as we would an array. \n",
    "\n",
    "This type of setup is fairly common when dealing with images. The particular function we used here - image_dataset_from_directory - does bulk data loading from the file structure on disk, handling all of the I/O details on its own. \n",
    "\n",
    "#### Dataset Components\n",
    "\n",
    "The dataset is basically the data itself, along with some extra information on how it is to be used. For example, the data and targets are both in the dataset, the validation split is preset, as is the batch size. \n",
    "\n",
    "<b>Note:</b> in larger scale applications the batch size may be constrained by the memory size of the GPU. We <i>don't</i> want to load more data that can fit into memory under any circumstance, as that means that the processor will need to wait for data to be loaded to and from main memory or disk, which is very slow. For us, this isn't a pressing concern, but it is something to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flowers\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Model with the Datasets\n",
    "\n",
    "For the most part, things don't change significantly when using the datasets. We get to drop the arguments that are embedded in the dataset itself, like how much data to hold for validation. \n",
    "\n",
    "#### Example of Different Outputs\n",
    "\n",
    "This model also has an example of an odd setup for its output. We probably don't want to do this in general, but it is worth seeing. Even though we are doing a classification, there is no activation on the output. As well, the loss function has a parameter \"from logits\" set to true. This is because the output is not a probability, but a raw value. This is basically offloading the softmax step to be done outside of the model's layers, because we can get output to the raw predicitons. For us, this likely won't be all that critical, it is something that you may see. One reason to do this is because in certain scenarios, the calculations may be more stable, easing convergance. Other than that edge case, we can treat it as interchangable, but probably less user friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(180, 180, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"))\n",
    "train_log = model.fit(train_ds, epochs=10, verbose=1, callbacks=[callback], validation_data=val_ds)\n",
    "train_eval = model.evaluate(train_ds)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "When using images we can employ data augmentation to increase the size of our dataset, allowing for better training and resulting in better models. More data is king when it comes to model quality, so this is very helpful. \n",
    "\n",
    "The reason data augmentation is common and easy with images, where it isn't as straightforward with structured data is due to the nature of an image and what we are generally trying to do with it. Image problems are generally things like recognition - identifying what is in an image. If we are looking to identify if there is a cat in an image we probably don't  care if the cat is on the left side, the right side, rotated in any direction, tilted, etc... cats move in stupid and random ways, because they're insane lunatics, all we care about is if the cat is somewhere in there. \n",
    "\n",
    "![Augmented Cat](images/cat_aug.png \"Augmented Cat\" )\n",
    "\n",
    "We can take advantage of this by doing all of those transformations to our images and using those transformed copies to augment our dataset! All of the mirrored, rotated, shifted, etc... images are just as good for the purposes of detecting a cat in an image, so we can use them. Free data!\n",
    "\n",
    "In practice this is common and keras makes it quite easy. We can create a mini-network and apply some transformations, then just stick this into the top of our model. Augmentation, when it makes sense like this, has few to no downsides. We can expect a more generalizable model as if our goal is to spot cats, being able to spot them on the left, or on the right, or shifted, or rotated is an actual thing that we directly want our model to be able to do. \n",
    "\n",
    "Augmenting data and generating training data is something that is relatively common in many areas of machine learning - gathering and storing data can be expensive, so we want to make the most of what we have. The image examples are probably the most easy to understand, but the same idea applies to other types of data. The creation of training data is obviously something that is highly application dependent, and follows a simple idea that we've touched on a few times - as long as it helps the model predict what we need, it doesn't matter what the training data actually looks like. We've seen this in things like doing a log-transformation on income, or grouping infrequent categories into \"other\", the only goal is to help the model, and we can sometimes do that by distorting the data.\n",
    "\n",
    "### Image Generators\n",
    "\n",
    "As well, we can also introduce image generators, which can also help quite a bit in preparing our data. Image generators are a way to create a dataset from a directory of images, but with some extra functionality. We can use them to apply transformations to the images, and we can also use them to apply some preprocessing to the images. Here, we can do some augemntation. These generators are like a \"smart\" version of a dataset, more specifically they take in the data, potentially do some things with it, then generate the data as output on demand. Here we will ask each one to generate some images with the transformations that we setup in the generator as augmentation steps. \n",
    "\n",
    "One thing to note here is that I have a seemingly redundant piece of pulling data. This is because some of the augmentation functions depend on knowing the data, such as normalization, which needs to know the range of values that it is normalizing. This is why we pull some data, use it to set those parameters, then pull the data again to actually create the datasets. This is weird, and poorly designed - I thought that this really wouldn't be the correct way to do it, but it looks like this is a bug (or poor design choice) in the newest version of tensorflow. There are a lot of choices of things that we can do to augment, the tensorflow documentation has a good list of them here: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator (I commented these out as it makes for more printable images if we don't use them)\n",
    "\n",
    "<b>Big Note:</b> In my trials I got a few warning messages that were along the lines of \"there is no registered converter\", along with a note about a while loop. As far as I can tell this is a bug in a newer version of tensorflow (it didn't happen when I wrote this). These warnings mean that the process will be slow, but things should otherwise work. I will assume that at some point this error will be fixed. For some more info: https://stackoverflow.com/questions/73304934/\n",
    "\n",
    "As far as I can see there are two workarounds, one is to downgrade tensorlfow, which I wouldn't reccomend on your python environment as it can be annoying, you could make another environment, your \"Python_augmenting_env\", and install the desired version there. The other is to basically rewrite the augmentation code, which is a bit of a pain. For us, we can try a specific import in Colab, since the environment is temporary, we don't need to worry about breaking anything else. I'm not going to put this in the code, because I don't want to cause any issues accidentally, but we can manually place it in the code if we desire. \n",
    "\n",
    "tensorflow-data-augmentation-gives-a-warning-using-a-while-loop-for-converting even the Tensorflow documentation example for image classification gets hit:\n",
    "\n",
    "![Augmentation Warning](images/augment_error.png \"Augmentation Warning\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAugmenter = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #fill_mode='nearest',\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2, \n",
    "    rescale=1./255,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "valAugmenter = ImageDataGenerator(\n",
    "    validation_split = 0.2,\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "# Pull Data to train the augmentation\n",
    "fitAugmenter = ImageDataGenerator(\n",
    "    validation_split = 0.1,\n",
    "    rescale=1./255\n",
    "    )\n",
    "fitAug = fitAugmenter.flow_from_directory(data_dir, seed=33, subset=\"validation\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n",
    "trainAugmenter.fit(fitAug.next()[0])\n",
    "\n",
    "augTrain = trainAugmenter.flow_from_directory(data_dir, seed=33, subset=\"training\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n",
    "augVal = valAugmenter.flow_from_directory(data_dir, seed=33, subset=\"validation\", batch_size=BATCH_SIZE, class_mode=\"categorical\",)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preview Image\n",
    "\n",
    "The generator generates files, on demand. In our context, we basically load the data into the generator, use the generator \"as\" the dataset, and the generator will generate the data, along with the augmentation, on demand when we fit the model. We can pull one image and print it to see what we get from the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, ytmp = augTrain.next()\n",
    "for i in range(0,4):\n",
    "    image = tmp[i]\n",
    "    label = ytmp[i]\n",
    "    print (label)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Using the image generators is pretty simple. The genearators \"become\" the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(augment)\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "train_log = model.fit(augTrain, epochs=BASE_EPOCHS, verbose=1, callbacks=[callback], validation_data=augVal)\n",
    "\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the original dataset from the first section and build a model incorporating some of the CNN features. Add data augmentation, then manipulate things such as the number of layers, kernel size, padding, dropouts, etc... to try to improve accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Reasoning on What to Try\n",
    "\n",
    "In the examples above, the training accuracy generally got quite good, and in the earlier models where we didn't do as many changes, there was a large gap between testing and training accuracy. This gap lessened as we added regularization and dropouts, so that's a good sign that the model was overfitting. Out model likely has enough capacity with about 2 hidden layers. This doesn't necissarily mean that we should stop there, but we can pretty safely thing that is \"enough\" in terms of the capacity of the model. If we add more layers it isn't to make the model able to learn the data, we've already gotten there. We have also seen that expanding the number of filters in a convolutional layer can help, so we can add that in. As well, we can augment the data, which should help in general.\n",
    "\n",
    "<b>Note:</b> the broken augmentation puts a bit of a damper on this one, as it is very slow to run trials. We should be able to add augemntation to one of the model configurations that got around 80% accuracy above, and see the overfit gap lessen, potentially leaving some room to allow for more fitting - either more epochs or more capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "trainDatagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.2,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "valDatagen = ImageDataGenerator(\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "# We have the data, so we can fit to it. \n",
    "trainDatagen.fit(X_train)\n",
    "\n",
    "train_generator = trainDatagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='training', seed=55)\n",
    "validation_generator = valDatagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='validation', seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=\"l2\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=metric_list)\n",
    "train_log = model.fit(train_generator,\n",
    "                      validation_data=validation_generator,\n",
    "                      epochs=BASE_EPOCHS, \n",
    "                      verbose=1, \n",
    "                      callbacks=[callback],\n",
    "                      workers=4)\n",
    "train_eval = model.evaluate(X_train, y_train)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_log)\n",
    "plot_acc(train_log)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
