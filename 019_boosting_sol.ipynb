{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Early on we looked at one type of ensemble model - bagging, specifically with Forests made from several trees. When bagging we take a bunch of copies of the data, slice it up into random subsets, let each model make predictions, then combine those predictions (via vote/average) into a final answer. Bagging is effective at combatting overfitting, and this is especially visible with those trees since we could see how overfitted a tree model could get if we allowed it to grow large. \n",
    "\n",
    "Boosting is another type of ensemble that takes a different approach to combining several models. Boosting is sequential - it takes the results from one model, and uses those results to guide the training of a subsequent model. In order to explain this, we need to first examine one other concept - weak and strong learners. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Learners and Strong Learners\n",
    "\n",
    "We can split predictive models into two classes, depending on their performance:\n",
    "<ul>\n",
    "<li> Weak learners - models that perform only slightly better than guessing. \n",
    "<li> Strong learners - models that perform \"much\" better than guessing.\n",
    "</ul>\n",
    "\n",
    "We've been looking at strong learners when building models so far - regression, SVM, SGD - all able to achieve a high level of accuracy if we tune them with appropriate hyperparameters, thus all strong learners.\n",
    "\n",
    "What's a weak learner? The easiest example is a tiny decision tree - if we were to limit the depth to 1 levels - representing one decision. Technically we can create weak learners out of almost any model by tuning them to eliminate variance. Trees are common though. \n",
    "\n",
    "![Stump](images/stump.png \"Stump\")\n",
    "\n",
    "### Why Would Anyone Want a Weak Learner?\n",
    "\n",
    "We generally want accuracy, so conciously moving to a less accurate model seems... odd. Following from the performance of weak and strong learners, we can draw another conclusion that should be intuitive:\n",
    "<ul>\n",
    "<li> Weak learners are simple to compute. E.g. a one-decision decision tree is simple. This is also called a stump. \n",
    "<li> Strong learners are hard to compute. \n",
    "</ul>\n",
    "\n",
    "The \"magic\" comes from combining weak learners with the concept of boosting. We can combine many simple and quick weak learning models together through boosting, and acheive an ensemble that can acheive high accuracy, like a strong learner. These boosted ensembles can deliver very good performance in practice, with low amounts of overfitting and high levels of accuracy. We'll look at two examples that are popular - Adaboost and XGboost. \n",
    "\n",
    "### Boosting Ensembles\n",
    "\n",
    "Boosting ensembles are another approach to combining models, that takes an alternative approach to the bagging ensembles that we looked at with forests. Boosting ensembles are different in that they use the results of one model to train the next model. In practice, that means that our boosted model will be a sequence of weak learners, each adding a little bit to the overall capability of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "Adaboost is short for adaptive boosting, it is one of, if not the, most commonly used boosting algorithms. Adaboost is conceptually fairly simple in how it learns:\n",
    "<ul>\n",
    "<li> Generate predictions for the data in the training set with the first model.\n",
    "    <ul>\n",
    "    <li> Predictions that were incorrect get their weights increased, so they are selected more often. \n",
    "    <li> Predictions that are correct have their weights decreased. \n",
    "    </ul>\n",
    "<li> Pass the data, along with the weights, on to the next model. \n",
    "<li> As the predictions are being made, the individual models are also evaluated and weighted. \n",
    "    <ul>\n",
    "    <li> Models that predict correctly get weighted higher. \n",
    "    <li> Models that do not predict correctly are weighted lower.  \n",
    "    </ul>\n",
    "<li> Repeat until everything is correct, or you've hit the limit. \n",
    "<li> Predictions are a weighted sum (using the weights of the models) of the predictions of all the models. \n",
    "</ul>\n",
    "\n",
    "![Adaboost](images/adaboost.ppm \"Adaboost\")\n",
    "\n",
    "We can think of the process roughly like this:\n",
    "<ul>\n",
    "<li> We make predictions with a bunch of quick but simple models, those that are the most accurate have their importance to the final predictions increased. \n",
    "<li> We take the data that hasn't been correctly classified, and predict it with more models until we get it correct. All the ones that we've already correctly predicted need less attention. \n",
    "</ul>\n",
    "\n",
    "It is kind of like if you were to look at a room of people, and classify them as either NFL players or gymnasts using some simple tests. First you'd look at weight, that would do a very good job and get a high score as a model. Then you may take the leftover people and split them by height. Then you'd take the rest and split them by net worth.... Eventually you may split them by number of broken bones or something similarly obscure. The models that do the best job of splitting the groups would get the biggest impact on the final classification. The records that are easy to classify would be done immediately with one of the important models, those that are harder would filter through more and more models until they are able to be accurately classified. \n",
    "\n",
    "Most examples of boosting look at classification because it is more illustrative. Regression works the same way, we just replace the accuracy rate with a metric like MSE. Accurate predictions are like correct ones, and the most accurate models are promoted in importance. \n",
    "\n",
    "### Return of the Tree\n",
    "\n",
    "The boosting concept that we see in Adaboost can work with any model, but is is normally done using very small decision trees. These trees are called stumps, and are very simple. They are normally limited to a depth of 1, and have only one decision. Trees have the benefit of being able to capture relationships that are non-linear, or are not easily captured even by a polynomial or radial model. Trees are also quite quick to predict, especially when very small, as each prediction is just a single decision. The overfitting tendency of trees that we constantly fought against when using them as strong learners is also largely negated here, as we don't have the high depth trees that tend towards overfitting. Trees as the base estimator for boosted models are leaders in the best performing non-neural network models available today. \n",
    "\n",
    "\n",
    "### Using Adaboost\n",
    "\n",
    "Using adabost is pretty similar to using every other model. Adaboost can perform both classification and regression, the mechanics of each rely on the underlying algorithm. That algorithm is normally a decision tree classifier/regressor, but other estimators can be used if we supply them as a parameter. If you've ever noticed the \"weights\" parameter on the documentation page of models, this is where it is used - adaboost updates the weights as it feeds the data through the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ada = pd.read_csv(\"data/titanic_train.csv\")\n",
    "y_ada = df_ada[\"Survived\"]\n",
    "X_ada = df_ada.drop(columns={\"Survived\"})\n",
    "df_ada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7802690582959642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "column_trans = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "    ('title_bow', SimpleImputer(strategy=\"median\"), [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "#Look at the viso at what he did for the gridesearch\n",
    "ada_pipe = Pipeline([\n",
    "    (\"ct\", column_trans),\n",
    "    (\"ada\", ada)\n",
    "])\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_ada, y_ada)\n",
    "ada_pipe.fit(X=Xtr, y=ytr.ravel())\n",
    "\n",
    "ada_preds = ada_pipe.predict(Xte)\n",
    "accuracy_score(yte, ada_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Predict quality. \n",
    "\n",
    "Try swapping some other estimator in the adaboost. If you have extra time, do a grid search for different estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  Id  \n",
       "0      9.4        5   0  \n",
       "1      9.8        5   1  \n",
       "2      9.8        5   2  \n",
       "3      9.8        6   3  \n",
       "4      9.4        5   4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"data/WineQT.csv\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wine = wine[\"quality\"]\n",
    "X_wine = wine.drop(columns={\"Id\", \"quality\"})\n",
    "X_wine_tr, X_wine_te, y_wine_tr, y_wine_te = train_test_split(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.639560730015005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "#Try with different estimator\n",
    "test_est = LinearRegression()\n",
    "#test_est = LinearSVR(max_iter=10000)\n",
    "wine_ada = AdaBoostRegressor(base_estimator=test_est, n_estimators=100, learning_rate=.1)\n",
    "#wine_ada = AdaBoostRegressor()\n",
    "\n",
    "wine_ada_pipe = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"ada\", wine_ada)\n",
    "])\n",
    "\n",
    "wine_ada_pipe.fit(X=X_wine_tr, y=y_wine_tr.ravel())\n",
    "\n",
    "wine_ada_preds = wine_ada_pipe.predict(X_wine_te)\n",
    "mean_squared_error(wine_ada_preds, y_wine_te, squared=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost and Gradient Boosting\n",
    "\n",
    "XGboost is short for extreme gradient boosting, a new and ofter very accurate ensemble method. \n",
    "\n",
    "#### Installing XGboost\n",
    "\n",
    "XGboost isn't part of SK Learn, we have to install it. The package name is xgboost and it can be installed through whatever means works on your computer:\n",
    "<ul>\n",
    "<li> pip install xgboost\n",
    "<li> conda install -c conda-forge xgboost\n",
    "<li> conda gui installation, if it is available there. \n",
    "</ul>\n",
    "\n",
    "I installed it via pip, and got weird errors (kernel died) when using it. Running:\n",
    "\n",
    "conda install -c conda-forge xgboost\n",
    "\n",
    "did fix it. \n",
    "\n",
    "There are dependencies, so the process may require installing other items to make it work. On my work Mac, after installing XGboost, I got an error and had to install another library with the command: \"brew install libomp\". This command was given to me by the error message when I tried to import xgboost in code. Based on past experience, there may be some variety \n",
    "\n",
    "Install documentation is here: https://xgboost.readthedocs.io/en/stable/install.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Before we can get eXtreme!!!!! with xgboost, we need to look at one of it's main component parts - gradient boosting. Gradient boosting is another type of boosting, similar to Adabost, however with gradient boosting the subsequent models are trained on the residual error of the previous model. \n",
    "\n",
    "The big difference is that gradient boosting is trained on the residual errors and this is the factor that increases importance for the next training. In Adaboost the high weight records are given more importance, in gradient boosting the higher gradients are given more importance. \n",
    "\n",
    "The process that gradient boosting uses is:\n",
    "<ul>\n",
    "<li> Make an initial set of predictions - this is often done by just making a generic prediction for all records, such as the average. This will generate residuals, or more generally, some measure of loss defined by the loss function. These residuals are the starting point. \n",
    "<li> Fit a weak learner with the feature set and the residuals of the previous round. In gradient boosting the weak learners (normally trees) tend to be larger - 8 to 32 nodes. \n",
    "<li> Repeat the previous step repeatedly until the limit of number of estimators is reached. \n",
    "<li> Predictions are the initial prediction, plus all of the residual predictions averaged together (scaled by the learning rate). This is called shrinkage. \n",
    "    <ul>\n",
    "    <li> y(pred) = y1 + (eta *  r1) + (eta * r2) + ....... + (eta * rN)\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "![Gradient](images/gradientboosting.png \"Gradient\" )\n",
    "\n",
    "So what happens is that the first model makes predictions, and the next one is trained on those residuals. The subsequent models act to \"bump\" the predictions up or down by trying to correct the errors of the previous model's predictions. Each model adjust the incomming predictions by a bit, until it has eventually been adjusted enough to fit the data (or we hit a size limit). This is why gradient boosting is often called \"shrinkage\", each additional model shrinks the errors a bit. This process is still relatively fast, as the trees are small and the number of estimators is limited. We can visualize the process like this:\n",
    "\n",
    "![Gradient Boosting](images/grad_boost.webp \"Gradient Boosting\" )\n",
    "\n",
    "At the end of the fitting process, we end up with a model that starts with an initial prediction, then each subsequent model provides a little adjustemnt, based on the residuals of the previous model.\n",
    "\n",
    "![Gradient Boosting](images/grad_boost.jpg \"Gradient Boosting\" )\n",
    "\n",
    "Gradient boosting also works for regression and classification. With the algorithm here it is easier to think of as a regression problem since it is based on residuals.  \n",
    "\n",
    "For the most part the parameters for gradient boosting are ones that we are familiar with - error metrics, learning rate, tree options (#leafs etc...), and the number of estimators used. Gradient boosting is typically fairly resistant to overfitting, so a large number of estimators will often be better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>BodyFat</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Neck</th>\n",
       "      <th>Chest</th>\n",
       "      <th>Abdomen</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Thigh</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>Biceps</th>\n",
       "      <th>Forearm</th>\n",
       "      <th>Wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0708</td>\n",
       "      <td>12.3</td>\n",
       "      <td>23</td>\n",
       "      <td>154.25</td>\n",
       "      <td>67.75</td>\n",
       "      <td>36.2</td>\n",
       "      <td>93.1</td>\n",
       "      <td>85.2</td>\n",
       "      <td>94.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0853</td>\n",
       "      <td>6.1</td>\n",
       "      <td>22</td>\n",
       "      <td>173.25</td>\n",
       "      <td>72.25</td>\n",
       "      <td>38.5</td>\n",
       "      <td>93.6</td>\n",
       "      <td>83.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>58.7</td>\n",
       "      <td>37.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>30.5</td>\n",
       "      <td>28.9</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0414</td>\n",
       "      <td>25.3</td>\n",
       "      <td>22</td>\n",
       "      <td>154.00</td>\n",
       "      <td>66.25</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>87.9</td>\n",
       "      <td>99.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>38.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.8</td>\n",
       "      <td>25.2</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0751</td>\n",
       "      <td>10.4</td>\n",
       "      <td>26</td>\n",
       "      <td>184.75</td>\n",
       "      <td>72.25</td>\n",
       "      <td>37.4</td>\n",
       "      <td>101.8</td>\n",
       "      <td>86.4</td>\n",
       "      <td>101.2</td>\n",
       "      <td>60.1</td>\n",
       "      <td>37.3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>32.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0340</td>\n",
       "      <td>28.7</td>\n",
       "      <td>24</td>\n",
       "      <td>184.25</td>\n",
       "      <td>71.25</td>\n",
       "      <td>34.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.9</td>\n",
       "      <td>63.2</td>\n",
       "      <td>42.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Density  BodyFat  Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  \\\n",
       "0   1.0708     12.3   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0   \n",
       "1   1.0853      6.1   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7   \n",
       "2   1.0414     25.3   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6   \n",
       "3   1.0751     10.4   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1   \n",
       "4   1.0340     28.7   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2   \n",
       "\n",
       "   Knee  Ankle  Biceps  Forearm  Wrist  \n",
       "0  37.3   21.9    32.0     27.4   17.1  \n",
       "1  37.3   23.4    30.5     28.9   18.2  \n",
       "2  38.9   24.0    28.8     25.2   16.6  \n",
       "3  37.3   22.8    32.4     29.4   18.2  \n",
       "4  42.2   24.0    32.2     27.7   17.7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gb = pd.read_csv(\"data/bodyfat.csv\")\n",
    "y_gbr = df_gb[\"BodyFat\"]\n",
    "X_gbr = df_gb.drop(columns={\"BodyFat\"})\n",
    "df_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428568605491951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr_pipe = Pipeline([\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"gbc\", gbr)\n",
    "])\n",
    "\n",
    "Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(X_gbr, y_gbr)\n",
    "gbr_pipe.fit(X=Xtr_r, y=ytr_r.ravel())\n",
    "\n",
    "gbr_preds = gbr_pipe.predict(Xte_r)\n",
    "mean_squared_error(yte_r, gbr_preds, squared=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoosting\n",
    "\n",
    "SKlearn also provides a package called HistGradientBoosting which is inspired by LightGBM. It can be much faster when datasets get large and has the side benefit of automatically handling missing values. Try with this one if you have a moment, details for the classifier are: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html The underlying change is that these algorithms bin (discreetize) the data up front, which reduces the number of splits and allows the math to be done on integers, which is faster. HGB also defaults to utilizing early stopping to speed processing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the classifier version of gradient boosting to predict titanic survival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026905829596412"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titan_gb_mod = GradientBoostingClassifier()\n",
    "\n",
    "titan_gb_ct = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "    ('title_bow', SimpleImputer(strategy=\"median\"), [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "titan_gb_pipe = Pipeline([\n",
    "    (\"ct\", titan_gb_ct),\n",
    "    (\"model\", titan_gb_mod)\n",
    "])\n",
    "\n",
    "titan_gb_pipe.fit(X=Xtr, y=ytr.ravel())\n",
    "\n",
    "titan_gb_preds = titan_gb_pipe.predict(Xte)\n",
    "accuracy_score(yte, titan_gb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classification and Stochastic Gradient Boosting\n",
    "\n",
    "Using gradient boosting for classification is similar. One additional hyperparameter we can utilize here is \"subsample\", which controls the fraction of records used for each learner. Setting this to be less than 1 causes the algorithm to use stochastic gradient boosting - utilizing a randomized subset of the data for each tree. This tends to protect against overfitting, similarly to how it works in a forest model. If the dataset is large, this can also speed things up. A split of 30% to 70% is pretty typical to try, typically towards the lower-middle of that range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161434977578476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(subsample=.3)\n",
    "\n",
    "column_trans2 = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "    ('title_bow', SimpleImputer(strategy=\"median\"), [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "gbc_pipe = Pipeline([\n",
    "    (\"ct\", column_trans2),\n",
    "    (\"gbc\", gbc)\n",
    "])\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_ada, y_ada)\n",
    "gbc_pipe.fit(X=Xtr, y=ytr.ravel())\n",
    "\n",
    "gbc_preds = gbc_pipe.predict(Xte)\n",
    "accuracy_score(yte, gbc_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost\n",
    "\n",
    "Ok, now we're ready to be eXtreme!!!! and use xgboost. XGboost is basically an implementation of gradient boosting that adds in many of the concpets that we've previously touched on to improve its speed, accuracy, and resiliance to overfitting - thus making it eXtreme!!! Some of the things xgboost incorporates are:\n",
    "\n",
    "<ul>\n",
    "<li> Parallelization - boosting is generally a sequential process, so it is hard to run in parallel. Xgboost is carefully written to allow the parts that can be run in parallel, to be run in parallel. This makes xgboost faster than it would be with a traditional boosted implementation. \n",
    "<li> Cross validation - xgboost builds in cross-validation as part of it's standard execution. \n",
    "<li> Regularization - xgboost applies regularization (L1 or L2) to limit overfitting. \n",
    "<li> Sparse and missing data handling - xgboost smartly deals with data that has missing values, or data that is sparse. \n",
    "<li> Optimization - xgboost is written to utilize hardware resources very efficiently, including both processing power and memory limits. \n",
    "</ul>\n",
    "\n",
    "![XGboost](images/xgboost.jpeg \"XGboost\" )\n",
    "\n",
    "In short, xgboost is effectively a compilation of many of the tools that we've looked at to build better models, all wrapped into one simple package. Outside of neural networks with very large datasets, xgboost is generally the most likely algorithm to be the \"best\", both in terms of accuracy and speed. This isn't universal, of course, different data will perform differently with different algorithms, but xgboost is likely to be a winner, probably more than any other algorithm. \n",
    "\n",
    "<b>Note:</b> these models are relatively new, xgboost itself was only created less than 10 years ago. Similar competitors like LightGBM and CatBoost are vying to surplant xgboost as the best algorithm. They are basically the same at their core - boosting of weak learners, with assorted optizations for accuracy and speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGboost and eXtreme!!! Speed\n",
    "\n",
    "One of the benefits of xgboost is that it is fast, eXtremely fast. The 70,000 digit images was very slow to process in past attempts with other algorithms, to the point that we cut it down to 10,000 or so to make the time manageable. We can see what xgboost can do for us with the full 70,000 images along with a very gentle PCA that should keep the vast majority of the varaiance. Recall we had a 150 component PCA that only slightly blurred the images. \n",
    "\n",
    "Some parameters that we may want to look at for xbgoost are:\n",
    "<ul>\n",
    "<li> booster: can be changed to gblinear to swap the default tree models to linear ones. \n",
    "<li> max_depth: same as in trees. \n",
    "<li> lambda/alpha: L2 and L1 regularization strength, respectively. \n",
    "<li> eta: amount of pruning. \n",
    "</ul>\n",
    "The full set is well documented, and fairly easy to understand, located: https://xgboost.readthedocs.io/en/stable/parameter.html \n",
    "\n",
    "Note: xgboost defaults to creating the maximal number of threads, similar to doing n_jobs=-1 for things like a forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Load Data\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taraz\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(max_depth=2, objective=['multi:softmax'], num_class=10, use_label_encoder=False, verbosity=1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('standard_scaler', StandardScaler()), \n",
    "    ('pca', PCA(200)), \n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train.astype('int'))\n",
    "xgb_preds = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.astype(int)\n",
    "print(accuracy_score(y_test, xgb_preds))\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_preds), annot=True, cmap=\"BrBG_r\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's... <h1>eXtreme!!!!!!!!!!!!!!!</h1> \n",
    "\n",
    "\n",
    "![eXtreme](images/xtreme.gif \"eXtreme\")\n",
    "\n",
    "We can process the whole set of 70,000 in a reasonable amount of time, at a high accuracy. XGboost is currently the overall \"king\" of the non-neural network algorithms for most problems. \n",
    "\n",
    "In general, boosted models tend to perform very well. The algorithms are able to generate models that are both well fitted to data, and resistand to overfitting. Elaborate implementations such as xgboost and its competitors wrap it multiple other optimizations to both optimize the model (e.g. regularization) and optimize the speed of the model (e.g. parallelization). One downside is that they can be sensitive to outliers, as they are based on residuals, and models aim to correct the errors in the past; this is something that is easy to mitigate though. Another downside is speed, as the models are built sequentially, they can be slow to train - though, as mentioned, the implementations such as xgboost are able to optimize this. These gradient boosting models are very commonly used, and are relevant for pretty much any classification or regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Predict the categories of the newsgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", shuffle=True, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", shuffle=True, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD(n_components=100)\n",
    "model = xgb.XGBClassifier(max_depth=2)\n",
    "\n",
    "news_steps = [\n",
    "    ('svd', tsvd),\n",
    "    ('m', model)\n",
    "    ]\n",
    "    \n",
    "news_model = Pipeline(steps=news_steps)\n",
    "news_model.fit(X_train, y_train)\n",
    "news_preds = news_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, news_preds))\n",
    "sns.heatmap(confusion_matrix(y_test, news_preds), annot=True, cmap=\"BrBG_r\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
