{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import PIL\n",
    "if IN_COLAB:\n",
    "    !pip install --ignore-installed Pillow==9.0.0\n",
    "    !pip install -U git+https://github.com/keisen/tf-keras-vis.git@4a90becb02ed3d44825300fcb807dd58157787ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "### Feature Extraction and Classification\n",
    "\n",
    "One of the key concepts needed with transfer learning is the separating of the feature extraction from the convolutional layers and the classification done in the fully connected layers.\n",
    "<ul>\n",
    "<li> The convolutional layer finds features in the image. I.e. the output of the end of the convolutional layers is a set of image-y features. \n",
    "<li> The fully connected layers take those features and classify the thing. \n",
    "</ul>\n",
    "\n",
    "The idea behind this is that we allow someone (like Google) to train their fancy network on a bunch of fast computers, using millions and millions of images. These classifiers get very good at extracting features from objects. \n",
    "\n",
    "When using these models we take those convolutional layers and slap on our own classifier at the end, so the pretrained convolutional layers extract a bunch of features with their massive amount of training, then we use those features to predict our data!\n",
    "\n",
    "### Tensorboard Up-Front\n",
    "\n",
    "we'll also launch the tensorboard prior to doing any training. Pay attention to the log locations in each callback, we can nest the logs in folders, then use the names and tensorboard's regex search to monitor each run as it progresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "\n",
    "There are several models that are pretrained and available to us to use. VGG16 is one developed to do image recognition, the name stands for \"Visual Geometry Group\" - a group of researchers at the University of Oxford who developed it, and ‘16’ implies that this architecture has 16 layers. The model got ~93% on the ImageNet test that we mentioned a couple of weeks ago. \n",
    "\n",
    "![VGG16](images/vgg16.png \"VGG16\" )\n",
    "\n",
    "#### Slide Convolutional Layers from Classifier\n",
    "\n",
    "When downloading the model we specifiy that we don't want the top - that's the classification part. When we remove the top we also allow the model to adapt to the shape of our images, so we specify the input size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "Our VGG 16 model comes with a preprocessing function to prepare the data in a way it is happy with. For this model the color encoding that it was trained on is different, so we should prepare the data properly to get good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_depth = 3\n",
    "\n",
    "train_ds_orig = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds_orig = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds_orig.class_names\n",
    "print(class_names)\n",
    "\n",
    "def preprocess(images, labels):\n",
    "  return tf.keras.applications.vgg16.preprocess_input(images), labels\n",
    "\n",
    "train_ds = train_ds_orig.map(preprocess)\n",
    "val_ds = val_ds_orig.map(preprocess)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Some Images for Tensorboard\n",
    "\n",
    "We'll record some images, both pre and post processing. The VGG model wants images to use a different representation than RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.create_file_writer(\"logs/VGG/train_data\")\n",
    "train_ds_iterator = train_ds_orig.as_numpy_iterator()\n",
    "train_proc_iterator = train_ds.as_numpy_iterator()\n",
    "samp_batch  = train_ds_iterator.next()\n",
    "proc_batch = train_proc_iterator.next()\n",
    "\n",
    "with file_writer.as_default():\n",
    "    images = np.reshape(samp_batch[0].astype(\"uint8\"), (-1, img_height, img_width, img_depth))\n",
    "    procImages = np.reshape(proc_batch[0].astype(\"uint8\"), (-1, img_height, img_width, img_depth))\n",
    "    #images = samp_batch\n",
    "    tf.summary.image(\"32 Original Images\", images, max_outputs=32, step=0)\n",
    "    tf.summary.image(\"32 Processed Images\", procImages, max_outputs=32, step=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add on New Classifier\n",
    "\n",
    "If we look at the previous summary of the model we can see that the last layer we have is a MaxPool layer. When making our own CNN this is the last layer before we add in the \"normal\" stuff for making predictions, this is the same. We need to flatten the data, then use dense layers and an output layer to classify the predictions. \n",
    "\n",
    "We end up with the pretrained parts finding features in images, and the custom part classifying images based on those features. If we think back to the concept of a convolutional network, the convolutional layers do the true heavy lifting in allowing us to do things like classify images, they take in the raw images and transform it into a set of features contained in that image. This ability to turn images into predictive features is the key - important parts of images like edges, corners, contrast, etc... are generic, and our borrowed model is excellent at finding these features in images. Our predicitons are unique, so we tweak the training of our model to make predictions for our data, into our classes - all based on the features that the borrowed model found! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model\n",
    "\n",
    "We take the model without the top, set the input image size, and then add our own classifier. Loading the model is simple, there are just a few things to specify:\n",
    "<ul>\n",
    "<li> weights=\"imagenet\" - tells the model to use the weights from its imagenet training. This is what brings the \"smarts\", so we want it. \n",
    "<li> include_top=False - tells the model to not bring over the classifier bits that we wnat to replace. \n",
    "<li> input_shape - the model is trained on specific data sizes (224x224x3). We can repurpose it by changing the input size. \n",
    "</ul>\n",
    "\n",
    "We also set the VGG model that we download to be not trainable. We don't want to overwrite all of the training that already exists, coming from the original training. What we want to be trained are the final dense parts we added on to classify our specific scenario. All the weights in the convolutional layers are kept the same, as they have been developed through large amounts of training; the weights in the fully connected layers will be trained, resulting in a model that combines the \"sight\" of the pretrained model with the context of what we are trying to classify. The VGG bits will just show as though they are one layer in our model, and for training purposes that makes sense. We can also see in the \"trainable params\" listing in the summary, the large number of weights in that VGG section we are borrowing are not trainable - that's the smart part of the model. \n",
    "\n",
    "<b>Note:</b> I think the \"top\" label is a bit misleading, as it isn't really the top, it is the part at the end that shows at the bottom of a summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, img_depth))\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "# Add Dense Stuff\n",
    "flatten_layer = Flatten()\n",
    "dense_layer_1 = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "drop_layer_1 = Dropout(.2)\n",
    "dense_layer_2 = Dense(256, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "prediction_layer = Dense(5)\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    drop_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and Train\n",
    "\n",
    "Once the new Frakenstein model is built we finish the training process as we normally would. The only difference is that here the weights of the VGG part of the model are not being adjusted during the backpropagation steps, only the weights in the layers that we added at the end are. For many, if not most, applications, this approach of adapting a pretrained model will give the best real world results. Unless you happen to live in a data centre, you probably lack both the data and the processing capacity to train any model from scratch to be as good as those that we can download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),  \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/initial/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "stopping_callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, mode=\"max\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/initial/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Models\n",
    "\n",
    "Lastly, we can adapt the entire model to our data. We'll unfreeze the original model, and then train the model again. The key addition here is that we set the learning rate to be extremely low (here it is 2 orders of magnitude smaller than the default) so the model doesn't totally rewrite all of the weights while training, rather it will only change a little bit - fine tuning its predictions to the actual data! Here the oringal convolutional layers are trainable, and the weights will be adjusted during training, but we dial the learning rate way down so that our changes only impact the model a little bit. This is a greater degree of fine tuning than we get when we lock the VGG layers, but it is still mainly relying on the previous training of the VGG model.\n",
    "\n",
    "The end result is a model that can take advantage of all of the training that the original model received before we downloaded it. That ability of extracting features from images is then reapplied to our data for making predictions based on the features identified in the original model. Finally we take the entire model and just gently train it to be a little more suited to our data. The best of all worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a copy of the above model for next test. \n",
    "copy_model = model\n",
    "\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    ")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/fine_tune/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/fine_tune/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1, callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer + Fine Tuning Results\n",
    "\n",
    "Yay, that's probably pretty accurate! In initial testing with 1 epoch, I got results around 80% before the fine tuning, and over 85% after the fine tuning. That's with 1 epoch! Other runs where we allow it to tune more trend to be even better - allowing 5 epochs of training + 5 epochs of fine tuning, my validation accuracy was around 90% and the training accuracy was nearing 100% - we could likely do even better with more aggressive regularization. \n",
    "\n",
    "This will likely be a great approach for something like image recognition!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the Model Looking?\n",
    "\n",
    "One of the things that we may wonder is how our models make decisions, or what are the looking at to do so. A tool that can help illustrate that is called a salience map. A salience map shows a visual representation of what parts of the image are impacting the final prediction the most. While the CNN process is largely a black box, this is one way to gain a little insight on what is going on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Focus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from tensorflow.keras.preprocessing.image import load_img\n",
    "    from tf_keras_vis.utils.scores import CategoricalScore\n",
    "    \n",
    "    # Image titles\n",
    "    image_titles = ['daisy', 'roses', 'tulips']\n",
    "    score = CategoricalScore([0, 2, 4])\n",
    "\n",
    "    # Load images and Convert them to a Numpy array\n",
    "    img1 = load_img('/root/.keras/datasets/flower_photos/daisy/100080576_f52e8ee070_n.jpg', target_size=(224, 224))\n",
    "    img2 = load_img('/root/.keras/datasets/flower_photos/roses/10894627425_ec76bbc757_n.jpg', target_size=(224, 224))\n",
    "    #img3 = load_img('/root/.keras/datasets/flower_photos/tulips/10128546863_8de70c610d.jpg', target_size=(224, 224))\n",
    "      img3 = load_img(\"/root/.keras/datasets/flower_photos/tulips/12764617214_12211c6a0c_m.jpg\", target_size=(224, 224))\n",
    "    images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
    "\n",
    "    # Preparing input data for VGG16\n",
    "    X = preprocess_input(images)\n",
    "\n",
    "    # Rendering\n",
    "    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    for i, title in enumerate(image_titles):\n",
    "        ax[i].set_title(title, fontsize=16)\n",
    "        ax[i].imshow(images[i])\n",
    "        ax[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "#### Show Saliency\n",
    "\n",
    "The bright spots in the image are the areas that the model is focusing on to make its prediction. The darker areas are not as important. We can think of this as a rough approximation of feature importance from something like a tree, only in 2D. \n",
    "\n",
    "Using a salience map in detail to tune our models goes beyond the scope of what we are going to do, but it does allow us to get at least some insight. The most direct thing that we can do is that we can figure out which parts of images are relied on for the model to do its job, this can help us to understand what the model is looking for. For something like image recognition, this could lead you to think about how the images are processed - for example, it is very common to snip out parts of larger images, usually the center, for use in a predictive model. This could show us evidence of if we are capturing the important parts or if we should modify that image prep process. I we were considering the padding decision, this could also give us an idea of if the edges matter or not for the model's predicitons. If the most important parts of the image are the \"thing\", then we are likely doing well, if they most important parts are background or the periphery, then we may been to change some things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "    replace2linear = ReplaceToLinear()\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tf_keras_vis.saliency import Saliency\n",
    "    # from tf_keras_vis.utils import normalize\n",
    "\n",
    "    # Create Saliency object.\n",
    "    saliency = Saliency(model, model_modifier=replace2linear, clone=True)\n",
    "\n",
    "    # Generate saliency map\n",
    "    saliency_map = saliency(score, X)\n",
    "    # Generate saliency map with smoothing that reduce noise by adding noise\n",
    "    saliency_map = saliency(score,\n",
    "                            X,\n",
    "                            smooth_samples=20, # The number of calculating gradients iterations.\n",
    "                            smooth_noise=0.20) # noise spread level.\n",
    "\n",
    "    # Render\n",
    "    f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    for i, title in enumerate(image_titles):\n",
    "        ax[i].set_title(title, fontsize=14)\n",
    "        ax[i].imshow(saliency_map[i], cmap='jet')\n",
    "        ax[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Drastic Retraining\n",
    "\n",
    "If we are extra ambitious we can also potentially slice the model even deeper, and take smaller portions to mix with our own models. The farther \"into\" the model you slice, the more of the original training will be removed and the more the model will learn from our training data. If done, this is a balancing act - we want to keep all of the smarts that the model has gotten from the original training, while getting the benefits of adaptation to our data. \n",
    "\n",
    "This is something that is hard to just eyeball - to splice parts of models together and create something that is actually superior likely requries a lot of experimentation, a solid understanding of the model's problem you're addressing, and some domain knowledge. For something like this adaptation of the VGG model, we'd probably start with some idea of what the model was weak at, build an understanding of what types of features it was extracting along the way, and insert our own layers where we think it would be most beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, img_depth))\n",
    "#base_model.trainable = False ## Not trainable weights\n",
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze the First 12 Layers\n",
    "\n",
    "We will set the first 12 layers to be frozen, and leave the rest open to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:12]:\n",
    "    layer.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Retraining\n",
    "\n",
    "Now we have larger portions of the model that can be trained. We will be losing some of the pretrained knowldge, replacing it with the training coming from our data. If we look at the trainable params above, there are a bunch that are trainable and a bunch that aren't.\n",
    "\n",
    "We are playing with fire here! Taking away more and more of the \"smart\" model will be risky for actual performance, we are pretty likely to make things worse as we go father and farther into removing the old training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Dense Stuff\n",
    "flatten_layer = Flatten()\n",
    "dense_layer_1 = Dense(512, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "dense_layer_2 = Dense(256, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')\n",
    "prediction_layer = Dense(5)\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    ")\n",
    "            \n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/VGG/drastic/\" + time_stamp\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights/VGG/drastic/\"+time_stamp+\"model.hdf5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, stopping_callback, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We likely see worse results when retraining more of the model, that's to be expected. In general, replacing the classifier and possibly some low learning rate fine tuning is the best solution for most cases like this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - ResNet50\n",
    "\n",
    "This is another pretrained network, containing 50 layers. We can use this one similarly to the last. Try to use transfer learning along with some of your added layers to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def preprocess50(images, labels):\n",
    "  return tf.keras.applications.resnet50.preprocess_input(images), labels\n",
    "\n",
    "train_ds = train_ds_orig.map(preprocess50)\n",
    "val_ds = val_ds_orig.map(preprocess50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train New Classifier\n",
    "\n",
    "Train model with new classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt Retraining Entire Model to Fine Tune\n",
    "\n",
    "We can attempt to unlock the model and retrain in fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning Conclusion\n",
    "\n",
    "Transfer learning is common, especially when working with things like images. Pretrained models that have seen millions upon millions of images get very good at \"understanding\" what is in an image, or extracting important features from those images. This basic ability to \"see\" image data is interchangeable between different types of image tasks that we may want to do. For image data, natural language, audio, video, it is likely that one of these large models will be more capable of extracting features from the data than we could ever hope to do from scratch. Since the basics of \"seeing a thing\" or \"reading a sentence\" is the same no matter the specific application, that ability to process the data that our pretrained models have can be repurposed to our specific ends. \n",
    "\n",
    "We can see lots of scenarios in the real world where people are adapting image recognition models trained by Google to do things like recognize objects in their home security system, or language models like the GPT family being adapted to better understand domain specific language. We'll likely see more of this, as the benefits of training on massive amounts of data are hard, if not impossible, to replicate. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
