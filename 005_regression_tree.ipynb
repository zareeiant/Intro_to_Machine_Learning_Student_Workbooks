{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (14,14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees\n",
    "\n",
    "Trees can also perform regressions in addition to decisions. Using the regression tree models is pretty straightforward and very similar to any other model like linear regression. The regression tree itself is mostly similar to the decision tree, the primary difference is that both the outcomes and the error metrics are adapted to numerical values. \n",
    "\n",
    "<b>A Regression Tree:</b>\n",
    "\n",
    "![Regression Tree](images/regression_tree.webp \"Regression Tree\" )\n",
    "\n",
    "We can start by creating and looking at a regresion tree, as always, the mechanics of making and training the model is the same as we are used to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df\n",
    "\n",
    "df = sklearn_to_df(sklearn.datasets.load_boston())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Model\n",
    "df2 = pd.get_dummies(df, drop_first=True)\n",
    "y = np.array(df2[\"target\"]).reshape(-1,1)\n",
    "X = np.array(df2.drop(columns={\"target\"}))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.get_depth())\n",
    "print(clf.score(X_test, y_test))\n",
    "print(X.shape, y.shape)\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a better image\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(clf,\n",
    "                     out_file=\"output/reg_tree_1.dot\",\n",
    "                     feature_names = df.drop(columns={\"target\"}).columns, \n",
    "                     class_names=[\"0\",\"1\"],\n",
    "                     filled = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree Decision Making\n",
    "\n",
    "The regression tree works very similarly to the decision tree. The key differences are:\n",
    "<ul>\n",
    "<li> <b>Predictions:</b> Instead of producing a classification at the end, it produces an average of all the values in that group. That average is the prediction for anything that falls into that leaf on the tree. \n",
    "<li> <b>Split Decisions: </b>Instead of using the information gain concept that decision trees do, a regression tree tries to minimize error when splitting, normally MSE. So the algorithm seeks splits that have the lowest average error between the error and the values.\n",
    "    <ul>\n",
    "    <li> As a note, this should be familiar from the idea of a cost function. We want the model to minimize the error, how we define error can change, but the process of finding the optimal choice is the same. \n",
    "    <li> Rather than measures of set purity, like gini or entropy, the model uses the error as the metric to measure which split generates the \"best\" fitting tree. \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Just like decision trees, there are a few options that we can set as HP - one of those being the error metric. We can choose absolute error or a couple of others for the error metric; we can also set limits to growth like depth and min_split_size. \n",
    "\n",
    "The more we allow the tree to expand, the more potential predictions we can make, but the more likely we are to overfit. Limiting the tree size means each terminal leaf will represent more records with its prediction, and the tree will be less likely to overfit.\n",
    "\n",
    "#### Regression Tree Limitations\n",
    "\n",
    "One specific weakness with regression trees is that they don't \"extend\" like a linear regression, they're bounded by whatever data they have. So if the maximum prediction that is generated in training is 50, no matter what future inputs look like it will never be able to predict beyond that. We can see this if we chart an example, there isn't a nice smooth prediction curve like a linear regression, we get blocky steps.\n",
    "\n",
    "![Regression Tree](images/regtree2.png \"Regression Tree\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Grid Search to Improve\n",
    "\n",
    "We can also utilize a grid search to do some HP tuning. Along with some other options we can try different error metrics. We can set a list for any of the hyperparameters that we want to use in the grid search, and every combination will be executed and evaluated. \n",
    "\n",
    "Note that the names for absolute and squared error are changing, so depending on the specific version of sklearn you have installed you might need to use absolute_error/squared_error or mae/mse, the meaning is the same, they just changed the label to be more descriptive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_para = {'min_samples_split':[2,3,4,5,6,7,8,9,10],\n",
    "            'max_depth':[7,8,9,10,11,12,13,14,15,16], \n",
    "            'criterion':[\"friedman_mse\", \"poisson\", \"squared_error\", \"absolute_error\"]}\n",
    "\n",
    "clfCV = GridSearchCV(estimator=DecisionTreeRegressor(random_state=0), param_grid=tree_para, cv=10) #See below for the CV argument\n",
    "clfCV.fit(X_train, y_train)\n",
    "clfCV.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimal combo from above and create a new model. We could have also grabbed the best model directly from above and saved it in a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeRegressor(max_depth=8, min_samples_split=8, random_state=0)\n",
    "clf2 = clf2.fit(X_train, y_train)\n",
    "print(clf2.score(X_test, y_test))\n",
    "plot_tree(clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a better image\n",
    "export_graphviz(clf2,\n",
    "                     out_file=\"output/reg_tree_2.dot\",\n",
    "                     feature_names = df.drop(columns={\"target\"}).columns, \n",
    "                     class_names=[\"0\",\"1\"],\n",
    "                     filled = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Predictions\n",
    "\n",
    "We can look at the predictions made by the tree (limit the tree size to make the chart above and the results obvious). Predictions are only at the results of one of the terminal leafs, we don't get a curve like a linear regression. \n",
    "\n",
    "This is a reason that regression trees aren't normally all that common, the number of distinct values that can be predicted is limited by the number of leafs in the tree. If we count the number of distinct predictions made and compare it to the number of total predictions made, we can see how we have a very small number of distinct values being predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf2.predict(X_test)\n",
    "sns.histplot(preds, binwidth=1)\n",
    "\n",
    "print(\"Number of predictions made:\", len(X_test))\n",
    "print(\"Unique predictions:\", len(np.unique(preds)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Predict the Target (BodyFat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df_ = pd.read_csv(\"data/bodyfat.csv\")\n",
    "\n",
    "#Change BodyFat to be named target, to make code reuse easier\n",
    "df_.rename(columns={\"BodyFat\":\"target\"}, inplace=True)\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees Please\n",
    "\n",
    "Trees are one of the common machine learning algorithms, and have several advantages:\n",
    "<ul>\n",
    "<li> They show how decisions are made. A human can follow a decision tree and see exactly what happens on the way to a prediction. \n",
    "<li> They (can be) quite fast. \n",
    "<li> They are more felxible than other algorithms in dealing with categorical data, as a tree can natively handle a categorical value. <b>Note:</b> this is true for a tree in theory, in practice, specific implementations may still require numerical inputs. \n",
    "<li> They work well in ensables, in particular many of the best non-neural network algorithms are based on boosing ensables of trees. We'll look at these later. \n",
    "<li> They are resistant to outliers.\n",
    "<li> Trees illustrate some of the internal processes of machine learning, as we can follow the actions of the algorithm and see how it makes decisions.\n",
    "</ul>\n",
    "\n",
    "There are also some downsides:\n",
    "<ul>\n",
    "<li> Regression trees are limited, and they can't extrapolate. \n",
    "<li> Forest ensables generally get better results, but don't maintain the same level of understandibility. \n",
    "<li> Overfitting is a concern, and we need to be careful to limit the growth of the tree.\n",
    "</ul>\n",
    "\n",
    "In practice, trees form the foundation model for several of the best and most recently developed non-neural network algorithms, like xgboost. We'll look at this later when we examine boosted ensemble models."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
