{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gGsMHgvKZVRO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_lu_r1ndZVRR"
      },
      "outputs": [],
      "source": [
        "# Helper to plot loss\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52niBYdKZVRS"
      },
      "source": [
        "# Keras, TensorFlow, and Neural Network Regression\n",
        "\n",
        "As we have seen, neural networks aren't quite as complex as they appear at first, however we still generally don't want to have to build them from scratch very often. The libraries that we will primarily use for creating neural network models are Tensorflow and Keras. \n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "Tensorflow, developed by Google, is one of the most popular libraries for neural networks. \n",
        "\n",
        "### Keras\n",
        "\n",
        "Keras is another package that provides an an API offering an easier to use interface to Tensorflow, allowing us to use it with code that is higher level, avoiding much of the linear math that can make Tensorflow frustrating. Since its introduction Keras has been wrapped in with Tensorflow and the two are normally now blended together as far as we are concerned. \n",
        "\n",
        "### Other Alternatives\n",
        "\n",
        "Keras and Tensorflow are not the only libraries of neural networks, the primary competitor to Tensorflow is PyTorch, which was developed by Facebook. PyTorch does pretty much the same thing as Tensorflow, we won't look at it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JrKP3wO_ZVRT",
        "outputId": "1ac1822c-99a5-4e98-ea4d-0158aed55c65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21608</th>\n",
              "      <td>360000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1530</td>\n",
              "      <td>1131</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1530</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98103</td>\n",
              "      <td>47.6993</td>\n",
              "      <td>-122.346</td>\n",
              "      <td>1530</td>\n",
              "      <td>1509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21609</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2310</td>\n",
              "      <td>5813</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2310</td>\n",
              "      <td>0</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>98146</td>\n",
              "      <td>47.5107</td>\n",
              "      <td>-122.362</td>\n",
              "      <td>1830</td>\n",
              "      <td>7200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21610</th>\n",
              "      <td>402101.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1350</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5944</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21611</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1600</td>\n",
              "      <td>2388</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1600</td>\n",
              "      <td>0</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>98027</td>\n",
              "      <td>47.5345</td>\n",
              "      <td>-122.069</td>\n",
              "      <td>1410</td>\n",
              "      <td>1287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21612</th>\n",
              "      <td>325000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1076</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5941</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>1357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
              "21608  360000.0         3       2.50         1530      1131     3.0   \n",
              "21609  400000.0         4       2.50         2310      5813     2.0   \n",
              "21610  402101.0         2       0.75         1020      1350     2.0   \n",
              "21611  400000.0         3       2.50         1600      2388     2.0   \n",
              "21612  325000.0         2       0.75         1020      1076     2.0   \n",
              "\n",
              "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
              "21608           0     0          3      8        1530              0   \n",
              "21609           0     0          3      8        2310              0   \n",
              "21610           0     0          3      7        1020              0   \n",
              "21611           0     0          3      8        1600              0   \n",
              "21612           0     0          3      7        1020              0   \n",
              "\n",
              "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
              "21608      2009             0    98103  47.6993 -122.346           1530   \n",
              "21609      2014             0    98146  47.5107 -122.362           1830   \n",
              "21610      2009             0    98144  47.5944 -122.299           1020   \n",
              "21611      2004             0    98027  47.5345 -122.069           1410   \n",
              "21612      2008             0    98144  47.5941 -122.299           1020   \n",
              "\n",
              "       sqft_lot15  \n",
              "21608        1509  \n",
              "21609        7200  \n",
              "21610        2007  \n",
              "21611        1287  \n",
              "21612        1357  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"data/house_data.csv\")\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uLE617U8ZVRV",
        "outputId": "08d0318a-9c52-4e72-b585-ade094db2591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21613 entries, 0 to 21612\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   price          21613 non-null  float64\n",
            " 1   bedrooms       21613 non-null  int64  \n",
            " 2   bathrooms      21613 non-null  float64\n",
            " 3   sqft_living    21613 non-null  int64  \n",
            " 4   sqft_lot       21613 non-null  int64  \n",
            " 5   floors         21613 non-null  float64\n",
            " 6   waterfront     21613 non-null  int64  \n",
            " 7   view           21613 non-null  int64  \n",
            " 8   condition      21613 non-null  int64  \n",
            " 9   grade          21613 non-null  int64  \n",
            " 10  sqft_above     21613 non-null  int64  \n",
            " 11  sqft_basement  21613 non-null  int64  \n",
            " 12  yr_built       21613 non-null  int64  \n",
            " 13  yr_renovated   21613 non-null  int64  \n",
            " 14  zipcode        21613 non-null  int64  \n",
            " 15  lat            21613 non-null  float64\n",
            " 16  long           21613 non-null  float64\n",
            " 17  sqft_living15  21613 non-null  int64  \n",
            " 18  sqft_lot15     21613 non-null  int64  \n",
            "dtypes: float64(5), int64(14)\n",
            "memory usage: 3.1 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ThkDxsnGZVRV",
        "outputId": "fef44a1a-6690-4bd7-b618-dd6261194d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21613, 18) (21613, 1)\n"
          ]
        }
      ],
      "source": [
        "y = np.array(df[\"price\"]).reshape(-1,1)\n",
        "X = np.array(df.drop(columns={\"price\"}))\n",
        "print(X.shape, y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DY9JoSCXZVRW"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "Creating a NN model is slightly different from the normal process that we are used to in sklearn. We need to do a little more work to set it up. \n",
        "\n",
        "### Create Model and Add Layers\n",
        "\n",
        "First we need to make a NN model, it comes \"empty\". We will use a sequential model, which is the most simple type but is less configurable (which we don't care about much right now). The limitation of sequential models is that they can only take in one tensor and only output one tensor. The other options here are \"functional\", which allows for the structure of the model to be configured, and \"model subclassing\", which allows you to build almost everything from scratch. These other types are more complex and more flexible, but actually aren't really needed for most applications, and we won't use them. These more complex models are commonly used for scenarios where the data is complex, such as a self driving car - a model needs to output steering as well as velocity. Also for more complex problems such as language or image processing, this flexibility allows for models to be created that are better able to extract the information from the data.\n",
        "\n",
        "#### Layers\n",
        "\n",
        "Next we need to add some layers. We will start simple with only two \"thinking\" layers, and one to do some processing. We can think of the layers roughly like steps of the sklearn pipeline, with data entering at the first layer and predictions flowing out of the final layer. \n",
        "\n",
        "In addition to \"normal\" neural network layers, there are many other types that can do all kinds of other stuff. One example we will use here is the normalization one at the front. This layer does exactly what you'd expect - it normalizes our data so the rest of the network can use it. The normalize layer will also automatically handle the 2D nature of the data that we are used to, so we don't need to worry about that aspect here. Other layers can do everything from regularization to image processing, they are also commonly inhierited for developers to create custom layers targeting specific tasks. We'll use a few of the other ones as we move through things. \n",
        "\n",
        "#### Dense Layers\n",
        "\n",
        "We'll use dense layers here. When adding the layer we need to specify a couple of things. One is the input dimensions - we need to tell the network what the shape of the incomming data is. \n",
        "\n",
        "The other argument is the units, which represents the output dimension. When using these Keras dense layers we don't need to specify each layer's input/output like we did when we made it by hand. We specify both, using units and input_dim, for the first layer that takes in the input; for subsequent layers we can just specify the output and Keras will automatically figure the rest out. \n",
        "\n",
        "Note that there is also an input layer that can be added, we can avoid the need for it by using the input_dim or input_shape as shown below. The two examples there do the same thing, since the input is flat - 18 features. If we are dealing with inputs that do not start out as flat, such as in an image, use the input_shape since you can specify all dimensions; we will see an example of this next time with some images. \n",
        "\n",
        "#### Activation Function\n",
        "\n",
        "For each of our layers we need to define which activation function to use. For now we will use the ReLU function, which is probably the most popular. We'll look at other ones later on. Note that we've left the activation function off of the final layer - we are doing regression so we want that raw value. This is the same idea as with linear regression - we don't want the prediction to be transformed through something like the sigmoid, we just want the number.\n",
        "\n",
        "#### Summary\n",
        "\n",
        "After we've constructed the model, the summary command give us, well, a summary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIquYAGtZVRX"
      },
      "source": [
        "We are dealing with a bunch of numerical inputs here, so we can add a normalization layer at the front end. Like with sklearn, we want to fit the normalization to the training data only. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zWPhKP3wZVRX",
        "outputId": "4c6bc4d8-4a5f-4da7-edd8-7792b597b931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-21 13:13:53.223061: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization (Normalizatio  (None, 18)               37        \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 18)                342       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 19        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 398\n",
            "Trainable params: 361\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
        "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6AH5AydZVRY"
      },
      "source": [
        "#### Compile Model\n",
        "\n",
        "Once a model is created we need to compile it. The complie step basically builds the layers we specified above and the loss and optimization parameters below together into a usable model object. When compiling the model we are providing it with the things it needs to calculate error:\n",
        "\n",
        "<ul>\n",
        "<li> Loss - we can provide a loss function that we'd like to use. \n",
        "<li> Optimizer - the optimizer is the algorithm that the model will use to perform the gradient descent to find the lowest error. Adam is a very common choice.\n",
        "<li> Learning rate - the learning rate is provided as a parameter of the optimizer. \n",
        "</ul>\n",
        "\n",
        "##### Optimizing Adam\n",
        "\n",
        "The optimizer is the algorithm used to perform the gradient descent and minimize error. For the most part this isn't something we need to be concerned about. The choice of optimizer is much more important if dealing with very large datasets because different optimizers have different levels of efficiency. For our purposes, we can use Adam and be pretty happy. Adam stands for Adaptive Moment Estimation which means basically that it will adjust itself depending on current gradients. It tends to be efficient both in time and memory, so it is very commonly used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fEya_QckZVRY"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n9TUG422ZVRZ"
      },
      "source": [
        "### Fit the Model\n",
        "\n",
        "The fit command does the same thing that we are used to, it trains the model, however there are some differences. The main difference is that batch_size is almost always set in neural networks, while the sklearn models just take all the data at once. \n",
        "\n",
        "What's a batch? Batches are just subsets of the data, so if the batch size is 100 the algorithm will grab 100 rows at a time before making an update to the weights and bias. There are a few reasons this exists:\n",
        "\n",
        "<ul>\n",
        "<li> Memory constraints - it is common with neural networks to deal with datasets that are extremely large. Processing data that can't fit entirely in RAM is very slow (the computer must swap data from the hard drive to RAM as it is needed) compared to data that is in RAM. Cutting the batch size can avoid this issue. \n",
        "<li> Speed - the math involved in the back propagation can sometimes be very computationally intensive. \n",
        "<li> Accuracy - batch size can have an impact on accuracy, though that impact is not very predictable. For the most part finding an optimal batch size will need to be grid-searched. \n",
        "</ul>\n",
        "\n",
        "The fit command also has the epoch paramater, which instructs on how many times to work through ALL of the data. We want to ensure we have enough epochs to find the optimal solution. Training rounds, or epochs, are one of the key tuning factors when using neural networks. Similar to large trees, large neural networks are capable of learning the training data very well, and carry the same risk of overfitting. With neural networks, a common approach to tuning is to allow the model to train, and cut it off when we start overfitting, or when the testing accuracy starts to decrease.\n",
        "\n",
        "#### Plot the Loss\n",
        "\n",
        "One very common visualization we see with neural networks is a plot of both training and validation loss vs number of epochs. Generally we'll see the training loss drop - first sharply as the model initially fits itself, then more slowly as it becomes more fitted. The validation loss will usually somewhat mirror the training loss, except it will often reach a minimum at some point before again increasing. This minimum point is our best model, when the validation loss starts increasing again, that is a sign that the model has become overfitted - customized to the training data, but less and less generalizable to new data. \n",
        "\n",
        "Set the verbosity to 1 in the fit to get a full list of the loss for each epoch to pinpoint the exact \"ideal\" number of epochs. We'll look more at this in a minute. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bDUXMj_pZVRZ",
        "outputId": "6facd69a-6643-4845-da6f-94199649ac5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "130/130 [==============================] - 1s 2ms/step - loss: 537760.5625 - val_loss: 550023.8750\n",
            "Epoch 2/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 537357.8750 - val_loss: 549338.8125\n",
            "Epoch 3/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 536394.5625 - val_loss: 548096.0000\n",
            "Epoch 4/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 534909.0625 - val_loss: 546350.0625\n",
            "Epoch 5/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 532942.6875 - val_loss: 544134.6250\n",
            "Epoch 6/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 530518.8125 - val_loss: 541476.6875\n",
            "Epoch 7/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 527661.6250 - val_loss: 538388.8125\n",
            "Epoch 8/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 524394.0000 - val_loss: 534895.1250\n",
            "Epoch 9/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 520733.6250 - val_loss: 531025.1875\n",
            "Epoch 10/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 516694.7500 - val_loss: 526769.6875\n",
            "Epoch 11/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 512290.3750 - val_loss: 522157.3125\n",
            "Epoch 12/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 507537.5312 - val_loss: 517199.5625\n",
            "Epoch 13/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 502444.5000 - val_loss: 511912.6562\n",
            "Epoch 14/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 497026.2812 - val_loss: 506296.4688\n",
            "Epoch 15/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 491295.4062 - val_loss: 500368.0938\n",
            "Epoch 16/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 485261.5312 - val_loss: 494141.1875\n",
            "Epoch 17/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 478943.2188 - val_loss: 487645.2188\n",
            "Epoch 18/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 472357.0625 - val_loss: 480860.0312\n",
            "Epoch 19/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 465520.2188 - val_loss: 473833.4062\n",
            "Epoch 20/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 458428.3750 - val_loss: 466565.2188\n",
            "Epoch 21/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 451105.9062 - val_loss: 459078.5625\n",
            "Epoch 22/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 443561.0000 - val_loss: 451376.1250\n",
            "Epoch 23/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 435812.5000 - val_loss: 443464.7500\n",
            "Epoch 24/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 427869.3750 - val_loss: 435388.4062\n",
            "Epoch 25/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 419809.3750 - val_loss: 427191.7812\n",
            "Epoch 26/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 411634.1875 - val_loss: 418870.1562\n",
            "Epoch 27/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 403363.8438 - val_loss: 410543.0625\n",
            "Epoch 28/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 395025.1875 - val_loss: 402131.6875\n",
            "Epoch 29/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 386630.1250 - val_loss: 393749.2812\n",
            "Epoch 30/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 378254.7188 - val_loss: 385374.5938\n",
            "Epoch 31/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 369955.2500 - val_loss: 377132.3750\n",
            "Epoch 32/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 361768.9688 - val_loss: 368936.3750\n",
            "Epoch 33/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 353721.3750 - val_loss: 360822.7500\n",
            "Epoch 34/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 345789.1250 - val_loss: 352874.8125\n",
            "Epoch 35/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 338028.5000 - val_loss: 345075.3750\n",
            "Epoch 36/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 330468.4062 - val_loss: 337560.6250\n",
            "Epoch 37/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 323182.6250 - val_loss: 330319.5312\n",
            "Epoch 38/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 316266.5625 - val_loss: 323404.3750\n",
            "Epoch 39/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 309695.2500 - val_loss: 316864.7188\n",
            "Epoch 40/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 303493.5938 - val_loss: 310687.8125\n",
            "Epoch 41/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 297672.7188 - val_loss: 304864.5312\n",
            "Epoch 42/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 292213.7188 - val_loss: 299410.6562\n",
            "Epoch 43/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 287077.3438 - val_loss: 294218.4062\n",
            "Epoch 44/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 282253.8125 - val_loss: 289292.5625\n",
            "Epoch 45/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 277689.9375 - val_loss: 284560.4062\n",
            "Epoch 46/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 273366.3125 - val_loss: 280077.7500\n",
            "Epoch 47/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 269234.6562 - val_loss: 275745.9062\n",
            "Epoch 48/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 265301.6875 - val_loss: 271571.9062\n",
            "Epoch 49/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 261560.6719 - val_loss: 267600.2812\n",
            "Epoch 50/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 258034.4375 - val_loss: 263938.0312\n",
            "Epoch 51/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 254769.2031 - val_loss: 260578.1562\n",
            "Epoch 52/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 251792.6875 - val_loss: 257425.0781\n",
            "Epoch 53/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 249034.2812 - val_loss: 254549.9531\n",
            "Epoch 54/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 246457.6562 - val_loss: 251952.1250\n",
            "Epoch 55/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 244042.9844 - val_loss: 249480.3281\n",
            "Epoch 56/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 241784.6094 - val_loss: 247199.2500\n",
            "Epoch 57/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 239668.8281 - val_loss: 245030.1250\n",
            "Epoch 58/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 237651.1562 - val_loss: 242909.7812\n",
            "Epoch 59/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 235709.8281 - val_loss: 240846.3750\n",
            "Epoch 60/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 233834.7344 - val_loss: 238814.8594\n",
            "Epoch 61/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 232022.9844 - val_loss: 236854.5625\n",
            "Epoch 62/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 230243.3594 - val_loss: 234888.6719\n",
            "Epoch 63/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 228473.6875 - val_loss: 232940.9062\n",
            "Epoch 64/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 226727.3594 - val_loss: 231044.3594\n",
            "Epoch 65/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 225001.5469 - val_loss: 229137.5156\n",
            "Epoch 66/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 223294.7031 - val_loss: 227215.5625\n",
            "Epoch 67/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 221595.9062 - val_loss: 225309.0000\n",
            "Epoch 68/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 219896.7812 - val_loss: 223424.5000\n",
            "Epoch 69/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 218217.7812 - val_loss: 221527.8438\n",
            "Epoch 70/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 216540.5000 - val_loss: 219651.8281\n",
            "Epoch 71/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 214873.1562 - val_loss: 217782.3281\n",
            "Epoch 72/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 213216.5469 - val_loss: 215925.3438\n",
            "Epoch 73/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 211566.0625 - val_loss: 214098.5156\n",
            "Epoch 74/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 209930.7188 - val_loss: 212277.5625\n",
            "Epoch 75/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 208306.7656 - val_loss: 210486.9844\n",
            "Epoch 76/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 206688.0938 - val_loss: 208716.1250\n",
            "Epoch 77/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 205085.2188 - val_loss: 206959.3906\n",
            "Epoch 78/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 203498.9375 - val_loss: 205235.0625\n",
            "Epoch 79/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 201928.8438 - val_loss: 203523.9688\n",
            "Epoch 80/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 200367.3125 - val_loss: 201808.3906\n",
            "Epoch 81/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 198808.4688 - val_loss: 200162.3281\n",
            "Epoch 82/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 197272.2188 - val_loss: 198514.8750\n",
            "Epoch 83/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 195754.7188 - val_loss: 196920.1250\n",
            "Epoch 84/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 194265.6406 - val_loss: 195365.0156\n",
            "Epoch 85/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 192804.7969 - val_loss: 193828.7812\n",
            "Epoch 86/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 191380.1250 - val_loss: 192328.7656\n",
            "Epoch 87/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 189998.3594 - val_loss: 190877.2344\n",
            "Epoch 88/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 188660.6094 - val_loss: 189458.3125\n",
            "Epoch 89/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 187345.8594 - val_loss: 188073.1719\n",
            "Epoch 90/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 186042.3438 - val_loss: 186703.5156\n",
            "Epoch 91/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 184756.9219 - val_loss: 185358.9688\n",
            "Epoch 92/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 183486.5156 - val_loss: 184034.3750\n",
            "Epoch 93/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 182239.8594 - val_loss: 182736.0625\n",
            "Epoch 94/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 181029.7344 - val_loss: 181476.7344\n",
            "Epoch 95/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 179844.5156 - val_loss: 180228.1875\n",
            "Epoch 96/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 178666.5781 - val_loss: 179017.0000\n",
            "Epoch 97/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 177512.1250 - val_loss: 177831.4375\n",
            "Epoch 98/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 176399.2344 - val_loss: 176684.8750\n",
            "Epoch 99/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 175324.9219 - val_loss: 175553.6250\n",
            "Epoch 100/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 174271.4688 - val_loss: 174442.9219\n",
            "Epoch 101/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 173237.7500 - val_loss: 173342.4375\n",
            "Epoch 102/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 172208.3125 - val_loss: 172260.1406\n",
            "Epoch 103/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 171190.4844 - val_loss: 171205.7656\n",
            "Epoch 104/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 170188.0156 - val_loss: 170161.1719\n",
            "Epoch 105/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 169207.9688 - val_loss: 169139.4062\n",
            "Epoch 106/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 168228.4844 - val_loss: 168125.4062\n",
            "Epoch 107/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 167271.0625 - val_loss: 167150.4688\n",
            "Epoch 108/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 166326.6250 - val_loss: 166193.3750\n",
            "Epoch 109/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 165397.0000 - val_loss: 165241.8281\n",
            "Epoch 110/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 164474.9531 - val_loss: 164320.3281\n",
            "Epoch 111/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 163565.6562 - val_loss: 163407.7969\n",
            "Epoch 112/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 162673.0000 - val_loss: 162509.3281\n",
            "Epoch 113/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 161784.3125 - val_loss: 161609.3750\n",
            "Epoch 114/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 160904.9062 - val_loss: 160712.4688\n",
            "Epoch 115/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 160030.8594 - val_loss: 159821.4531\n",
            "Epoch 116/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 159163.0938 - val_loss: 158938.5781\n",
            "Epoch 117/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 158292.9844 - val_loss: 158078.5469\n",
            "Epoch 118/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 157424.2188 - val_loss: 157180.0469\n",
            "Epoch 119/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 156550.8594 - val_loss: 156316.0000\n",
            "Epoch 120/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 155683.7344 - val_loss: 155452.4844\n",
            "Epoch 121/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 154809.5469 - val_loss: 154573.4062\n",
            "Epoch 122/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 153934.0156 - val_loss: 153718.4219\n",
            "Epoch 123/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 153062.5000 - val_loss: 152877.0312\n",
            "Epoch 124/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 152196.7031 - val_loss: 152041.6250\n",
            "Epoch 125/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 151346.7969 - val_loss: 151214.7188\n",
            "Epoch 126/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 150508.0781 - val_loss: 150404.3281\n",
            "Epoch 127/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 149677.3281 - val_loss: 149593.5000\n",
            "Epoch 128/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 148858.7969 - val_loss: 148795.6250\n",
            "Epoch 129/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 148042.2656 - val_loss: 148004.9375\n",
            "Epoch 130/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 147235.3125 - val_loss: 147233.5000\n",
            "Epoch 131/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 146440.2188 - val_loss: 146456.7969\n",
            "Epoch 132/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 145654.5781 - val_loss: 145692.8594\n",
            "Epoch 133/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 144871.6562 - val_loss: 144929.2656\n",
            "Epoch 134/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 144095.1406 - val_loss: 144185.7969\n",
            "Epoch 135/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 143327.3750 - val_loss: 143445.0781\n",
            "Epoch 136/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 142567.8281 - val_loss: 142715.2188\n",
            "Epoch 137/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 141821.5469 - val_loss: 141998.7188\n",
            "Epoch 138/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 141101.0000 - val_loss: 141290.4688\n",
            "Epoch 139/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 140394.3125 - val_loss: 140608.0781\n",
            "Epoch 140/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 139691.7656 - val_loss: 139913.8906\n",
            "Epoch 141/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 138999.4688 - val_loss: 139255.1250\n",
            "Epoch 142/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 138321.6719 - val_loss: 138613.7500\n",
            "Epoch 143/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 137660.1875 - val_loss: 137987.7969\n",
            "Epoch 144/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 137012.2031 - val_loss: 137378.6562\n",
            "Epoch 145/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 136378.4219 - val_loss: 136786.2344\n",
            "Epoch 146/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 135758.0938 - val_loss: 136187.1406\n",
            "Epoch 147/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 135138.6875 - val_loss: 135632.1406\n",
            "Epoch 148/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 134538.6875 - val_loss: 135081.5000\n",
            "Epoch 149/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 133946.3281 - val_loss: 134537.2031\n",
            "Epoch 150/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 133368.9844 - val_loss: 134011.6562\n",
            "Epoch 151/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 132808.5625 - val_loss: 133515.9688\n",
            "Epoch 152/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 132264.1094 - val_loss: 133016.5938\n",
            "Epoch 153/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 131736.0938 - val_loss: 132525.5156\n",
            "Epoch 154/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 131219.8438 - val_loss: 132061.4219\n",
            "Epoch 155/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 130724.0859 - val_loss: 131616.8906\n",
            "Epoch 156/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 130230.1953 - val_loss: 131181.2188\n",
            "Epoch 157/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 129752.7891 - val_loss: 130768.4922\n",
            "Epoch 158/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 129276.6719 - val_loss: 130341.9062\n",
            "Epoch 159/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 128815.0938 - val_loss: 129939.9766\n",
            "Epoch 160/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 128368.7969 - val_loss: 129555.5938\n",
            "Epoch 161/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127928.8359 - val_loss: 129165.6328\n",
            "Epoch 162/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127492.5938 - val_loss: 128795.4297\n",
            "Epoch 163/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127069.7891 - val_loss: 128419.1094\n",
            "Epoch 164/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 126650.0703 - val_loss: 128071.6719\n",
            "Epoch 165/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 126243.4688 - val_loss: 127723.1719\n",
            "Epoch 166/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 125844.9922 - val_loss: 127387.4609\n",
            "Epoch 167/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 125455.8359 - val_loss: 127046.4062\n",
            "Epoch 168/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 125078.3750 - val_loss: 126701.6016\n",
            "Epoch 169/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124706.4609 - val_loss: 126368.3672\n",
            "Epoch 170/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124331.1875 - val_loss: 126038.2656\n",
            "Epoch 171/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 123967.3359 - val_loss: 125715.9688\n",
            "Epoch 172/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 123607.3828 - val_loss: 125400.6641\n",
            "Epoch 173/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 123259.9219 - val_loss: 125089.9062\n",
            "Epoch 174/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 122921.0469 - val_loss: 124800.6016\n",
            "Epoch 175/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 122580.5000 - val_loss: 124494.4766\n",
            "Epoch 176/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 122250.5312 - val_loss: 124206.0859\n",
            "Epoch 177/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 121926.0078 - val_loss: 123938.4297\n",
            "Epoch 178/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 121609.0391 - val_loss: 123653.6953\n",
            "Epoch 179/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 121289.7344 - val_loss: 123386.4531\n",
            "Epoch 180/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 120984.0000 - val_loss: 123126.8516\n",
            "Epoch 181/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 120680.6484 - val_loss: 122860.5469\n",
            "Epoch 182/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 120386.6328 - val_loss: 122601.5859\n",
            "Epoch 183/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 120105.1172 - val_loss: 122357.5000\n",
            "Epoch 184/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 119829.6016 - val_loss: 122113.8984\n",
            "Epoch 185/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 119558.0391 - val_loss: 121882.6016\n",
            "Epoch 186/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 119297.3047 - val_loss: 121659.0625\n",
            "Epoch 187/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 119039.1328 - val_loss: 121434.5391\n",
            "Epoch 188/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 118786.3125 - val_loss: 121213.1406\n",
            "Epoch 189/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 118537.8984 - val_loss: 121007.6250\n",
            "Epoch 190/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118295.5234 - val_loss: 120800.3359\n",
            "Epoch 191/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 118056.9062 - val_loss: 120589.2500\n",
            "Epoch 192/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 117821.8125 - val_loss: 120396.4766\n",
            "Epoch 193/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 117593.3750 - val_loss: 120210.0547\n",
            "Epoch 194/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 117373.5391 - val_loss: 120026.1641\n",
            "Epoch 195/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 117155.2969 - val_loss: 119834.3047\n",
            "Epoch 196/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 116940.2812 - val_loss: 119666.1016\n",
            "Epoch 197/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 116735.8125 - val_loss: 119495.2578\n",
            "Epoch 198/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116535.2969 - val_loss: 119349.0859\n",
            "Epoch 199/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 116337.5469 - val_loss: 119190.4688\n",
            "Epoch 200/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 116140.6484 - val_loss: 119038.7891\n",
            "Epoch 201/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115950.7812 - val_loss: 118898.2109\n",
            "Epoch 202/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115769.3750 - val_loss: 118758.8906\n",
            "Epoch 203/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115596.4688 - val_loss: 118622.8750\n",
            "Epoch 204/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 115429.4922 - val_loss: 118488.3047\n",
            "Epoch 205/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115262.1719 - val_loss: 118362.5391\n",
            "Epoch 206/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 115110.6719 - val_loss: 118241.0234\n",
            "Epoch 207/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 114960.7812 - val_loss: 118117.3828\n",
            "Epoch 208/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114815.6016 - val_loss: 118015.0859\n",
            "Epoch 209/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 114677.6562 - val_loss: 117902.3203\n",
            "Epoch 210/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 114546.1328 - val_loss: 117793.6172\n",
            "Epoch 211/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 114414.7344 - val_loss: 117685.3828\n",
            "Epoch 212/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114291.3672 - val_loss: 117592.9844\n",
            "Epoch 213/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114173.8906 - val_loss: 117504.9297\n",
            "Epoch 214/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114060.9922 - val_loss: 117412.7266\n",
            "Epoch 215/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 113950.4688 - val_loss: 117326.7656\n",
            "Epoch 216/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 113847.6719 - val_loss: 117239.7109\n",
            "Epoch 217/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 113735.1641 - val_loss: 117159.8438\n",
            "Epoch 218/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113643.1250 - val_loss: 117068.7891\n",
            "Epoch 219/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 113538.8984 - val_loss: 116989.0703\n",
            "Epoch 220/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 113443.4453 - val_loss: 116908.5078\n",
            "Epoch 221/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113357.5312 - val_loss: 116831.9766\n",
            "Epoch 222/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 113269.3984 - val_loss: 116762.4609\n",
            "Epoch 223/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113184.2500 - val_loss: 116690.9688\n",
            "Epoch 224/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113098.9219 - val_loss: 116611.5469\n",
            "Epoch 225/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113016.0703 - val_loss: 116543.5078\n",
            "Epoch 226/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112938.6016 - val_loss: 116490.7031\n",
            "Epoch 227/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112859.5938 - val_loss: 116436.5703\n",
            "Epoch 228/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112789.7734 - val_loss: 116373.0234\n",
            "Epoch 229/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112715.5547 - val_loss: 116299.4219\n",
            "Epoch 230/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112647.5156 - val_loss: 116237.6016\n",
            "Epoch 231/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112582.1016 - val_loss: 116187.6875\n",
            "Epoch 232/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112511.1016 - val_loss: 116150.1094\n",
            "Epoch 233/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112445.5000 - val_loss: 116087.2969\n",
            "Epoch 234/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112383.2578 - val_loss: 116037.6250\n",
            "Epoch 235/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112322.7188 - val_loss: 115985.3516\n",
            "Epoch 236/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112259.7812 - val_loss: 115933.3125\n",
            "Epoch 237/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112202.0859 - val_loss: 115889.0938\n",
            "Epoch 238/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112141.1484 - val_loss: 115842.0625\n",
            "Epoch 239/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112085.4922 - val_loss: 115792.3438\n",
            "Epoch 240/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112031.1172 - val_loss: 115754.8281\n",
            "Epoch 241/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111972.1172 - val_loss: 115704.9219\n",
            "Epoch 242/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111922.2422 - val_loss: 115661.7422\n",
            "Epoch 243/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111865.7031 - val_loss: 115606.7344\n",
            "Epoch 244/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111815.5625 - val_loss: 115580.2578\n",
            "Epoch 245/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 111764.1484 - val_loss: 115533.0000\n",
            "Epoch 246/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111708.6719 - val_loss: 115488.2734\n",
            "Epoch 247/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111661.5547 - val_loss: 115459.1172\n",
            "Epoch 248/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111617.5703 - val_loss: 115424.9375\n",
            "Epoch 249/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 111570.6094 - val_loss: 115376.2891\n",
            "Epoch 250/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111522.0938 - val_loss: 115334.9609\n",
            "Epoch 251/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111473.4609 - val_loss: 115298.6875\n",
            "Epoch 252/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111432.3672 - val_loss: 115261.3516\n",
            "Epoch 253/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111389.2422 - val_loss: 115245.1797\n",
            "Epoch 254/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 111345.0547 - val_loss: 115203.7188\n",
            "Epoch 255/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 111304.5859 - val_loss: 115177.9922\n",
            "Epoch 256/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111265.7422 - val_loss: 115124.3984\n",
            "Epoch 257/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111224.0156 - val_loss: 115101.0391\n",
            "Epoch 258/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111184.7969 - val_loss: 115064.1016\n",
            "Epoch 259/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111139.9688 - val_loss: 115020.7891\n",
            "Epoch 260/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111098.5234 - val_loss: 114983.9688\n",
            "Epoch 261/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111060.2969 - val_loss: 114947.6797\n",
            "Epoch 262/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111022.7422 - val_loss: 114899.4219\n",
            "Epoch 263/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110979.7812 - val_loss: 114875.6953\n",
            "Epoch 264/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110946.1328 - val_loss: 114865.2734\n",
            "Epoch 265/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110903.8281 - val_loss: 114807.4531\n",
            "Epoch 266/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110865.5078 - val_loss: 114771.2891\n",
            "Epoch 267/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110827.4219 - val_loss: 114753.7344\n",
            "Epoch 268/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110786.6875 - val_loss: 114711.0938\n",
            "Epoch 269/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 110753.2266 - val_loss: 114673.9375\n",
            "Epoch 270/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 110712.2891 - val_loss: 114636.8125\n",
            "Epoch 271/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110675.0156 - val_loss: 114609.9141\n",
            "Epoch 272/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110644.3750 - val_loss: 114577.8438\n",
            "Epoch 273/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110605.1094 - val_loss: 114546.3359\n",
            "Epoch 274/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110571.0156 - val_loss: 114516.2578\n",
            "Epoch 275/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 110534.1250 - val_loss: 114486.1172\n",
            "Epoch 276/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110499.9141 - val_loss: 114450.3750\n",
            "Epoch 277/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110462.4375 - val_loss: 114428.1484\n",
            "Epoch 278/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110429.2344 - val_loss: 114402.9531\n",
            "Epoch 279/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110391.8125 - val_loss: 114366.3828\n",
            "Epoch 280/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110357.6875 - val_loss: 114331.5156\n",
            "Epoch 281/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110323.1875 - val_loss: 114310.0000\n",
            "Epoch 282/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 110292.9219 - val_loss: 114276.8281\n",
            "Epoch 283/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110253.4766 - val_loss: 114242.3594\n",
            "Epoch 284/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110222.5625 - val_loss: 114209.4219\n",
            "Epoch 285/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110188.1328 - val_loss: 114188.6250\n",
            "Epoch 286/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110155.6250 - val_loss: 114171.4766\n",
            "Epoch 287/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110122.4453 - val_loss: 114131.1094\n",
            "Epoch 288/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110089.8203 - val_loss: 114110.0391\n",
            "Epoch 289/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 110060.1953 - val_loss: 114095.0391\n",
            "Epoch 290/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110024.9766 - val_loss: 114045.5547\n",
            "Epoch 291/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 109988.8984 - val_loss: 114023.1875\n",
            "Epoch 292/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109956.9141 - val_loss: 113995.5469\n",
            "Epoch 293/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109924.7734 - val_loss: 113964.8125\n",
            "Epoch 294/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109892.0547 - val_loss: 113936.1562\n",
            "Epoch 295/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 109862.6719 - val_loss: 113911.1484\n",
            "Epoch 296/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109832.3047 - val_loss: 113869.5391\n",
            "Epoch 297/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109798.0156 - val_loss: 113841.2969\n",
            "Epoch 298/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109770.0703 - val_loss: 113817.1719\n",
            "Epoch 299/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109734.3516 - val_loss: 113780.2656\n",
            "Epoch 300/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109701.6328 - val_loss: 113770.2344\n",
            "Epoch 301/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109671.5156 - val_loss: 113737.8281\n",
            "Epoch 302/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109641.8750 - val_loss: 113698.0938\n",
            "Epoch 303/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109612.1562 - val_loss: 113674.6797\n",
            "Epoch 304/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109576.6953 - val_loss: 113654.6562\n",
            "Epoch 305/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109546.5312 - val_loss: 113623.8828\n",
            "Epoch 306/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109516.6172 - val_loss: 113596.3203\n",
            "Epoch 307/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109482.5547 - val_loss: 113562.4141\n",
            "Epoch 308/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 109452.1250 - val_loss: 113543.1172\n",
            "Epoch 309/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109421.7500 - val_loss: 113501.0469\n",
            "Epoch 310/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109389.6641 - val_loss: 113478.8516\n",
            "Epoch 311/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109365.1094 - val_loss: 113452.8125\n",
            "Epoch 312/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109330.4766 - val_loss: 113429.8047\n",
            "Epoch 313/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109301.4844 - val_loss: 113408.3281\n",
            "Epoch 314/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109270.8281 - val_loss: 113368.0234\n",
            "Epoch 315/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 109241.4688 - val_loss: 113343.9609\n",
            "Epoch 316/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 109213.1562 - val_loss: 113324.8906\n",
            "Epoch 317/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109183.2031 - val_loss: 113287.1094\n",
            "Epoch 318/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109152.0234 - val_loss: 113272.6641\n",
            "Epoch 319/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109123.2656 - val_loss: 113241.8984\n",
            "Epoch 320/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109091.1250 - val_loss: 113224.1250\n",
            "Epoch 321/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109061.3906 - val_loss: 113200.5156\n",
            "Epoch 322/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109030.8125 - val_loss: 113190.1172\n",
            "Epoch 323/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109000.4141 - val_loss: 113144.2812\n",
            "Epoch 324/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108969.0938 - val_loss: 113135.7422\n",
            "Epoch 325/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108942.1719 - val_loss: 113104.7031\n",
            "Epoch 326/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108912.5938 - val_loss: 113072.2109\n",
            "Epoch 327/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108882.9375 - val_loss: 113052.6953\n",
            "Epoch 328/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 108856.7500 - val_loss: 113039.1797\n",
            "Epoch 329/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108825.2500 - val_loss: 113007.2734\n",
            "Epoch 330/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108794.8203 - val_loss: 112978.9609\n",
            "Epoch 331/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108769.5312 - val_loss: 112956.0156\n",
            "Epoch 332/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108739.1172 - val_loss: 112932.0078\n",
            "Epoch 333/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 108713.8281 - val_loss: 112903.5703\n",
            "Epoch 334/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 108686.0391 - val_loss: 112890.2969\n",
            "Epoch 335/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108660.6953 - val_loss: 112863.7266\n",
            "Epoch 336/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 108626.5234 - val_loss: 112836.4609\n",
            "Epoch 337/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108602.7891 - val_loss: 112809.8125\n",
            "Epoch 338/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108571.1328 - val_loss: 112791.2812\n",
            "Epoch 339/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108548.7344 - val_loss: 112765.4844\n",
            "Epoch 340/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108520.8984 - val_loss: 112749.1484\n",
            "Epoch 341/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108491.4766 - val_loss: 112723.9609\n",
            "Epoch 342/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 108466.7812 - val_loss: 112696.6641\n",
            "Epoch 343/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 108445.1016 - val_loss: 112677.1797\n",
            "Epoch 344/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108416.9453 - val_loss: 112652.4531\n",
            "Epoch 345/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108387.7891 - val_loss: 112640.7500\n",
            "Epoch 346/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108363.2656 - val_loss: 112615.9141\n",
            "Epoch 347/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108339.8203 - val_loss: 112596.4141\n",
            "Epoch 348/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108312.0469 - val_loss: 112573.1875\n",
            "Epoch 349/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108284.4141 - val_loss: 112544.3125\n",
            "Epoch 350/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108259.4453 - val_loss: 112521.4688\n",
            "Epoch 351/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108234.6250 - val_loss: 112500.8672\n",
            "Epoch 352/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 108207.6719 - val_loss: 112479.0703\n",
            "Epoch 353/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108182.6328 - val_loss: 112453.2734\n",
            "Epoch 354/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108156.2891 - val_loss: 112432.7188\n",
            "Epoch 355/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108130.3359 - val_loss: 112403.2266\n",
            "Epoch 356/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108105.8281 - val_loss: 112378.0312\n",
            "Epoch 357/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108083.0859 - val_loss: 112360.7656\n",
            "Epoch 358/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108056.7109 - val_loss: 112343.6953\n",
            "Epoch 359/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 108032.0312 - val_loss: 112318.7031\n",
            "Epoch 360/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108008.6250 - val_loss: 112296.2500\n",
            "Epoch 361/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 107989.3203 - val_loss: 112276.6719\n",
            "Epoch 362/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107962.9297 - val_loss: 112255.7344\n",
            "Epoch 363/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107941.5469 - val_loss: 112246.3672\n",
            "Epoch 364/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107916.6875 - val_loss: 112218.5000\n",
            "Epoch 365/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107894.1250 - val_loss: 112202.7500\n",
            "Epoch 366/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107868.7188 - val_loss: 112179.1328\n",
            "Epoch 367/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107846.2344 - val_loss: 112163.5625\n",
            "Epoch 368/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107825.7188 - val_loss: 112138.6172\n",
            "Epoch 369/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107795.2109 - val_loss: 112117.6094\n",
            "Epoch 370/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107776.5547 - val_loss: 112095.3281\n",
            "Epoch 371/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107751.1016 - val_loss: 112076.8516\n",
            "Epoch 372/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 107727.7812 - val_loss: 112052.4609\n",
            "Epoch 373/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107706.7109 - val_loss: 112036.3203\n",
            "Epoch 374/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107683.2188 - val_loss: 112016.3047\n",
            "Epoch 375/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107656.7656 - val_loss: 111996.8438\n",
            "Epoch 376/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107637.5078 - val_loss: 111979.7188\n",
            "Epoch 377/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 107613.9922 - val_loss: 111954.4453\n",
            "Epoch 378/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107590.0000 - val_loss: 111926.9219\n",
            "Epoch 379/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107569.5547 - val_loss: 111916.2891\n",
            "Epoch 380/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107544.2344 - val_loss: 111900.1719\n",
            "Epoch 381/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107523.0234 - val_loss: 111867.1250\n",
            "Epoch 382/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107497.5312 - val_loss: 111851.6250\n",
            "Epoch 383/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107477.5859 - val_loss: 111819.5156\n",
            "Epoch 384/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107454.1172 - val_loss: 111811.9844\n",
            "Epoch 385/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107432.8438 - val_loss: 111789.2188\n",
            "Epoch 386/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107409.0078 - val_loss: 111752.0859\n",
            "Epoch 387/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 107387.1719 - val_loss: 111741.3750\n",
            "Epoch 388/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107363.2578 - val_loss: 111721.5469\n",
            "Epoch 389/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 107344.1016 - val_loss: 111714.3281\n",
            "Epoch 390/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107319.0703 - val_loss: 111700.6719\n",
            "Epoch 391/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107299.0234 - val_loss: 111664.2109\n",
            "Epoch 392/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107279.9141 - val_loss: 111644.6250\n",
            "Epoch 393/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 107259.9297 - val_loss: 111636.1562\n",
            "Epoch 394/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107233.1016 - val_loss: 111605.3594\n",
            "Epoch 395/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 107208.7031 - val_loss: 111593.7188\n",
            "Epoch 396/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107187.4219 - val_loss: 111574.8203\n",
            "Epoch 397/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107166.1797 - val_loss: 111555.1562\n",
            "Epoch 398/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107144.3594 - val_loss: 111538.7734\n",
            "Epoch 399/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107120.9688 - val_loss: 111521.6094\n",
            "Epoch 400/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107099.8594 - val_loss: 111505.0078\n",
            "Epoch 401/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107079.7969 - val_loss: 111485.3438\n",
            "Epoch 402/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107056.5938 - val_loss: 111465.9922\n",
            "Epoch 403/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107036.8750 - val_loss: 111437.3438\n",
            "Epoch 404/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107011.9297 - val_loss: 111428.2109\n",
            "Epoch 405/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106987.8906 - val_loss: 111399.5078\n",
            "Epoch 406/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106964.8438 - val_loss: 111383.1875\n",
            "Epoch 407/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106945.4062 - val_loss: 111359.6719\n",
            "Epoch 408/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106930.0703 - val_loss: 111328.0000\n",
            "Epoch 409/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106903.0078 - val_loss: 111315.5156\n",
            "Epoch 410/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106879.3672 - val_loss: 111298.5547\n",
            "Epoch 411/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106858.9219 - val_loss: 111275.8125\n",
            "Epoch 412/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106839.0625 - val_loss: 111266.8594\n",
            "Epoch 413/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106815.6797 - val_loss: 111242.7500\n",
            "Epoch 414/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106796.4922 - val_loss: 111219.7500\n",
            "Epoch 415/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106780.4922 - val_loss: 111218.2734\n",
            "Epoch 416/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106756.3125 - val_loss: 111193.0625\n",
            "Epoch 417/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106734.0703 - val_loss: 111168.5625\n",
            "Epoch 418/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106712.2500 - val_loss: 111150.4297\n",
            "Epoch 419/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106695.4375 - val_loss: 111140.7500\n",
            "Epoch 420/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106672.6484 - val_loss: 111118.5625\n",
            "Epoch 421/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106652.9453 - val_loss: 111081.7500\n",
            "Epoch 422/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106629.6953 - val_loss: 111065.4453\n",
            "Epoch 423/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 106611.7969 - val_loss: 111041.3203\n",
            "Epoch 424/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106589.0547 - val_loss: 111029.2422\n",
            "Epoch 425/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106566.4922 - val_loss: 111004.5781\n",
            "Epoch 426/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106553.5078 - val_loss: 110981.1094\n",
            "Epoch 427/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106525.7734 - val_loss: 110959.7891\n",
            "Epoch 428/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106508.3516 - val_loss: 110945.1250\n",
            "Epoch 429/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 106490.3750 - val_loss: 110931.2344\n",
            "Epoch 430/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106468.5312 - val_loss: 110900.8750\n",
            "Epoch 431/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106444.9375 - val_loss: 110878.0547\n",
            "Epoch 432/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106427.7734 - val_loss: 110868.1953\n",
            "Epoch 433/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106408.4844 - val_loss: 110844.9609\n",
            "Epoch 434/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106388.7188 - val_loss: 110819.2266\n",
            "Epoch 435/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106372.0859 - val_loss: 110803.4766\n",
            "Epoch 436/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 106348.5703 - val_loss: 110784.6406\n",
            "Epoch 437/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106327.3594 - val_loss: 110777.8125\n",
            "Epoch 438/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106305.3750 - val_loss: 110758.8281\n",
            "Epoch 439/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106285.2969 - val_loss: 110734.1250\n",
            "Epoch 440/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106267.9922 - val_loss: 110720.2344\n",
            "Epoch 441/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106248.7266 - val_loss: 110689.7500\n",
            "Epoch 442/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 106231.2031 - val_loss: 110672.7266\n",
            "Epoch 443/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106210.3984 - val_loss: 110662.3594\n",
            "Epoch 444/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 106189.7969 - val_loss: 110639.7500\n",
            "Epoch 445/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106173.6953 - val_loss: 110612.0469\n",
            "Epoch 446/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 106151.6953 - val_loss: 110595.8125\n",
            "Epoch 447/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 106131.7109 - val_loss: 110588.2422\n",
            "Epoch 448/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 106111.7812 - val_loss: 110563.2734\n",
            "Epoch 449/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 106097.5078 - val_loss: 110552.2109\n",
            "Epoch 450/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 106078.3203 - val_loss: 110538.0938\n",
            "Epoch 451/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106059.2656 - val_loss: 110524.7031\n",
            "Epoch 452/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106037.1484 - val_loss: 110493.4844\n",
            "Epoch 453/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106018.2656 - val_loss: 110471.5391\n",
            "Epoch 454/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105996.1250 - val_loss: 110451.8594\n",
            "Epoch 455/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105981.1484 - val_loss: 110439.5391\n",
            "Epoch 456/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105962.0234 - val_loss: 110423.2031\n",
            "Epoch 457/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105941.0469 - val_loss: 110408.0078\n",
            "Epoch 458/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105924.8750 - val_loss: 110378.4688\n",
            "Epoch 459/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 105899.8906 - val_loss: 110363.2109\n",
            "Epoch 460/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 105883.2656 - val_loss: 110337.8281\n",
            "Epoch 461/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105861.6641 - val_loss: 110325.5469\n",
            "Epoch 462/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 105840.5312 - val_loss: 110309.1797\n",
            "Epoch 463/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 105822.1719 - val_loss: 110286.1641\n",
            "Epoch 464/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 105801.6875 - val_loss: 110272.6719\n",
            "Epoch 465/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 105783.5156 - val_loss: 110259.6719\n",
            "Epoch 466/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105760.8984 - val_loss: 110252.3359\n",
            "Epoch 467/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 105742.5000 - val_loss: 110226.7500\n",
            "Epoch 468/1000\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 105721.4844 - val_loss: 110213.8828\n",
            "Epoch 469/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 105702.5938 - val_loss: 110205.4219\n",
            "Epoch 470/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 105688.3359 - val_loss: 110175.4297\n",
            "Epoch 471/1000\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 105664.5156 - val_loss: 110174.3516\n",
            "Epoch 472/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 105644.9141 - val_loss: 110154.8984\n",
            "Epoch 473/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105627.6719 - val_loss: 110135.5703\n",
            "Epoch 474/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105611.5078 - val_loss: 110129.2109\n",
            "Epoch 475/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105590.4844 - val_loss: 110097.6719\n",
            "Epoch 476/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105571.8047 - val_loss: 110090.2266\n",
            "Epoch 477/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 105554.9297 - val_loss: 110076.1875\n",
            "Epoch 478/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105535.2656 - val_loss: 110050.3203\n",
            "Epoch 479/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105517.3281 - val_loss: 110050.6562\n",
            "Epoch 480/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105497.0156 - val_loss: 110027.5156\n",
            "Epoch 481/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105480.9922 - val_loss: 110011.6172\n",
            "Epoch 482/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105462.8906 - val_loss: 109991.0000\n",
            "Epoch 483/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105444.1016 - val_loss: 109978.8984\n",
            "Epoch 484/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105430.6719 - val_loss: 109965.9141\n",
            "Epoch 485/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105408.4062 - val_loss: 109959.7891\n",
            "Epoch 486/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105395.7188 - val_loss: 109942.5703\n",
            "Epoch 487/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105379.4766 - val_loss: 109934.9062\n",
            "Epoch 488/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105356.8828 - val_loss: 109903.0625\n",
            "Epoch 489/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105342.8750 - val_loss: 109899.3672\n",
            "Epoch 490/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105325.6719 - val_loss: 109892.6797\n",
            "Epoch 491/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105313.1016 - val_loss: 109872.9766\n",
            "Epoch 492/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105290.9922 - val_loss: 109861.8438\n",
            "Epoch 493/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105275.5000 - val_loss: 109839.4609\n",
            "Epoch 494/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105253.7656 - val_loss: 109830.5234\n",
            "Epoch 495/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105238.0703 - val_loss: 109813.6250\n",
            "Epoch 496/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105221.4453 - val_loss: 109794.3281\n",
            "Epoch 497/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105205.0547 - val_loss: 109786.8047\n",
            "Epoch 498/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105190.3750 - val_loss: 109770.2266\n",
            "Epoch 499/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105172.4375 - val_loss: 109753.3047\n",
            "Epoch 500/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105155.5703 - val_loss: 109743.1250\n",
            "Epoch 501/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 105141.5234 - val_loss: 109717.8438\n",
            "Epoch 502/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105122.6484 - val_loss: 109701.4688\n",
            "Epoch 503/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105112.7656 - val_loss: 109697.0938\n",
            "Epoch 504/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105096.1328 - val_loss: 109676.6797\n",
            "Epoch 505/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105080.8438 - val_loss: 109650.0859\n",
            "Epoch 506/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 105065.3125 - val_loss: 109646.9922\n",
            "Epoch 507/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105051.1641 - val_loss: 109637.4453\n",
            "Epoch 508/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 105040.4219 - val_loss: 109613.7109\n",
            "Epoch 509/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105022.1328 - val_loss: 109604.0078\n",
            "Epoch 510/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 105008.6641 - val_loss: 109595.2812\n",
            "Epoch 511/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104993.8125 - val_loss: 109571.9688\n",
            "Epoch 512/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104981.1016 - val_loss: 109549.1172\n",
            "Epoch 513/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104967.7578 - val_loss: 109546.2812\n",
            "Epoch 514/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104950.8281 - val_loss: 109530.2266\n",
            "Epoch 515/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104935.7031 - val_loss: 109516.4531\n",
            "Epoch 516/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104924.0391 - val_loss: 109494.4922\n",
            "Epoch 517/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104908.7891 - val_loss: 109487.8750\n",
            "Epoch 518/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104893.7812 - val_loss: 109472.3828\n",
            "Epoch 519/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104880.5859 - val_loss: 109446.0703\n",
            "Epoch 520/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104871.6641 - val_loss: 109432.4922\n",
            "Epoch 521/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104850.6406 - val_loss: 109428.5781\n",
            "Epoch 522/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 104841.1484 - val_loss: 109420.7266\n",
            "Epoch 523/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104828.3672 - val_loss: 109401.8594\n",
            "Epoch 524/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104814.2422 - val_loss: 109385.8984\n",
            "Epoch 525/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104801.2188 - val_loss: 109368.6953\n",
            "Epoch 526/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104784.6797 - val_loss: 109355.6172\n",
            "Epoch 527/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104774.6562 - val_loss: 109339.2109\n",
            "Epoch 528/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104755.9141 - val_loss: 109330.8750\n",
            "Epoch 529/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104746.0391 - val_loss: 109318.1719\n",
            "Epoch 530/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104737.3125 - val_loss: 109302.1953\n",
            "Epoch 531/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 104720.4375 - val_loss: 109282.2734\n",
            "Epoch 532/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104710.8047 - val_loss: 109279.8203\n",
            "Epoch 533/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104692.7109 - val_loss: 109262.6406\n",
            "Epoch 534/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104681.3047 - val_loss: 109239.3203\n",
            "Epoch 535/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104666.0859 - val_loss: 109234.1875\n",
            "Epoch 536/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 104652.9141 - val_loss: 109234.0859\n",
            "Epoch 537/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104641.2891 - val_loss: 109206.2891\n",
            "Epoch 538/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104628.1406 - val_loss: 109194.3438\n",
            "Epoch 539/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104611.9922 - val_loss: 109180.5547\n",
            "Epoch 540/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104597.0156 - val_loss: 109169.3438\n",
            "Epoch 541/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104589.3359 - val_loss: 109154.1641\n",
            "Epoch 542/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104572.7656 - val_loss: 109154.0234\n",
            "Epoch 543/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104560.5625 - val_loss: 109153.1484\n",
            "Epoch 544/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104549.6328 - val_loss: 109126.9766\n",
            "Epoch 545/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104532.2578 - val_loss: 109115.7578\n",
            "Epoch 546/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 104519.3906 - val_loss: 109101.6250\n",
            "Epoch 547/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104507.9844 - val_loss: 109072.4609\n",
            "Epoch 548/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104492.0781 - val_loss: 109063.9531\n",
            "Epoch 549/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 104480.3750 - val_loss: 109052.4453\n",
            "Epoch 550/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104470.8906 - val_loss: 109044.2578\n",
            "Epoch 551/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104452.5938 - val_loss: 109030.0078\n",
            "Epoch 552/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104440.6250 - val_loss: 109012.3750\n",
            "Epoch 553/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104424.7109 - val_loss: 109002.3672\n",
            "Epoch 554/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104417.6641 - val_loss: 108979.4062\n",
            "Epoch 555/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104405.5469 - val_loss: 108981.1953\n",
            "Epoch 556/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104392.6797 - val_loss: 108946.8438\n",
            "Epoch 557/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104374.0469 - val_loss: 108958.6953\n",
            "Epoch 558/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104364.2891 - val_loss: 108951.7812\n",
            "Epoch 559/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104350.0312 - val_loss: 108929.5703\n",
            "Epoch 560/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104336.7734 - val_loss: 108918.6016\n",
            "Epoch 561/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104324.3359 - val_loss: 108885.5938\n",
            "Epoch 562/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104314.5234 - val_loss: 108866.4062\n",
            "Epoch 563/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104302.7969 - val_loss: 108866.6953\n",
            "Epoch 564/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104291.8047 - val_loss: 108852.6094\n",
            "Epoch 565/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104280.6406 - val_loss: 108834.2422\n",
            "Epoch 566/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104265.4453 - val_loss: 108826.7812\n",
            "Epoch 567/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104254.7109 - val_loss: 108811.2031\n",
            "Epoch 568/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104240.8828 - val_loss: 108800.0469\n",
            "Epoch 569/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104228.0859 - val_loss: 108801.2812\n",
            "Epoch 570/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104218.5078 - val_loss: 108772.6406\n",
            "Epoch 571/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104205.8281 - val_loss: 108760.9141\n",
            "Epoch 572/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104195.7109 - val_loss: 108741.3516\n",
            "Epoch 573/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104183.0000 - val_loss: 108731.9609\n",
            "Epoch 574/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 104172.6562 - val_loss: 108730.7969\n",
            "Epoch 575/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104158.3359 - val_loss: 108720.2266\n",
            "Epoch 576/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104147.6484 - val_loss: 108699.0547\n",
            "Epoch 577/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104141.3125 - val_loss: 108682.4219\n",
            "Epoch 578/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104124.2109 - val_loss: 108679.5312\n",
            "Epoch 579/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104114.0703 - val_loss: 108670.7500\n",
            "Epoch 580/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 104104.5938 - val_loss: 108656.3672\n",
            "Epoch 581/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104087.5547 - val_loss: 108632.4766\n",
            "Epoch 582/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104080.6797 - val_loss: 108616.3438\n",
            "Epoch 583/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104066.6094 - val_loss: 108602.9531\n",
            "Epoch 584/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104057.4531 - val_loss: 108592.6094\n",
            "Epoch 585/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104047.7500 - val_loss: 108578.8906\n",
            "Epoch 586/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 104030.5547 - val_loss: 108579.8906\n",
            "Epoch 587/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104024.0547 - val_loss: 108563.9297\n",
            "Epoch 588/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104010.5469 - val_loss: 108546.9375\n",
            "Epoch 589/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 104000.7578 - val_loss: 108544.2734\n",
            "Epoch 590/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103989.5234 - val_loss: 108532.2422\n",
            "Epoch 591/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103975.8516 - val_loss: 108521.4062\n",
            "Epoch 592/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103964.4297 - val_loss: 108510.6953\n",
            "Epoch 593/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103955.7422 - val_loss: 108488.8125\n",
            "Epoch 594/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103942.4453 - val_loss: 108477.0469\n",
            "Epoch 595/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103930.5781 - val_loss: 108469.0859\n",
            "Epoch 596/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103922.3359 - val_loss: 108454.5547\n",
            "Epoch 597/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103909.5469 - val_loss: 108445.7266\n",
            "Epoch 598/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103900.1094 - val_loss: 108427.2422\n",
            "Epoch 599/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103891.6875 - val_loss: 108407.5234\n",
            "Epoch 600/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 103882.8828 - val_loss: 108400.4922\n",
            "Epoch 601/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103866.8438 - val_loss: 108392.3125\n",
            "Epoch 602/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103859.6562 - val_loss: 108377.0312\n",
            "Epoch 603/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103846.7031 - val_loss: 108370.2578\n",
            "Epoch 604/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103833.3984 - val_loss: 108361.1094\n",
            "Epoch 605/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103826.1250 - val_loss: 108356.3359\n",
            "Epoch 606/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103813.8047 - val_loss: 108330.7266\n",
            "Epoch 607/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103804.9922 - val_loss: 108310.1797\n",
            "Epoch 608/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103791.8125 - val_loss: 108309.2578\n",
            "Epoch 609/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103783.6641 - val_loss: 108293.2031\n",
            "Epoch 610/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 103771.7422 - val_loss: 108275.4219\n",
            "Epoch 611/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103766.2656 - val_loss: 108258.3281\n",
            "Epoch 612/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103753.0703 - val_loss: 108260.4844\n",
            "Epoch 613/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103745.1094 - val_loss: 108229.5391\n",
            "Epoch 614/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103733.0000 - val_loss: 108226.1250\n",
            "Epoch 615/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103723.7500 - val_loss: 108203.0625\n",
            "Epoch 616/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103714.1406 - val_loss: 108201.3594\n",
            "Epoch 617/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103703.7812 - val_loss: 108195.8125\n",
            "Epoch 618/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103696.9609 - val_loss: 108170.3359\n",
            "Epoch 619/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103682.8750 - val_loss: 108172.1875\n",
            "Epoch 620/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103673.0391 - val_loss: 108159.8594\n",
            "Epoch 621/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103665.3906 - val_loss: 108142.7578\n",
            "Epoch 622/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103651.9062 - val_loss: 108133.0156\n",
            "Epoch 623/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103644.0312 - val_loss: 108132.0156\n",
            "Epoch 624/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103636.7422 - val_loss: 108124.2578\n",
            "Epoch 625/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103625.6094 - val_loss: 108115.0234\n",
            "Epoch 626/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103617.8828 - val_loss: 108099.0703\n",
            "Epoch 627/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103604.8125 - val_loss: 108074.4219\n",
            "Epoch 628/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103596.8281 - val_loss: 108067.1172\n",
            "Epoch 629/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103583.4922 - val_loss: 108051.5234\n",
            "Epoch 630/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103575.1562 - val_loss: 108029.1484\n",
            "Epoch 631/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103564.7422 - val_loss: 108024.9141\n",
            "Epoch 632/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103555.7031 - val_loss: 108025.1875\n",
            "Epoch 633/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103546.9141 - val_loss: 108018.0859\n",
            "Epoch 634/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103538.6562 - val_loss: 108007.3750\n",
            "Epoch 635/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103525.8203 - val_loss: 107987.5156\n",
            "Epoch 636/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103522.3359 - val_loss: 107985.7500\n",
            "Epoch 637/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103510.0312 - val_loss: 107965.9766\n",
            "Epoch 638/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103497.3438 - val_loss: 107943.5547\n",
            "Epoch 639/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103488.0703 - val_loss: 107935.7109\n",
            "Epoch 640/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103481.0859 - val_loss: 107939.2656\n",
            "Epoch 641/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103471.5859 - val_loss: 107929.9062\n",
            "Epoch 642/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103465.0938 - val_loss: 107899.9297\n",
            "Epoch 643/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103450.5547 - val_loss: 107893.5781\n",
            "Epoch 644/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103444.2266 - val_loss: 107892.5625\n",
            "Epoch 645/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103437.3281 - val_loss: 107886.9375\n",
            "Epoch 646/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103422.5625 - val_loss: 107859.9688\n",
            "Epoch 647/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103414.8672 - val_loss: 107851.7266\n",
            "Epoch 648/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103405.0859 - val_loss: 107857.5234\n",
            "Epoch 649/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103395.4531 - val_loss: 107828.2344\n",
            "Epoch 650/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103388.6094 - val_loss: 107825.6094\n",
            "Epoch 651/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103380.8984 - val_loss: 107830.1094\n",
            "Epoch 652/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103369.6953 - val_loss: 107802.9141\n",
            "Epoch 653/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103359.6484 - val_loss: 107784.5391\n",
            "Epoch 654/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103350.2656 - val_loss: 107770.7578\n",
            "Epoch 655/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103342.1875 - val_loss: 107774.3438\n",
            "Epoch 656/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103336.5234 - val_loss: 107750.8203\n",
            "Epoch 657/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103322.0391 - val_loss: 107748.3750\n",
            "Epoch 658/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103314.9062 - val_loss: 107740.5078\n",
            "Epoch 659/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 103305.8906 - val_loss: 107719.0000\n",
            "Epoch 660/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103296.8516 - val_loss: 107713.7891\n",
            "Epoch 661/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103286.4453 - val_loss: 107702.9297\n",
            "Epoch 662/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103277.9297 - val_loss: 107707.1953\n",
            "Epoch 663/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 103270.1172 - val_loss: 107682.4219\n",
            "Epoch 664/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103263.3828 - val_loss: 107679.7344\n",
            "Epoch 665/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103249.1875 - val_loss: 107662.9922\n",
            "Epoch 666/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 103242.3516 - val_loss: 107667.3047\n",
            "Epoch 667/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 103236.3281 - val_loss: 107642.3906\n",
            "Epoch 668/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103225.5078 - val_loss: 107640.0703\n",
            "Epoch 669/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103220.5156 - val_loss: 107626.1641\n",
            "Epoch 670/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103208.2031 - val_loss: 107614.4219\n",
            "Epoch 671/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103200.1484 - val_loss: 107610.8359\n",
            "Epoch 672/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103194.2344 - val_loss: 107594.6875\n",
            "Epoch 673/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103185.5391 - val_loss: 107578.3281\n",
            "Epoch 674/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103177.2578 - val_loss: 107590.3047\n",
            "Epoch 675/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103166.8516 - val_loss: 107567.2812\n",
            "Epoch 676/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103157.4375 - val_loss: 107552.3984\n",
            "Epoch 677/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103149.7812 - val_loss: 107526.6250\n",
            "Epoch 678/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103141.8828 - val_loss: 107525.4609\n",
            "Epoch 679/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103129.3516 - val_loss: 107518.3516\n",
            "Epoch 680/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103123.2812 - val_loss: 107514.9297\n",
            "Epoch 681/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 103114.3906 - val_loss: 107499.4609\n",
            "Epoch 682/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103103.1094 - val_loss: 107498.1641\n",
            "Epoch 683/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103097.1562 - val_loss: 107482.7656\n",
            "Epoch 684/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103088.4844 - val_loss: 107475.2109\n",
            "Epoch 685/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 103077.6016 - val_loss: 107466.0312\n",
            "Epoch 686/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 103069.5859 - val_loss: 107458.2344\n",
            "Epoch 687/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103061.2422 - val_loss: 107449.3281\n",
            "Epoch 688/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103052.0703 - val_loss: 107436.4531\n",
            "Epoch 689/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103042.6484 - val_loss: 107426.3984\n",
            "Epoch 690/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103036.1797 - val_loss: 107433.9922\n",
            "Epoch 691/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103028.0078 - val_loss: 107412.6953\n",
            "Epoch 692/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103017.8125 - val_loss: 107399.2812\n",
            "Epoch 693/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103013.4766 - val_loss: 107389.0859\n",
            "Epoch 694/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 103002.3438 - val_loss: 107376.9766\n",
            "Epoch 695/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102994.1641 - val_loss: 107364.3047\n",
            "Epoch 696/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102987.8594 - val_loss: 107355.5312\n",
            "Epoch 697/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102975.0312 - val_loss: 107340.9141\n",
            "Epoch 698/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 102968.8125 - val_loss: 107332.0938\n",
            "Epoch 699/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 102958.2578 - val_loss: 107326.9141\n",
            "Epoch 700/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102951.1484 - val_loss: 107320.9844\n",
            "Epoch 701/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102939.8203 - val_loss: 107291.2109\n",
            "Epoch 702/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 102933.0234 - val_loss: 107292.7734\n",
            "Epoch 703/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102926.0469 - val_loss: 107284.6406\n",
            "Epoch 704/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102918.1797 - val_loss: 107271.5625\n",
            "Epoch 705/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102909.6406 - val_loss: 107267.0625\n",
            "Epoch 706/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102897.9141 - val_loss: 107246.6406\n",
            "Epoch 707/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102892.5547 - val_loss: 107242.6094\n",
            "Epoch 708/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 102882.3281 - val_loss: 107228.9766\n",
            "Epoch 709/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 102876.9844 - val_loss: 107214.6641\n",
            "Epoch 710/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 102867.2422 - val_loss: 107196.9922\n",
            "Epoch 711/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102858.8828 - val_loss: 107202.8438\n",
            "Epoch 712/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 102850.6875 - val_loss: 107178.8906\n",
            "Epoch 713/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102844.5000 - val_loss: 107175.1953\n",
            "Epoch 714/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102834.4688 - val_loss: 107156.4609\n",
            "Epoch 715/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102824.5391 - val_loss: 107156.7188\n",
            "Epoch 716/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102818.3828 - val_loss: 107134.4766\n",
            "Epoch 717/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102805.6562 - val_loss: 107138.9922\n",
            "Epoch 718/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102801.8438 - val_loss: 107130.4453\n",
            "Epoch 719/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102792.0078 - val_loss: 107101.5938\n",
            "Epoch 720/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102784.8359 - val_loss: 107096.9922\n",
            "Epoch 721/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102775.9766 - val_loss: 107085.5781\n",
            "Epoch 722/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102775.3906 - val_loss: 107079.7969\n",
            "Epoch 723/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102763.0859 - val_loss: 107056.2266\n",
            "Epoch 724/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102755.3672 - val_loss: 107055.0000\n",
            "Epoch 725/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102747.8984 - val_loss: 107055.6875\n",
            "Epoch 726/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 102739.3750 - val_loss: 107029.1328\n",
            "Epoch 727/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102729.8438 - val_loss: 107030.5625\n",
            "Epoch 728/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102725.3359 - val_loss: 107013.0625\n",
            "Epoch 729/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 102714.4688 - val_loss: 107017.7344\n",
            "Epoch 730/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102710.7031 - val_loss: 107014.1328\n",
            "Epoch 731/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102701.7500 - val_loss: 107009.0547\n",
            "Epoch 732/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102693.5859 - val_loss: 106990.8750\n",
            "Epoch 733/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102686.2188 - val_loss: 106966.2578\n",
            "Epoch 734/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102676.9922 - val_loss: 106975.0703\n",
            "Epoch 735/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102668.9219 - val_loss: 106956.6406\n",
            "Epoch 736/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102664.1719 - val_loss: 106942.2500\n",
            "Epoch 737/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 102657.6172 - val_loss: 106925.4375\n",
            "Epoch 738/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102651.8047 - val_loss: 106925.6406\n",
            "Epoch 739/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102641.3594 - val_loss: 106919.0312\n",
            "Epoch 740/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102633.7969 - val_loss: 106903.6328\n",
            "Epoch 741/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102627.7656 - val_loss: 106905.5078\n",
            "Epoch 742/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102618.5781 - val_loss: 106877.3828\n",
            "Epoch 743/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102610.3281 - val_loss: 106871.6875\n",
            "Epoch 744/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102603.2812 - val_loss: 106869.5312\n",
            "Epoch 745/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 102594.0156 - val_loss: 106854.1406\n",
            "Epoch 746/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102591.3516 - val_loss: 106848.5625\n",
            "Epoch 747/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102578.7109 - val_loss: 106825.7422\n",
            "Epoch 748/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102574.7109 - val_loss: 106817.0234\n",
            "Epoch 749/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102569.6172 - val_loss: 106816.5859\n",
            "Epoch 750/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102564.4922 - val_loss: 106817.0078\n",
            "Epoch 751/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102551.0234 - val_loss: 106804.7578\n",
            "Epoch 752/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102545.3750 - val_loss: 106797.7031\n",
            "Epoch 753/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102541.5156 - val_loss: 106776.6797\n",
            "Epoch 754/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102534.5859 - val_loss: 106781.6641\n",
            "Epoch 755/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102526.9062 - val_loss: 106760.9922\n",
            "Epoch 756/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102517.9297 - val_loss: 106756.4141\n",
            "Epoch 757/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102508.7500 - val_loss: 106739.9062\n",
            "Epoch 758/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102503.3047 - val_loss: 106719.0938\n",
            "Epoch 759/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102502.9688 - val_loss: 106712.7188\n",
            "Epoch 760/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102492.5312 - val_loss: 106715.7188\n",
            "Epoch 761/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102482.1250 - val_loss: 106709.4297\n",
            "Epoch 762/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102475.0000 - val_loss: 106707.2500\n",
            "Epoch 763/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102470.0625 - val_loss: 106703.2734\n",
            "Epoch 764/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102460.0547 - val_loss: 106694.0781\n",
            "Epoch 765/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102455.1172 - val_loss: 106681.3047\n",
            "Epoch 766/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102449.9766 - val_loss: 106660.9922\n",
            "Epoch 767/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102446.1797 - val_loss: 106653.8125\n",
            "Epoch 768/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102437.3594 - val_loss: 106648.1562\n",
            "Epoch 769/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102427.4062 - val_loss: 106655.5078\n",
            "Epoch 770/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102422.4844 - val_loss: 106644.7891\n",
            "Epoch 771/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102414.5469 - val_loss: 106624.8516\n",
            "Epoch 772/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102411.9219 - val_loss: 106636.1016\n",
            "Epoch 773/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102402.4688 - val_loss: 106603.8516\n",
            "Epoch 774/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102394.0156 - val_loss: 106602.0938\n",
            "Epoch 775/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102387.2969 - val_loss: 106596.2344\n",
            "Epoch 776/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102382.4766 - val_loss: 106580.5625\n",
            "Epoch 777/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102372.1953 - val_loss: 106579.9609\n",
            "Epoch 778/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102369.4062 - val_loss: 106558.9922\n",
            "Epoch 779/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102363.2188 - val_loss: 106547.8516\n",
            "Epoch 780/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102355.5859 - val_loss: 106538.8906\n",
            "Epoch 781/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102348.4688 - val_loss: 106539.5391\n",
            "Epoch 782/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102344.9844 - val_loss: 106529.0391\n",
            "Epoch 783/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102339.4141 - val_loss: 106524.2500\n",
            "Epoch 784/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102331.8750 - val_loss: 106528.6094\n",
            "Epoch 785/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102326.0469 - val_loss: 106506.0469\n",
            "Epoch 786/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102318.8203 - val_loss: 106482.9609\n",
            "Epoch 787/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102310.8906 - val_loss: 106496.1172\n",
            "Epoch 788/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102303.9531 - val_loss: 106475.3047\n",
            "Epoch 789/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102301.0234 - val_loss: 106475.8516\n",
            "Epoch 790/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102293.4297 - val_loss: 106460.0625\n",
            "Epoch 791/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102289.2969 - val_loss: 106453.3125\n",
            "Epoch 792/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 102279.4141 - val_loss: 106449.2734\n",
            "Epoch 793/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102274.1016 - val_loss: 106427.8516\n",
            "Epoch 794/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102267.7344 - val_loss: 106424.3125\n",
            "Epoch 795/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102260.5859 - val_loss: 106408.7578\n",
            "Epoch 796/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102252.6016 - val_loss: 106422.6953\n",
            "Epoch 797/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102247.2344 - val_loss: 106401.1250\n",
            "Epoch 798/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102241.0547 - val_loss: 106401.6797\n",
            "Epoch 799/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102234.0547 - val_loss: 106400.9609\n",
            "Epoch 800/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102227.6250 - val_loss: 106393.5859\n",
            "Epoch 801/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102223.1484 - val_loss: 106371.1797\n",
            "Epoch 802/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102219.2500 - val_loss: 106364.0703\n",
            "Epoch 803/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102210.1172 - val_loss: 106346.4922\n",
            "Epoch 804/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102204.8750 - val_loss: 106341.4766\n",
            "Epoch 805/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102196.8125 - val_loss: 106326.8672\n",
            "Epoch 806/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102194.1250 - val_loss: 106340.0938\n",
            "Epoch 807/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102183.3984 - val_loss: 106329.9375\n",
            "Epoch 808/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102176.4453 - val_loss: 106327.4844\n",
            "Epoch 809/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102173.3438 - val_loss: 106300.2578\n",
            "Epoch 810/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102163.9688 - val_loss: 106296.3438\n",
            "Epoch 811/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102160.0938 - val_loss: 106291.1719\n",
            "Epoch 812/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102151.1797 - val_loss: 106281.7891\n",
            "Epoch 813/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102146.6719 - val_loss: 106278.1719\n",
            "Epoch 814/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102140.1875 - val_loss: 106265.5859\n",
            "Epoch 815/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102134.0234 - val_loss: 106260.5859\n",
            "Epoch 816/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102126.2422 - val_loss: 106249.1172\n",
            "Epoch 817/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102118.7031 - val_loss: 106250.3281\n",
            "Epoch 818/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102113.7266 - val_loss: 106223.5547\n",
            "Epoch 819/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102105.3281 - val_loss: 106215.2422\n",
            "Epoch 820/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102101.1953 - val_loss: 106205.1016\n",
            "Epoch 821/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102094.3750 - val_loss: 106204.5156\n",
            "Epoch 822/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102088.8125 - val_loss: 106179.0625\n",
            "Epoch 823/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102081.7188 - val_loss: 106179.5938\n",
            "Epoch 824/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102074.3359 - val_loss: 106176.7578\n",
            "Epoch 825/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102072.4453 - val_loss: 106170.8359\n",
            "Epoch 826/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 102061.8984 - val_loss: 106149.9688\n",
            "Epoch 827/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102059.6484 - val_loss: 106138.8281\n",
            "Epoch 828/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102051.7969 - val_loss: 106135.6797\n",
            "Epoch 829/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102043.5234 - val_loss: 106132.3125\n",
            "Epoch 830/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102038.5547 - val_loss: 106126.0781\n",
            "Epoch 831/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 102033.4297 - val_loss: 106123.1953\n",
            "Epoch 832/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 102030.6953 - val_loss: 106126.0547\n",
            "Epoch 833/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102021.3672 - val_loss: 106125.3672\n",
            "Epoch 834/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102014.1172 - val_loss: 106112.9688\n",
            "Epoch 835/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102010.4219 - val_loss: 106085.8828\n",
            "Epoch 836/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102002.6641 - val_loss: 106090.0781\n",
            "Epoch 837/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 101995.7266 - val_loss: 106065.3125\n",
            "Epoch 838/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101991.3047 - val_loss: 106064.6797\n",
            "Epoch 839/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101985.1719 - val_loss: 106058.3516\n",
            "Epoch 840/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101979.6562 - val_loss: 106051.4375\n",
            "Epoch 841/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101975.7656 - val_loss: 106040.9062\n",
            "Epoch 842/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101970.7422 - val_loss: 106039.8203\n",
            "Epoch 843/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101964.7578 - val_loss: 106025.2812\n",
            "Epoch 844/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101959.4453 - val_loss: 106033.4609\n",
            "Epoch 845/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101954.6406 - val_loss: 106015.5078\n",
            "Epoch 846/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101946.8125 - val_loss: 106013.7578\n",
            "Epoch 847/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101939.1719 - val_loss: 105995.4766\n",
            "Epoch 848/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101933.8359 - val_loss: 105993.8047\n",
            "Epoch 849/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101928.8281 - val_loss: 105992.0703\n",
            "Epoch 850/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101925.1250 - val_loss: 105964.0938\n",
            "Epoch 851/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101918.3438 - val_loss: 105956.1172\n",
            "Epoch 852/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 101916.5312 - val_loss: 105980.3750\n",
            "Epoch 853/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101910.9297 - val_loss: 105956.6875\n",
            "Epoch 854/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101902.9062 - val_loss: 105961.6328\n",
            "Epoch 855/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101896.3516 - val_loss: 105932.0625\n",
            "Epoch 856/1000\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 101895.1875 - val_loss: 105933.5312\n",
            "Epoch 857/1000\n",
            "130/130 [==============================] - 1s 4ms/step - loss: 101886.4688 - val_loss: 105927.8750\n",
            "Epoch 858/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101881.3047 - val_loss: 105905.2656\n",
            "Epoch 859/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101873.8828 - val_loss: 105905.0625\n",
            "Epoch 860/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101868.0469 - val_loss: 105901.2344\n",
            "Epoch 861/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101862.4062 - val_loss: 105890.5469\n",
            "Epoch 862/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101858.8516 - val_loss: 105901.5000\n",
            "Epoch 863/1000\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 101855.7734 - val_loss: 105876.8828\n",
            "Epoch 864/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101848.6641 - val_loss: 105874.6562\n",
            "Epoch 865/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101844.3594 - val_loss: 105859.6953\n",
            "Epoch 866/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101839.4375 - val_loss: 105860.3125\n",
            "Epoch 867/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101833.2344 - val_loss: 105855.7656\n",
            "Epoch 868/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101826.0703 - val_loss: 105865.0312\n",
            "Epoch 869/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101819.9062 - val_loss: 105842.0312\n",
            "Epoch 870/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101818.7656 - val_loss: 105821.9531\n",
            "Epoch 871/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101808.1875 - val_loss: 105826.9141\n",
            "Epoch 872/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101805.2266 - val_loss: 105837.8984\n",
            "Epoch 873/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101800.9141 - val_loss: 105820.0234\n",
            "Epoch 874/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101796.9766 - val_loss: 105810.4297\n",
            "Epoch 875/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101790.4453 - val_loss: 105788.5234\n",
            "Epoch 876/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101786.4062 - val_loss: 105801.0781\n",
            "Epoch 877/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101785.4375 - val_loss: 105794.0547\n",
            "Epoch 878/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101772.8750 - val_loss: 105787.7266\n",
            "Epoch 879/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101774.2969 - val_loss: 105788.5234\n",
            "Epoch 880/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101761.6484 - val_loss: 105777.7188\n",
            "Epoch 881/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101762.3203 - val_loss: 105758.7500\n",
            "Epoch 882/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101756.1094 - val_loss: 105770.8906\n",
            "Epoch 883/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101752.4375 - val_loss: 105764.2422\n",
            "Epoch 884/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101742.7422 - val_loss: 105750.1719\n",
            "Epoch 885/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101739.0469 - val_loss: 105741.5938\n",
            "Epoch 886/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101733.6797 - val_loss: 105729.3438\n",
            "Epoch 887/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101733.3281 - val_loss: 105742.0469\n",
            "Epoch 888/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101723.8438 - val_loss: 105732.7109\n",
            "Epoch 889/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101721.3516 - val_loss: 105735.0391\n",
            "Epoch 890/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101714.1016 - val_loss: 105729.4609\n",
            "Epoch 891/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101709.1016 - val_loss: 105716.3516\n",
            "Epoch 892/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101706.4688 - val_loss: 105711.0859\n",
            "Epoch 893/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101697.8281 - val_loss: 105710.7500\n",
            "Epoch 894/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101696.8047 - val_loss: 105704.9219\n",
            "Epoch 895/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101692.5781 - val_loss: 105700.3047\n",
            "Epoch 896/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101683.0156 - val_loss: 105684.4844\n",
            "Epoch 897/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101682.9844 - val_loss: 105674.1641\n",
            "Epoch 898/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101675.6016 - val_loss: 105667.0312\n",
            "Epoch 899/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101669.6484 - val_loss: 105671.1562\n",
            "Epoch 900/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101666.7969 - val_loss: 105649.4141\n",
            "Epoch 901/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101663.9609 - val_loss: 105642.9297\n",
            "Epoch 902/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101653.5859 - val_loss: 105647.5859\n",
            "Epoch 903/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101651.1016 - val_loss: 105646.5625\n",
            "Epoch 904/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101649.0547 - val_loss: 105638.2578\n",
            "Epoch 905/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101642.5156 - val_loss: 105634.8906\n",
            "Epoch 906/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101636.5000 - val_loss: 105626.8438\n",
            "Epoch 907/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101632.5938 - val_loss: 105610.8594\n",
            "Epoch 908/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101625.9766 - val_loss: 105611.7891\n",
            "Epoch 909/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101624.4453 - val_loss: 105604.8984\n",
            "Epoch 910/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101619.6797 - val_loss: 105612.1641\n",
            "Epoch 911/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101612.9062 - val_loss: 105605.4844\n",
            "Epoch 912/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101610.4375 - val_loss: 105592.5781\n",
            "Epoch 913/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101603.9375 - val_loss: 105580.5625\n",
            "Epoch 914/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101598.4453 - val_loss: 105588.2734\n",
            "Epoch 915/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101594.6484 - val_loss: 105571.7578\n",
            "Epoch 916/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101590.3906 - val_loss: 105572.7891\n",
            "Epoch 917/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101586.1797 - val_loss: 105551.9219\n",
            "Epoch 918/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101578.8750 - val_loss: 105549.4453\n",
            "Epoch 919/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101575.8047 - val_loss: 105558.3203\n",
            "Epoch 920/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101570.1172 - val_loss: 105553.2969\n",
            "Epoch 921/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101564.7344 - val_loss: 105528.8047\n",
            "Epoch 922/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101562.7031 - val_loss: 105534.2656\n",
            "Epoch 923/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101555.7734 - val_loss: 105521.3750\n",
            "Epoch 924/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101552.6641 - val_loss: 105514.6016\n",
            "Epoch 925/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101551.1484 - val_loss: 105515.4219\n",
            "Epoch 926/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101544.4062 - val_loss: 105516.0703\n",
            "Epoch 927/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101539.8359 - val_loss: 105506.0938\n",
            "Epoch 928/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101534.1875 - val_loss: 105486.7188\n",
            "Epoch 929/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101530.3750 - val_loss: 105474.0625\n",
            "Epoch 930/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101525.7656 - val_loss: 105477.7969\n",
            "Epoch 931/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101519.4375 - val_loss: 105475.2656\n",
            "Epoch 932/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101517.8594 - val_loss: 105468.0781\n",
            "Epoch 933/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101510.9531 - val_loss: 105468.0156\n",
            "Epoch 934/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101507.3281 - val_loss: 105458.6719\n",
            "Epoch 935/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101501.9844 - val_loss: 105466.7891\n",
            "Epoch 936/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101498.2344 - val_loss: 105439.6719\n",
            "Epoch 937/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101494.7031 - val_loss: 105440.5547\n",
            "Epoch 938/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101493.6875 - val_loss: 105449.7656\n",
            "Epoch 939/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101484.2188 - val_loss: 105442.0703\n",
            "Epoch 940/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101479.3516 - val_loss: 105438.4297\n",
            "Epoch 941/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101474.1406 - val_loss: 105415.9375\n",
            "Epoch 942/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101472.1250 - val_loss: 105402.1250\n",
            "Epoch 943/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101465.5156 - val_loss: 105405.4609\n",
            "Epoch 944/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101465.6016 - val_loss: 105401.1406\n",
            "Epoch 945/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101463.4766 - val_loss: 105400.7578\n",
            "Epoch 946/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101452.3516 - val_loss: 105386.3281\n",
            "Epoch 947/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101450.9297 - val_loss: 105386.8906\n",
            "Epoch 948/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101445.9844 - val_loss: 105368.7656\n",
            "Epoch 949/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101441.9297 - val_loss: 105359.9688\n",
            "Epoch 950/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101433.9453 - val_loss: 105362.0859\n",
            "Epoch 951/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101432.5312 - val_loss: 105357.1094\n",
            "Epoch 952/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101424.3672 - val_loss: 105354.8047\n",
            "Epoch 953/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101421.4375 - val_loss: 105354.4453\n",
            "Epoch 954/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101417.0938 - val_loss: 105346.9688\n",
            "Epoch 955/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101413.3594 - val_loss: 105349.5938\n",
            "Epoch 956/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101408.0312 - val_loss: 105329.4609\n",
            "Epoch 957/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101408.4062 - val_loss: 105326.2656\n",
            "Epoch 958/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101398.7266 - val_loss: 105330.5547\n",
            "Epoch 959/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101397.6172 - val_loss: 105330.7422\n",
            "Epoch 960/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101391.1328 - val_loss: 105330.1250\n",
            "Epoch 961/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101387.8281 - val_loss: 105325.2344\n",
            "Epoch 962/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101383.3516 - val_loss: 105329.7656\n",
            "Epoch 963/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101378.0859 - val_loss: 105306.0859\n",
            "Epoch 964/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101374.1016 - val_loss: 105298.6328\n",
            "Epoch 965/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101369.3438 - val_loss: 105286.1953\n",
            "Epoch 966/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101368.9688 - val_loss: 105302.4453\n",
            "Epoch 967/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101361.0547 - val_loss: 105285.6328\n",
            "Epoch 968/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101354.3047 - val_loss: 105285.1641\n",
            "Epoch 969/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101350.2656 - val_loss: 105271.1641\n",
            "Epoch 970/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101347.2188 - val_loss: 105272.9062\n",
            "Epoch 971/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101340.7578 - val_loss: 105275.3594\n",
            "Epoch 972/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101336.6797 - val_loss: 105259.3047\n",
            "Epoch 973/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101334.9297 - val_loss: 105269.6641\n",
            "Epoch 974/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101332.0859 - val_loss: 105261.4453\n",
            "Epoch 975/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101326.4531 - val_loss: 105241.6328\n",
            "Epoch 976/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101321.1719 - val_loss: 105248.2578\n",
            "Epoch 977/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101316.0938 - val_loss: 105259.2109\n",
            "Epoch 978/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101309.7344 - val_loss: 105234.6094\n",
            "Epoch 979/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101306.6641 - val_loss: 105230.4219\n",
            "Epoch 980/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101301.7656 - val_loss: 105214.3828\n",
            "Epoch 981/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101299.2031 - val_loss: 105214.3750\n",
            "Epoch 982/1000\n",
            "130/130 [==============================] - 0s 4ms/step - loss: 101292.4766 - val_loss: 105215.8359\n",
            "Epoch 983/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101292.4766 - val_loss: 105214.3438\n",
            "Epoch 984/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101284.1094 - val_loss: 105195.5859\n",
            "Epoch 985/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101281.3281 - val_loss: 105195.8750\n",
            "Epoch 986/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101278.2344 - val_loss: 105177.7422\n",
            "Epoch 987/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101276.3516 - val_loss: 105212.6719\n",
            "Epoch 988/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101266.4688 - val_loss: 105167.1484\n",
            "Epoch 989/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101261.1953 - val_loss: 105164.7734\n",
            "Epoch 990/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101258.0625 - val_loss: 105171.5312\n",
            "Epoch 991/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101253.9062 - val_loss: 105186.5234\n",
            "Epoch 992/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101250.8203 - val_loss: 105164.6172\n",
            "Epoch 993/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101245.4844 - val_loss: 105152.0469\n",
            "Epoch 994/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101237.7734 - val_loss: 105157.7344\n",
            "Epoch 995/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101234.3516 - val_loss: 105144.9219\n",
            "Epoch 996/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 101227.0234 - val_loss: 105130.1797\n",
            "Epoch 997/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101225.9688 - val_loss: 105126.8438\n",
            "Epoch 998/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101219.7578 - val_loss: 105107.3750\n",
            "Epoch 999/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101219.2266 - val_loss: 105114.1250\n",
            "Epoch 1000/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101212.3125 - val_loss: 105114.2422\n",
            "169/169 [==============================] - 0s 1ms/step - loss: 101503.2266\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_log = model.fit(X_train, y_train, epochs=500, batch_size=100, validation_split=.2, verbose=1)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3vc6DpiZVRZ"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Make a simple neural network for predicting the price of homes in California. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZqmBczcMZVRZ",
        "outputId": "da98e1e8-4f49-4a0b-ee35-34c696f10dca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  \n",
              "0    -122.23  \n",
              "1    -122.22  \n",
              "2    -122.24  \n",
              "3    -122.25  \n",
              "4    -122.25  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "Xcal = pd.DataFrame(cal.data)\n",
        "ycal = pd.DataFrame(cal.target)\n",
        "Xcal.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8TxqgdHUZVRa",
        "outputId": "f29561ee-1d8f-4b8a-f19f-b28371799e23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15480, 8)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_cal,  X_test_cal, y_train_cal, y_test_cal = train_test_split(Xcal, ycal)\n",
        "X_train_cal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k0kBmDdOZVRa",
        "outputId": "6bec0963-b4c7-424b-b2df-f9d98e110e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_1 (Normalizat  (None, 8)                17        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170\n",
            "Trainable params: 153\n",
            "Non-trainable params: 17\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cal_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "cal_normalizer.adapt(np.array(X_train_cal))\n",
        "\n",
        "cal_model = Sequential()\n",
        "cal_model.add(cal_normalizer)\n",
        "cal_model.add(Dense(8, input_shape=(8,), activation='relu'))\n",
        "cal_model.add(Dense(8, activation='relu'))\n",
        "cal_model.add(Dense(1))\n",
        "cal_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XmCWLbNdZVRb",
        "outputId": "be1ca8d5-731f-4ff5-cc39-03ac81607ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "248/248 [==============================] - 2s 5ms/step - loss: 4.0834 - val_loss: 1.1671\n",
            "Epoch 2/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 1.1068 - val_loss: 0.8129\n",
            "Epoch 3/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.8408 - val_loss: 0.7013\n",
            "Epoch 4/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.6903 - val_loss: 0.6170\n",
            "Epoch 5/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.5599\n",
            "Epoch 6/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.5287 - val_loss: 0.5226\n",
            "Epoch 7/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4927 - val_loss: 0.4995\n",
            "Epoch 8/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4673 - val_loss: 0.4774\n",
            "Epoch 9/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.4618\n",
            "Epoch 10/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4463\n",
            "Epoch 11/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4332\n",
            "Epoch 12/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.4318\n",
            "Epoch 13/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.4170\n",
            "Epoch 14/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.4130\n",
            "Epoch 15/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4071\n",
            "Epoch 16/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4009\n",
            "Epoch 17/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.3993\n",
            "Epoch 18/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3922\n",
            "Epoch 19/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3887\n",
            "Epoch 20/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3862\n",
            "Epoch 21/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3853\n",
            "Epoch 22/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3881\n",
            "Epoch 23/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3792\n",
            "Epoch 24/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3525 - val_loss: 0.3749\n",
            "Epoch 25/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3746\n",
            "Epoch 26/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3714\n",
            "Epoch 27/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3711\n",
            "Epoch 28/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3693\n",
            "Epoch 29/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3688\n",
            "Epoch 30/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3664\n",
            "Epoch 31/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.3650\n",
            "Epoch 32/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.3632\n",
            "Epoch 33/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3676\n",
            "Epoch 34/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3606\n",
            "Epoch 35/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3648\n",
            "Epoch 36/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3582\n",
            "Epoch 37/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3664\n",
            "Epoch 38/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3594\n",
            "Epoch 39/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3549\n",
            "Epoch 40/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3344 - val_loss: 0.3544\n",
            "Epoch 41/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.3559\n",
            "Epoch 42/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.3545\n",
            "Epoch 43/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3527\n",
            "Epoch 44/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.3509\n",
            "Epoch 45/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.3515\n",
            "Epoch 46/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3497\n",
            "Epoch 47/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.3490\n",
            "Epoch 48/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3482\n",
            "Epoch 49/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3490\n",
            "Epoch 50/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3489\n",
            "Epoch 51/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3469\n",
            "Epoch 52/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.3479\n",
            "Epoch 53/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3454\n",
            "Epoch 54/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3498\n",
            "Epoch 55/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3454\n",
            "Epoch 56/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3292 - val_loss: 0.3488\n",
            "Epoch 57/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3444\n",
            "Epoch 58/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3443\n",
            "Epoch 59/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3243 - val_loss: 0.3434\n",
            "Epoch 60/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3426\n",
            "Epoch 61/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3436\n",
            "Epoch 62/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3238 - val_loss: 0.3431\n",
            "Epoch 63/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3431\n",
            "Epoch 64/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3439\n",
            "Epoch 65/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3255 - val_loss: 0.3490\n",
            "Epoch 66/100\n",
            "248/248 [==============================] - 0s 988us/step - loss: 0.3273 - val_loss: 0.3461\n",
            "Epoch 67/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.3512\n",
            "Epoch 68/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3442\n",
            "Epoch 69/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3435\n",
            "Epoch 70/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3409\n",
            "Epoch 71/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3417\n",
            "Epoch 72/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3431\n",
            "Epoch 73/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3211 - val_loss: 0.3424\n",
            "Epoch 74/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3437\n",
            "Epoch 75/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.3411\n",
            "Epoch 76/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.3427\n",
            "Epoch 77/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3196 - val_loss: 0.3449\n",
            "Epoch 78/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3426\n",
            "Epoch 79/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3379\n",
            "Epoch 80/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3385\n",
            "Epoch 81/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3431\n",
            "Epoch 82/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3192 - val_loss: 0.3453\n",
            "Epoch 83/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3192 - val_loss: 0.3378\n",
            "Epoch 84/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3220 - val_loss: 0.3443\n",
            "Epoch 85/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3394\n",
            "Epoch 86/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3182 - val_loss: 0.3394\n",
            "Epoch 87/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.3400\n",
            "Epoch 88/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3400\n",
            "Epoch 89/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3185 - val_loss: 0.3382\n",
            "Epoch 90/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3379\n",
            "Epoch 91/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.3368\n",
            "Epoch 92/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3177 - val_loss: 0.3481\n",
            "Epoch 93/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3378\n",
            "Epoch 94/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3384\n",
            "Epoch 95/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.3365\n",
            "Epoch 96/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.3362\n",
            "Epoch 97/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.3374\n",
            "Epoch 98/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3140 - val_loss: 0.3404\n",
            "Epoch 99/100\n",
            "248/248 [==============================] - 0s 1ms/step - loss: 0.3146 - val_loss: 0.3332\n",
            "Epoch 100/100\n",
            "248/248 [==============================] - 0s 2ms/step - loss: 0.3136 - val_loss: 0.3430\n",
            "162/162 [==============================] - 0s 929us/step - loss: 0.3350\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cal_model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam())\n",
        "cal_train_log = cal_model.fit(X_train_cal, y_train_cal, epochs=100, batch_size=50, validation_split=.2, verbose=1)\n",
        "cal_model.evaluate(X_test_cal, y_test_cal)\n",
        "plot_loss(cal_train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUfNO6HeZVRb"
      },
      "source": [
        "## Basics of Overfitting and Underfitting in Neural Networks\n",
        "\n",
        "Just like any other type of model, our primary task in trying to attain an accurate set of predictions is to balance the overfitting and underfitting. In a neural network, the ideas are the same as with standard models, however the tools and their usage can differ slightly. \n",
        "\n",
        "### Add Data\n",
        "\n",
        "Adding data to the training set is the number one way to improve accuracy. As noted above, neural networks are commonly able to acheive very high accuracy levels if provided with very large training sets. For smaller datasets, the probability of a neural network being the best model is much lower than with big data. There isn't a replacement for having large amounts of data (though there are a few tricks that we'll look at later), and modern large neural networks are (usually) the best tool that is able to take advantage of all that data. \n",
        "\n",
        "In the near future we'll look at some common pre-trained models that people/orgs such as Google have shared, most notably ones that do things like image recognition. These models are typically trained on really large datasets - often 10s of GB or more. This massive amount of training data allows these models to be more accurate than anything that we could create, but would be unrealistic for most people to train just due to the processing power and time needed. We can take them and adjust them a bit to our needs though...\n",
        "\n",
        "### Model Capacity\n",
        "\n",
        "The model capacity is the \"size\" of the model - refering to the combination of the number of neurons on each layer and the number of layers. \n",
        "\n",
        "In general the larger a feature set is, the larger a capacity we will need to be able to avoid underfitting and make accurate predictions. However, similar to a decision tree, if the model becomes too large for the data, we are likely to overfit. \n",
        "\n",
        "In big data scenarios (e.g. Google or Tesla training image recognition models) the feature sets can be massive (e.g. a 5 megapixel image is at least 15 million features) so the networks used have a very high capacity. Because there is a lot of training data, the model is able to have a huge capacity, but not overfit. These models can take FOREVER to process (e.g. weeks with the work paralellized on dedicated and fast machines) but they are able to make very accurate predictions since they get all the \"benefits\" of overfitting - predictions highly tailored to the training data; along with all the \"benefits\" of underfitting - since there is so much training data, they are still generalized enough to predict new data. \n",
        "\n",
        "The combination of large datasets, deep networks, and fast processing allows for most of the modern AI that we see or interact with. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OZWfGHNqZVRb",
        "outputId": "1e595b6c-0e69-477e-dc67-c505ef26ba74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_2 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               2432      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,134\n",
            "Trainable params: 52,097\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WXjXrQEeZVRb",
        "outputId": "7d25815a-6f32-4c0d-c89b-a255ff13816f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 2ms/step - loss: 71213.1719\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=500, batch_size=1000, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTocTQASZVRc"
      },
      "source": [
        "### Early Stopping\n",
        "\n",
        "Early stopping is very common with neural networks, due to the common pattern mentioned above of the optimal balance of over/under fitting occuring at some point within many, potentially dozens/hundreds/thousands, of epochs. Early stopping kills the process after it detects that validation loss is going back up. \n",
        "\n",
        "We can put early stopping in place by using a Keras function called a callback, which has odd syntax, but is quite simple to use. The patience pararmeter controls how many epcohs of worsening scores are tolerated before implementing the stop. The restore_best_weights tells the model to roll back all of its weights to the optimal point - so we automatically get the best model post-training. \n",
        "\n",
        "In most cases we probabyl want to use early stopping along with a high epoch number. We can let the model train, and just tell us when it is finished. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b0MYLfTBZVRc"
      },
      "outputs": [],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SFlOi1yGZVRc",
        "outputId": "256f7158-b58b-48b4-d17e-877c40d84201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 1ms/step - loss: 68510.4297\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True) \n",
        "\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=500, batch_size=100, validation_split=.2, verbose=0, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXfwUFjSZVRc"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "Like other linear models, we can implement regularization to help tame overfitting. \n",
        "\n",
        "We can use both L2 (Ridge) regularization that will limit growth of coefficients, and L1 (Lasso) regularization that is able to eliminate features by shrinking their coefficients to 0. The functionality is the same as we are used to, a regularization term is added to the loss, and the optimization, such as gradient descent, is then performed as normal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cgwwVgJ6ZVRd",
        "outputId": "7c4c3e58-3c0a-42ef-b502-764d87747a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_4 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               2432      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 35,622\n",
            "Trainable params: 35,585\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Regularization\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l2\"))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bnRVShe2ZVRd",
        "outputId": "fb5bbfba-efcd-4d12-ba31-427900c8da53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 1ms/step - loss: 67661.2031\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FXboQ7N2ZVRd"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Neural networks also commonly employ a technique call dropouts to prevent overfitting. This works just like the name says, every time the data is moved from one layer to another some portion of the features are randomly held out from being used. The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own. This sounds somewhat weird, but is actually effective. The number of features held out is called the dropout rate, typically between .2 and .5. \n",
        "\n",
        "An analogy can be drawn to the bootstrapping we looked at with trees - some random subset of features is selected each time, resulting in each batch getting \"a slightly different look at the data\", thus preventing overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gUJv1QPEZVRd",
        "outputId": "b6f5125b-b396-4dfd-9cd4-95d84f9c452e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_5 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,246\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Dropout\n",
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(512, input_dim=18, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EUJHDMXbZVRd",
        "outputId": "aff714cd-a820-49b3-8ede-e24737a275b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 1s 6ms/step - loss: 71829.8359\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=500, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AkcKFzNZVRd"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Once the model is trained, using it is mostly familiar to us from the sklearn stuff. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S-jw_zjdZVRe",
        "outputId": "0a3fbf07-c78c-4bea-fa3e-652875330375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "71829.8403049246"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = model.predict(X_test)\n",
        "mean_absolute_error(y_test, preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9YyxGjZVRe"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use the California data from previously and try to add some regularization things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGstyyzcZVRe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYC8NGfZVRe"
      },
      "source": [
        "### Customized Loss\n",
        "\n",
        "Most scenarios are totally fine with a standard loss function, but what if we have something odd? What if we are playing on The Price is Right? We want to get as close as we can, without going over. We can write a loss function to mirror that!\n",
        "\n",
        "More practically, some real life scenarios have a disperse impact of different types of error. For example, if you are working for a call centre and predicting the number of agents to staff. Having slightly too many may be an error that costs a little bit of money, but not that big of a deal. Predicting too few might incur serious penalties if callers wait and you violate an SLA. Being off in one direction is bad, being off in the other direction can cause you to \"fall off of a cliff\" so to speak. In cases where the impact of the error is not uniform, custom loss functions may make sense. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GNjj1-aHZVRe"
      },
      "outputs": [],
      "source": [
        "def priceIsRight(y_true, y_pred):\n",
        "    if y_pred <= y_true:\n",
        "        return (y_true - y_pred) ** 2\n",
        "    else:\n",
        "        return (y_true - y_pred) ** 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyN6IYzbZVRe"
      },
      "source": [
        "## Optional Exercise\n",
        "\n",
        "Try to use the California data with a customized loss function. \n",
        "\n",
        "Note: this is a 20 way classification, so you'll probably want that many neurons on the output layer, an appropriate activation (softmax), and the y values will need to be run through np_utils.to_categorical. As well, think about the loss function, try categorical crossentropy.\n",
        "\n",
        "We'll look at activation and loss functions more next week. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAEhLMc6ZVRe"
      },
      "source": [
        "## Big Exercise - Newsgroup Classification\n",
        "\n",
        "Try to classify the newsgroup data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pCHlBNRQZVRe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "remove = (\"headers\", \"footers\", \"quotes\")\n",
        "\n",
        "data_train = fetch_20newsgroups(\n",
        "    subset=\"train\", shuffle=True, remove=remove)\n",
        "\n",
        "data_test = fetch_20newsgroups(\n",
        "    subset=\"test\", shuffle=True, remove=remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1e1Iq4hZVRe",
        "outputId": "e2d62827-98eb-4a5b-87ea-e91475bd0b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (11314, 1971374)   Test: (7532, 1971374)\n"
          ]
        }
      ],
      "source": [
        "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
        "X_train = news_tf.fit_transform(data_train.data)\n",
        "y_train = data_train.target\n",
        "X_test = news_tf.transform(data_test.data)\n",
        "y_test = data_test.target\n",
        "print(\"Train:\", X_train.shape, \"  Test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DIHW5SORZVRf"
      },
      "outputs": [],
      "source": [
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_train = np_utils.to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "86ThuxsYZVRf"
      },
      "outputs": [],
      "source": [
        "in_size = 200\n",
        "tsvd = TruncatedSVD(n_components=in_size)\n",
        "X_train = tsvd.fit_transform(X_train)\n",
        "X_test = tsvd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7H1YvGdZVRf",
        "outputId": "309f9e45-1f30-4b0f-e64e-a15cbd2bac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_53 (Dense)            (None, 600)               120600    \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 600)               360600    \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 600)               0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 600)               360600    \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 600)               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 400)               240400    \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 400)               0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 400)               160400    \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 400)               0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 200)               80200     \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 200)               0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 200)               40200     \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 20)                4020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,367,020\n",
            "Trainable params: 1,367,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(in_size*3, input_dim=in_size, activation='relu'))\n",
        "model.add(Dense(in_size*3, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*3, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*2, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*2, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size, activation='relu'))\n",
        "model.add(Dense(20, activation=\"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t6JlBX_aZVRf",
        "outputId": "eee94ee4-6a24-4da5-9222-23f4a3decdc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "91/91 [==============================] - 2s 11ms/step - loss: 2.7730 - accuracy: 0.1015 - val_loss: 2.5135 - val_accuracy: 0.1436\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 2.0940 - accuracy: 0.2654 - val_loss: 1.8665 - val_accuracy: 0.3411\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 1.5779 - accuracy: 0.4559 - val_loss: 1.4100 - val_accuracy: 0.5369\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 1.2894 - accuracy: 0.5760 - val_loss: 1.2725 - val_accuracy: 0.6019\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 1.1379 - accuracy: 0.6346 - val_loss: 1.2396 - val_accuracy: 0.6178\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 1.0170 - accuracy: 0.6742 - val_loss: 1.2824 - val_accuracy: 0.6160\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.9123 - accuracy: 0.7062 - val_loss: 1.2385 - val_accuracy: 0.6407\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.8327 - accuracy: 0.7373 - val_loss: 1.2973 - val_accuracy: 0.6341\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.7458 - accuracy: 0.7627 - val_loss: 1.3977 - val_accuracy: 0.6222\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.7057 - accuracy: 0.7756 - val_loss: 1.3395 - val_accuracy: 0.6288\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.6491 - accuracy: 0.7963 - val_loss: 1.4084 - val_accuracy: 0.6558\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.5687 - accuracy: 0.8197 - val_loss: 1.4971 - val_accuracy: 0.6385\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.5434 - accuracy: 0.8243 - val_loss: 1.5894 - val_accuracy: 0.6438\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.4745 - accuracy: 0.8495 - val_loss: 1.6263 - val_accuracy: 0.6452\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.4262 - accuracy: 0.8650 - val_loss: 1.7924 - val_accuracy: 0.6438\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.4246 - accuracy: 0.8694 - val_loss: 1.6010 - val_accuracy: 0.6447\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.4010 - accuracy: 0.8757 - val_loss: 1.7030 - val_accuracy: 0.6368\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.3679 - accuracy: 0.8864 - val_loss: 1.6972 - val_accuracy: 0.6522\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.3792 - accuracy: 0.8832 - val_loss: 1.7306 - val_accuracy: 0.6602\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 1s 10ms/step - loss: 0.3422 - accuracy: 0.8926 - val_loss: 1.6462 - val_accuracy: 0.6593\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.3109 - accuracy: 0.9075 - val_loss: 1.8239 - val_accuracy: 0.6531\n",
            "Epoch 22/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.2872 - accuracy: 0.9100 - val_loss: 2.1946 - val_accuracy: 0.6412\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 1.6080 - accuracy: 0.5971\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True) \n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=1, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Tuning Summary vs Reality\n",
        "\n",
        "The techniques above generally expose a pattern that we can use to make accurate models:\n",
        "<ul>\n",
        "<li> Create a model that is accurate and overfitted. (Or setup process to generate an overfitted model)\n",
        "<li> Use hyperparameter tuning-ish methods to trial several different models. \n",
        "<li> Use tools such as regularization and early stopping to \"trim\" the overfitting back. \n",
        "</ul>\n",
        "\n",
        "For the most part, for what we are doing, this will probably work fine, without forcing us to put a tonne of thought into things upfront; we can just make a massive model, then reduce it. In practice the main downside to this type of brute force approach is time, and by extension, cost. For us the datasets are mostly small enough that in an extreme case we could do something like setup a bunch of trials, let our computer train and test overnight, and wake up to a model that is pretty good. If our data was scaled up by a factor of 10,000 this becomes less practical. We would want to sample the data to make each trial run much more quickly, but we'd still be dealing with a non-trivial amount of processing time for each model that we want to try. Doing things that reduce the processing requirements of each epoch, such as feature selection, choosing a \"correctly\" sized model, or smart sets of hyperparameters to try, will reduce the amount of \"bad\" trials, and allow us to dedicate more time to \"good\" trials. This is one of the, relatively rare, scenarios that paying attention to processing time can have massive impacts on the end results. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "022_keras_tensor_sol.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml3950')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
